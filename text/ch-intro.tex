% \begin{savequote}[8cm]
% \textlatin{Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

% There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...
%   \qauthor{--- Cicero's \textit{de Finibus Bonorum et Malorum}}
% \end{savequote}

\chapter{\label{ch:intro}Introduction} 

\minitoc

Since early theorising about Artificial Intelligence (AI), experts have noted the transformative potential of such technology. Now, with the popular advent of Machine Learning (ML) models and transformer architecture in particular, we see rapid mass-market adoption of a variety of AI tools. In many fields which deal with sensitive decision-making, this adoption introduces new ethical challenges. In the fields of talent and Talent Identification (TI) in particular, many of these ethical challenges centre on Diversity, Equity, and Inclusion (DEI) or related notions such as equality or justice. In this section, we lay out the case for the application of AI systems (specifically interpretable AI decision assistants) to DEI issues in TI, paying special attention to the potential pitfalls and broader implications of our proposal.

\section{Motivation} % Diversity and Inclusion vs ESG?
This is where I explain why I'm doing this research. I explain why care about talent identification and how IAIDSTs can solve problems here. I respond to the notion that we should simply not use AI here, and highlight problems in the field already. I explain why this is a problem for Computer Science as well as for Talent Science (tl;dr: additional reasons to care about things xai overlooks because of the application domain)

\section{Positions}
\subsection{Trust Explanations to Do What They Say} 
Increasingly, decisions affecting the lives of lay people are made by AI algorithms. And while these algorithms may be useful, they can also be dangerous. Both unwarranted trust in such an algorithm may lead to wrong and damaging decisions, while unwarranted mistrust in such an algorithm may lead to disuse. Neither outcome is favourable. Hardin draws an important distinction between whether someone or something is trusted and whether that trust is well-placed; i.e. it is worthy of trust \cite{hardin_trust_2002}. 

It is clear, then, that trust in AI algorithms should be \textit{calibrated}, so that users are led to trust trustworthy AI systems and distrust untrustworthy AI systems. \textcite{jacovi_formalizing_2021} propose that trust in AI systems can be understood in terms of a contract between the system and the trustor. 

\textcite{jacovi_formalizing_2021} define a model of human-AI trust resting on two key properties: the \textit{vulnerability} of the user to the model and the user's ability to \textit{anticipate} the impact of the AI model’s decisions. In the human context, person $A$ trusts person $B$ if and only if $A$ believes that $B$ will act in $A$'s best interest, and $A$ accepts vulnerability to $B$'s actions. In the machine context, we do not always expect the machine to act in our best interests. Instead, user $U$ trusts AI model $M$ if and only if $U$ can anticipate and accepts vulnerability to $M$'s actions \cite{jacovi_formalizing_2021}.

Moreover, trust often does not have a blanket scope; typically, $U$ will trust $M$ regarding some particular actions or range of actions, though a broader trust will include many such actions. In the algorithmic context, this scope is clearly limited – unlike humans, trust in algorithms should never be broad; warranted trust is always scoped to a region in which the algorithm's actions can be anticipated, and in which users might reasonably accept vulnerability to these actions. Generally, this scope is limited to some subsection of the intended use cases of the AI system. Trust placed in an AI system to do something it was not intended to do is often unwarranted; trust placed in an AI system to do something it does not claim to do is always unwarranted. Thus, for an algorithm to be trustworthy in a given scope, that algorithm should demonstrate both that a user can anticipate behaviour in that scope and that the anticipated behaviour is such that users might accept vulnerability to the algorithm. We call this demonstration a \emph{contract}, and call this sort of trust \emph{contractual trust} \cite{jacovi_formalizing_2021}.

Following this framing, the extent to which an algorithm warrants trust is modulated by the extent to which it adheres to its contract. Therefore, when the developers of an algorithm provide a contract regarding the intended use of an algorithm, we can evaluate the trustworthiness of an algorithm by evaluating adherence to the contract.

One method of evaluating adherence to contract comes from a user observing the AI algorithm's reasoning process by way of an explanation or an interpretation. However, unlike human decision-makers, few algorithms are inherently capable of explaining their reasoning. The growing field of Explainable Artificial Intelligence (xAI) aims to develop methods for explaining the reasoning algorithms, often with a broad goal of increasing warranted trust in algorithms. However, though it is clear that these algorithms often increase trust in algorithms, it is not always clear that this trust is warranted, as demonstrated by \textcite{jacobs_how_2021}. Thus, it seems, there are times where even an explanation of an AI algorithm should be distrusted.

\subsubsection{Trust in Explanations of AI Algorithms}
Explanation algorithms help us determine whether to trust AI algorithms, but only if we trust the explanation methods. But when can we trust an explanation algorithm? And what are we trusting it to do? The answer that we are trusting these algorithms to \emph{explain} AI systems is insufficient, because what it means to explain is unspecified. AI explainers can be put to a number of different uses, and different algorithms should be trusted for different uses; contracting to behave appropriately in all of these uses is infeasible (an end-user demands a different explanation than a domain expert), so explanations methods should contract to provide only a particular type of explanation.

Much like AI models themselves, we contend that xAI algorithms should be trusted to uphold specific contracts with respect to the ways in which they are used. For example, a model like \textit{recourse}, developed in \textcite{ustun_actionable_2019}, designed to informs end-users of what must be done to change their determination, should not be trusted to report errors in model or to point out the most important features. Similarly, a model like \textit{Scoped Anchors}, developed in \textcite{ribeiro_anchors_2018}, designed to simplify predictions into rule-based approximations, should not be trusted to provide recourse information.

We also contend that xAI methods should be evaluated on whether they can be trusted to do what they say. That is, a good xAI method is one that fulfills its intended use case. Much like trust in AI algorithms, trust in explanations of AI algorithms is contractual; xAI methods should be evaluated in terms of the extent to which they uphold the terms of a contract between the explainer and explainee; and an explainee's trust should be calibrated accordingly. We should not trust an explanation algorithm to do something it has not promised to do. 

The absence of contracts is not a mere conceptual problem; it creates a problematic dialectic and hinders effective critique of xAI methods. To demonstrate this, we consider two particular kinds of AI explanations: SHAP explanations, introduced in \textcite{lundberg_unified_2017}, and counterfactual explanations, introduced in \textcite{wachter_counterfactual_2017}. Both papers focus on the mathematical properties of the explanation algorithm introduce, but neither makes clear what they contend a good explanation consists in or specifies a circumscribed set of use cases for their methods. We consider two evaluation articles: \textcite{kumar_problems_2020}'s evaluation of the SHAP method, and \textcite{barocas_hidden_2020}'s evaluation of counterfactual explanations. Both articles rely on similar notions regarding the purpose of explanations – frameworks that the authors of SHAP and counterfactual explanations do not make clear that they subscribe to. For instance, \textcite{barocas_hidden_2020}'s critique counterfactual explanations on the grounds that they are not useful in providing users with actionable information. Similarly, \textcite{kumar_problems_2020} argue that SHAP cannot be used to inform users' actions.

We contend that, like trust in AI algorithms, trust in AI explanation algorithms is composed of an ability to anticipate the algorithm and an acceptance of vulnerability to the algorithm's actions. In both cases, this trust is only desirable if it is warranted. The scope of the trust, in both cases, should be clearly enumerated in a contract, and AI and explanation algorithms alike should be evaluated for trustworthiness within this scope.

\section{Design Recommendations}
Throughout this thesis, we explore three case studies considering different applications of AI systems as DSTs to tasks in talent identification. In each case study, we discover unique findings implying specific design recommendations for the development of AI systems as DSTs. However, three main principles emerge in many of the case studies. These principles are: human-centrism, trustworthiness through transparency, and the qualitative / quantitative (QQ) balance.

\subsection{Human-Centrism}
The \emph{human-centrism} principle states:

\begin{quote}
    Human-Centrism: Humans should always be in the loop. Most obviously, humans should have final say. Less obvious, humans should have design input.
\end{quote}

Human-centrism is often held in high importance in the field of human-computer interaction \cite{findasource}. Moreso in the context of AI systems. However, while it is often important to center humans in the application of an AI system, the nature of AI models often means they are developed before the user or use-case is considered \cite{findasource}. This, we note, is at direct odds with our principle of human-centrism. We recommend instead that, at least in the high-stakes context of talent identification, stakeholders should be involved in all stages of the AI system's lifecycle, from design to deployment.

The importance of user design input is made clear when we consider the idiosyncratic needs of different talent investment programs. As we find in Chapter \ref{ch:usingxai}, different selectors have different needs from explanations: some prefer visual explanations, while others prefer verbal explanations; some are pre-primed with a desire to engage deeply with the information provided, others are primed to index decisions on a single factor. Similarly, as we find in Chapter \ref{ch:iaicasestudy}, different selectors have different preferences around who to select: some place special importance on the roles applicants play in groups, others place a special emphasis on diversity metrics; some consider the applicants who preform the best on tests to be the most talented, others consider the applicants who preform the best in projects or interviews to be the most talented. While systems can be designed to accommodate these differences, this must be understood early in the design process.

\subsection{Trustworthiness Through Transparency}
The \emph{trustworthiness through transparency} principle states:

\begin{quote}
    Trustworthiness Through Transparency: When possible, AI systems should be trustworthy. When not possible, they should reveal through transparency their untrustworthiness.
\end{quote}

The explainable AI community places great importance on discussions of trust and trustworthiness \cite{jacovi_formalizing_2021,vereschak_how_2021}. While it is well-understood that explanation systems impact user trust in AI outputs, the best means to utilise this impact is still debated. Some contend a goal of explainability should be to increase user trust in AI, while others claim it should modulate it \cite{jacovi_formalizing_2021,kruger_problem_2022}. We contend that the goal of explainability should be to increase warranted trust in AI systems. This is to say, users should trust AI systems when they are trustworthy, and distrust them when they are not.  This is especially clear in the context of a DST, where a user distrusting the tool when it is incorrect brings about better overall task performance. In the context of talent identification, where the stakes are high, it is especially important that users trust the AI system only when it is trustworthy. Thus, we contend, AI systems should be trustworthy whenever possible, and when not possible, they should reveal through transparency their untrustworthiness.

Less often discussed is the role of trust in explanation in this context. Concerningly, in Chapter \ref{ch:xailimitations}, we find that popular explanation algorithms, naively applied, often fail to properly modulate trust. However, we also find in Chapter \ref{ch:usingxai} that these algorithms can be modified to better modulate trust. We recommend that, when designing an AI system as a DST, developers should consider the impact of explanation on user trust, and design their explanation systems to modulate trust in a way that is warranted. For explanations, too, should only be trusted when they are trustworthy.

In Chapter \ref{ch:usingxai}, we implement SHAP explanations for the purposes of providing an understanding of relative score importance in an applicant scoring algorithm. We find that SHAP explanations can be used to effectively help users understand the relative importance of different subscores. However, in this case, we find that selectors modulate their trust in bot the scoring algorithm and the SHAP scores using a variety of qualitative factors. They do not use the SHAP explanations to modulate trust in the overall algorithm score. Thus, we recommend the use of SHAP explanations here, but only to understand the importance of the relative subscores.

Conversely, in Chapter \ref{ch:studentuse}, we explore an implementation of a generative AI detector as a means of providing selectors a system to flag or disqualify applicants who rely on generative AI systems to write their essays. While this detector here is a sort of explanation of AI-generated essay text (it explains whether the text was generated by an AI system), it is not the sort of explanation that should be used to modulate trust in the student essay. Unlike in Chapter \ref{ch:usingxai}, where selectors modulate their trust in the algorithm score using a variety of qualitative factors, selectors cannot modulate trust in this detector using other factors. Thus, we recommend against the use of this detector here.

\subsection{Qualitative / Quantitative (QQ) Balance}
The \emph{QQ balance} principle states:

\begin{quote}
    QQ Balance: Balance the quantitative and the qualitative. Human decision-makers often require both.
\end{quote}

Many authors in the talent identification literature make mention of something like the QQ balance principle, but we find that it is often not well-understood. In Chapters \ref{ch:usingxai} and \ref{ch:iaicasestudy}, we find that selectors often require both quantitative and qualitative information to make decisions. When quantitative information is neglected, selectors are forced to make decisions on a case-by-case basis without important numerical context comparing applicants to a larger group; when qualitative information is neglected, selectors are unable to consider applicants holistically, and are reduced to making decisions on a spreadsheet. We recommend that developers of AI systems as DSTs consider the balance between quantitative and qualitative information in their systems, and design their systems to provide both when necessary.

\section{Contributions}
In this thesis, we make a number of theoretical, technical and practical contributions.

The theoretical contributions are:

\begin{itemize}
    \item We articulate where talent identification experts should use IAIDSTs to assist in their decision-making, and which IAI systems are appropriate for which purposes
    \item We apply research-through-design methods to talent identification to produce a novel approach to the creation of human-centric selection tooling
\end{itemize}

The technical contributions are:

\begin{itemize}
    \item We develop a novel method for visualising and understanding cohort-level decisions surrounding diversity called the Selection Possibility Frontier
    \item We further implement a novel algorithm for approximating the Selection Possibility Frontier
    \item We produce several qualitative, quantitative, and mixed-modal datasets relating to talent identification
\end{itemize}

The practical contributions are:

\begin{itemize}
    \item We demonstrate a yet-undemonstrated risk associated with naive application of post-hoc explainability to a range of problems
    \item We demonstrate the utility and limitation of detection tools for generative-AI-written essays in a scholarship context, and explore implications for scholarship programs
\end{itemize}

\section{Thesis Structure}
In Chapter \ref{ch:background}, we provide a background on the field of talent identification, the field of explainable AI, and the field of diversity, equity, and inclusion; we further explore intersections between these fields of research and identify gaps in the literature. Chapters \ref{ch:xailimitations} and \ref{ch:usingxai} contain our first case study examining the limitations and uses of post-hoc xAI. In Chapter \ref{ch:xailimitations}, we explore the limitations of popular xAI methods in the context of talent identification. In Chapter \ref{ch:usingxai}, we explore the use of SHAP explanations in the context of talent identification. Chapters \ref{ch:iaicasestudy} and \ref{ch:spf} contain our second case study investigating a user-centric design approach using more intrinsically interpretable models. In Chapter \ref{ch:iaicasestudy}, we present a participatory design study of the development of an IAIDST for talent identification. In Chapter \ref{ch:spf} we implement one of the designs from Chapter \ref{ch:iaicasestudy}. Chapter \ref{ch:studentuse} contains the entirety of our third case study, and explores the use of generative AI detection tools as DSTs in the context of talent identification. In Chapter \ref{ch:discussion}, we discuss our design recommendations, note limitations of our approach, and conclude.

\section{Publications}

\begin{itemize}
    \item \textcite{natarajan_trust_2023} Trust Explanations to do What They Say
    \item \textcite{...} `You Trust Me, But Should You?': Misleading Explanations of AI Outputs
\end{itemize}