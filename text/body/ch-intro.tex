% \begin{savequote}[8cm]
% \textlatin{Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

% There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...
%   \qauthor{--- Cicero's \textit{de Finibus Bonorum et Malorum}}
% \end{savequote}

\chapter{\label{ch:intro}Introduction} 

\minitoc

% To-do: rewrite

Since early theorising about Artificial Intelligence (AI), experts have noted the transformative potential of such technology. Now, with the popular advent of Machine Learning (ML) models and transformer architecture in particular, we see rapid mass-market adoption of a variety of AI tools. In many fields which deal with sensitive decision-making, this adoption introduces new ethical challenges. In the fields of talent and Talent Identification (TI) in particular, many of these ethical challenges centre on decision tool interpretability; diversity, equity, and inclusion (DEI); and applicant usage of generative AI. In this section, we lay out the case for the application of AI systems (specifically interpretable AI decision assistants) to these issues in TI, paying special attention to the potential pitfalls and broader implications of our proposal.

\section{Abstracts from Papers} %to-do: replace this with something more coherent.

\subsection{GenAI}
Applicants increasingly use generative AI (genAI) to write essays for universities, jobs, and scholarships. While genAI detectors serve as anti-plagiarism tools, these detectors don’t fully address the challenges faced by selection. Through Action Research (AR) with scholarship selection practitioners, we identified key decisions surrounding genAI, (e.g., whether and how to ban their use in application or broader revisions to application processes). We developed a framework for categorising these decisions and assessed two popular detectors, Originality.ai and GPTZero. Although neither detector fully meets the needs for reliably identifying and disqualifying AI-plagiarised essays, both can support ex-post process decisions. We applied GPTZero to evaluate genAI use across years and by channel partners (who source and support program applicants), finding an overall increase in genAI usage year-over-year but no evidence of widespread genAI use among channel partners. Our findings support the program’s channel partnership approach but suggest the need for new assessment methodologies.

\subsection{XAI}
Post-hoc Explainable AI (xAI) methods build trust in AI-based Decision Support Tools (DSTs), but critics warn they may induce decision-maker over-reliance on AI. We differentiate between two decision stages for using xAI: \emph{in-process}, where xAI informs primary decisions (e.g., whether to select a candidate), and \emph{ex-post}, where xAI informs second-order decisions (e.g., whether to change the candidate selection process). In an online study, we evaluate three xAI methods (SHAP, Anchor, and Confidence) \emph{in-process} and find that SHAP and Confidence induce unwarranted trust. We then conduct participatory design sessions presenting SHAP explanations to scholarship selection practitioners in the \emph{ex-post} context. These explanations can inform and support \emph{ex-post} decisions, but effective presentation and contextualisation is key. We conclude that, while critics’ warnings of xAI-induced misplaced trust hold \emph{in-process}, this misplaced trust is not unique to post-hoc xAI. Furthermore, post-hoc xAI methods, even if misleading \emph{in-process}, can still be valuable \emph{ex-post}.

\subsection{Diversity}
When selecting applicants for scholarships, universities, or jobs, practitioners often aim for a diverse cohort of qualified recipients. However, differing articulations, constructs, and notions of diversity prevents decision-makers from operationalising and progressing towards the diversity they all agree is needed . To understand this challenge of translation from values, to requirements, to decision support tools (DSTs), we conducted participatory design studies exploring professionals' varied perceptions of diversity and how to build for them. Our results suggest three definitions of diversity: bringing together \emph{different perspectives}; \emph{ensuring representativeness} of a base population; and \emph{contextualising applications}, which we use to create the \emph{Diversity Triangle}. We experience-prototyped DSTs reflecting each angle of the \emph{Diversity Triangle} to enhance decision-making around diversity. We find that notions of diversity are highly diverse; efforts to design DSTs for diversity should start by working with organisations to distil `diversity' into definitions and design requirements.

\subsection{SPF}
We hypothesize that the complexity of selecting personnel in a way that jointly optimizes for talent (performance on an ability measure) and diversity impedes organizations from meeting their diversity, equity, and inclusion goals. To formalize this, we prove that maximizing cohort diversity is computationally complex and incorporate this complexity into a selection model by adding computational costs. To test the model's predictions, we construct an algorithm to estimate the diversity-talent frontier, which we apply to data from a scholarship and talent investment program. We find that finalists could have been 13\% (15.6\%) more diverse (more talented) without reducing talent (diversity). We also show that the program selected a significantly more diverse and talented cohort after we provided them with a frontier estimate. We conclude by using program data to demonstrate how the frontier estimation procedure can be used to evaluate the efficacy of alternative screening approaches. This reveals that if the program had screened on IQ, they would have significantly reduced diversity and overlooked many of the most talented applicants.

\section{Motivation}
(To-do) This is where I explain why I'm doing this research. I explain why care about talent identification and how IAIDSTs can solve problems here. I respond to the notion that we should simply not use AI here, and highlight problems in the field already. I explain why this is a problem for Computer Science as well as for Talent Science (tl;dr: additional reasons to care about things xai overlooks because of the application domain)

\section{Contributions}
In this thesis, we make a number of theoretical, technical and practical contributions. These are:

\begin{itemize}
    \item We articulate where talent identification experts should use IAIDSTs to assist in their decision-making, and which IAI systems are appropriate for which purposes
    \item We apply research-through-design methods to talent identification to produce a novel approach to the creation of human-centric selection tooling
    \item We develop a novel method for visualising and understanding cohort-level decisions surrounding diversity called the Selection Possibility Frontier
    \item We further implement a novel algorithm for approximating the Selection Possibility Frontier
    \item We produce several qualitative, quantitative, and mixed-modal datasets relating to talent identification
    \item We demonstrate a yet-undemonstrated risk associated with naive application of post-hoc explainability to a range of problems
    \item We demonstrate the utility and limitation of detection tools for generative-AI-written essays in a scholarship context, and explore implications for scholarship programs
\end{itemize}

\section{Thesis Structure}
In this chapter, we explore our motivation for focusing on the application of IAIDSTs to talent identification, enumerate our contributions, and provide an account of challenges talent identification. Among these challenges, we focus the remainder of this thesis on three: limited human understanding of the AI tools, limited ability to consider diversity, and the risks of applicant usage of generative AI.

In Chapter \ref{ch:background}, we provide a background on the field of talent identification, the field of explainable AI, and the field of diversity, equity, and inclusion; we further explore intersections between these fields of research and identify gaps in the literature. Chapters \ref{ch:xailimitations} and \ref{ch:usingxai} explore our first challenge: limited human understanding of the AI tools. In Chapter \ref{ch:xailimitations}, we explore the limitations of popular xAI methods in the context of talent identification. In Chapter \ref{ch:usingxai}, we explore the use of SHAP explanations in the context of talent identification. Chapters \ref{ch:iaicasestudy} and \ref{ch:spf}  address challenges related to consideration of diversity in TI. In Chapter \ref{ch:iaicasestudy}, we present a participatory design study, which aims to use IAIDSTs to help TI professionals account for diversity. In Chapter \ref{ch:spf} we implement one of the designs from Chapter \ref{ch:iaicasestudy}. Chapter \ref{ch:studentuse} attempts to mitigate the risks of applicant usage of generative AI. It explores the use of generative AI detection tools as DSTs in the context of talent identification. Finally, in Chapter \ref{ch:discussion}, we discuss our design recommendations, note limitations of our approach, and conclude.

\section{Publications}

\begin{itemize}
    \item \textcite{natarajan_trust_2023} Trust Explanations to do What They Say
    \item \textcite{citation needed} `You Trust Me, But Should You?': Misleading Explanations of AI Outputs
\end{itemize}