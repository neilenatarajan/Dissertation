% \begin{savequote}[8cm]
% Alles Gescheite ist schon gedacht worden.\\
% Man muss nur versuchen, es noch einmal zu denken.

% All intelligent thoughts have already been thought;\\
% what is necessary is only to try to think them again.
%   \qauthor{--- Johann Wolfgang von Goethe \cite{von_goethe_wilhelm_1829}}
% \end{savequote}

\chapter{\label{ch:discussion}Discussion}

\minitoc

% to-do: A discussion would say "here's what I learned, here's what I'm not sure about, here's limitations, this is what makes this domain hard or complicated, this is how it relates back to debates and paradigms in HCI..." Read through to ensure this says that!!

\section{A Reflection on the Nature of this Thesis}
In researching for and writing this thesis, I have come across a number of compelling arguments to \emph{not} do this research. Taken together, rather than placing limits on the scientific achievements of this work, they question its place in the broader whole. These arguments are, in some cases, compelling. Thus, in reflecting on the work I present here, I find it prudent to offer an apology, both in regret and defense, of this work.

In a seminal provocation piece, \textcite{Barocas_Hood_Ziewitz_2013} question whether algorithms challenge or enforce existing moral codes; I answer both. In designing AI systems for scholarship selection processes (often funded or administered by the most powerful people in the world), I contribute to an already vast power disparity between the world's richest and poorest. Scholarships such as those offered by Schmidt, MacArthur, Marshall, or Rhodes, while providing opportunities for individuals from disadvantaged backgrounds, serve to entrench their funders in institutional power structures. By working within these structures and aiming to optimize their selection processes, I am, to some extent, complicit in maintaining these power dynamics \cite{Ziegler_2008}. It would be disingenuous to claim that my work fundamentally challenges the goals or existence of these institutions. Instead, I seek to improve the processes within the existing framework, capitalising on the expressed goodwill of these institutions to improve the lives of disenfranchised groups of people from around the world – in doing so, I build systems challenging the social allowance of intellectual poverty, but simultaneously enforce the socioeconomic positions of the wealthy few.

% “It’s an oxford thesis” is also part of the apology.  is part of the apology. In the context of a scholarship thing, it’s worth saying that we are entrenched in elitism. We are working with the decision-makers and trying to help them optimise for their stated goals. Someone should challenge the goals of these institutions, or perhaps even their existence. We don’t. It would be disingenupus to work with Larry Ellison and Eric Schmidt and the Rhodes Trust, publish at Ox, then say that we’re learnin about underrepresented minorities from their perspective. Rather, we are working with them and situating ourselves within their decision strucuters. We’re trying to make these processes better for society, but we are working within the structure.

% We are not trying to undermine the power or authority o these institutions. We are capitalising on expressed goodwill that these institutions have and trying to make it better for society. This is not critical theory, and I know billionaire philanthropy is inherently bad.

%  - critical attention on algorithms Kitchin_2017
% I would be remiss to ignore the critical attention that algorithms have garnered as they proliferate through society. NISSENBAUM1998237 

% something about \textcite{Warikoo_2019} maybe?

% to-do: discuss how this all relates to fairml. This won't be in the overall literature review chapter, so it should be mentioned here. Find the work at facct on recruitment and relate it here.

\section{The Role of AI Systems in Selection: From Decision Support to Selection-Oriented AI}
\subsection{Current Challenges in AI for Selection}
AI tools have long been posed as both replacements and support systems for decision-makers both without and within selection processes \cite{barocas_big_2016,jacobs_how_2021,hildebrandt_law_nodate,yarger2020algorithmic,mattu_how_nodate}. Naively, implementations of AI systems supplant human decision-makers, often to disastrous effect \cite{mattu_how_nodate}.

More human-centric AI systems, e.g., explainable AI (xAI), often instead serve as decision support tools (DSTs), and place the stakeholder (the selection practitioner) at the core of the decision-making process. These systems then focus on enhancing the experience of human decision-makers, offering tools that satisfy the subjective desiderata of selection practitioners. \textcite{Lipton} critiques post-hoc notions of explainability on the grounds that, in seeking to satisfy stakeholder desiderata, xAI tools may prove more misleading than insightful. While Chapter \ref{ch:xai} extends this critique to the practice of post-hoc justification more broadly, this human-centric paradigm of development threatens to extend the critique to all such systems.

The problem of selection involves a complex interplay of interests. On the one hand, selection organizations are driven by the need to identify the best possible cohort of candidates, aiming to optimize organizational performance and success. On the other hand, candidates are motivated by the desire to be selected for opportunities that match their potential and aspirations. In practice, these two interests are at odds, and while DSTs might support one or the other, the needs of supporting both decisions outstrips the capacity of any individual DST. % to-do: cite

But beyond these two core stakeholders lies a broader societal interest in the outcomes of selection processes. Society at large has a vested interest in ensuring that these processes uphold essential social values such as fairness, diversity, integrity, and justice. In the case of pro-social programs such as global scholarships who seek for their scholars to do good with their careers, society similarly has an interest in ensuring that the most apt scholars are selected.

By centring these DSTs solely on the needs either applicants or of decision-makers within organizations, we potentially compromise the integrity of the selection process and fail to address the larger social implications of these selection decisions. E.g., algorithms designed to streamline applicant evaluations may inadvertently reinforce existing biases, thereby undermining efforts to promote diversity and inclusion. Similarly, AI systems may prioritize ease of use and decision-making speed at the expense of fairness and transparency. Thus, there is a need for DSTs that, rather than centring on a group of stakeholders, orients itself around the task of selection itself.

\subsection{Proposing a New Paradigm: Selection-Oriented AI (SOAI)}
In response to these challenges, this thesis proposes a novel paradigm: SOAI. SOAI reimagines the role of AI systems in talent identification, and advocates for a shift away from the human-centric framework toward a values-centered approach (wherein the design of AI systems is grounded in the social values that selection processes ought to uphold). While selection practitioners remain important users of these systems, they are not the sole focus of design efforts. Instead, SOAI emphasizes the importance of evaluating the broader social impact of AI-driven selection processes, with particular attention to how these processes align with principles of fairness, diversity, and justice \cite{batyavalue}.

The shift toward SOAI represents a necessary evolution in the use of AI systems in talent identification. By prioritizing the social values that selection processes ought to reflect, SOAI challenges the current practitioner-centered approach and introduces a new standard for evaluating the success of AI in decision support. The implementation of SOAI can provide a path forward in creating AI systems that not only assist in the identification of talent but also ensure that the processes by which talent is selected are fair, just, and inclusive.

\section{Design Recommendations for SOAI Designers}
\subsection{Design for Specific Social Values}
While we wish to design to support all social values in the selection process, Chapter \ref{ch:diversity} demonstrates the difficulty of unpacking the social value of diversity; in Chapter \ref{ch:diversity}, we find success instead focusing on smaller component values that comprise diversity. Similarly, each decision point supported in Chapter \ref{ch:genai} implies a specific ontology about the role of generative AI in selection; these, too, stem from specific social values promoted by an organisation.

There is support for this from the literature. For example, literature on algorithmic fairness has long wrestled with contradictions between measurements of different kinds of fairnesses \cite{pmlr-v80-kearns18a}. While `individual fairness' draws on procedural notions of justice to ensure that applicants are treated equally regardless of differences in protected or irrelevant characteristics \cite{dwork_fairness_2012}, `group fairness' draws on distributive justice in seeking to achieve parity between different demographic groups \cite{Citron_2008,Olsaretti_2018}. There exists literature attempting to reconcile these notions: \textcite{pmlr-v28-zemel13} attempt to reconcile this in practice by simultaneously optimising for multiple fairness metrics; \textcite{lahoti2019ifairlearningindividuallyfair} seek only to optimise for individual fairness, and yet find in-practice increases in group fairness; and \textcite{binns_apparent_2019} contend that standard, blunt implementations of individual fairness should be replaced with a more nuanced formulation compatible with group fairness. Nonetheless, as they are often implemented, these two notions of fairness are often in conflict, and though designing to support both may be possible, it is liable, in a scholarship context, to create unclarity of the sort plaguing diversity, impeding program desire to assess these concepts with specific targets. 

We suggest this generalises to SOAI practices in general: rather than designing around myriad values, only to find conflicting design implications of these disparate values, designers seeking to support social values in selection processes should focus on specific social values worthy of consideration.

\subsection{Identify Decision Points with the Decision Matrix}
In Chapter \ref{ch:context}, we conceive of selection as a series of decisions. We introduce the Decision Matrix framework to categorise the many decision points that selection practitioners face according to their two most germane axes: the stakes of the decision and its stage in selection. This framework allows designers to reason about groups of decision points in much the same way that the explainable AI community reasons about groups of explainability techniques \cite{ford_play_2020,kumar_problems_2020,doshi-velez_towards_2017,friedrich_taxonomy_2011,molnar_interpretable_2019}. In Chapter \ref{ch:xai}, we respond to criticisms isolated to \textcite{friedrich_taxonomy_2011}'s `post-hoc' explanations; here, the taxonomic distinctions are used in criticism to expand the scope of individual critiques \cite{barocas_hidden_2020,kumar_problems_2020}. We suggest the Decision Matrix can be used similarly, to discuss and critique decisions in a scholarship context.

But Chapter \ref{ch:genai} extends the Decision Matrix framework beyond merely a taxonomy of decision points. We use the Decision Matrix to isolate desired or required properties of DSTs based on the taxonomy of the decision point they seek to support, and to then determine which categories of decisions different genAI detectors are suitable to support. In this way, we suggest designers use the Decision Matrix to not only categorise decision points, but to also guide the design of DSTs to support these decisions.

Finally, we caution designers following this design recommendation to ensure to to also follow design recommendation \ref{ssec:real_change}, as the Decision Matrix framework provides a necessary, but perhaps not sufficient, set of properties.

\subsection{Balance Qualitative and Quantitative Information in Presentation}
Human decision-makers often desire both a qualitative understanding of applicants and quantitative metrics to compare them. In Chapters \ref{ch:xai} and \ref{ch:diversity}, we find that selection practitioners from \rise and \eit seek to make decisions informed by both kinds of information; despite this, the desired balance between these modes of information varies based both on practitioner and type of decision. When quantitative information is neglected, practitioners are forced to make decisions on a case-by-case basis without important numerical context comparing applicants to a larger group; when qualitative information is neglected, practitioners are unable to consider applicants holistically. Developers following SOAI should consider the balance between quantitative and qualitative information in their systems, and design their systems to provide both when necessary.

We again find parallels in the fairness literature to this balance. Qualitative information enables the selectors considerations of applicants as individuals, and this combines with the process of holistic review to create full pictures of applicants. \cite{dwork_fairness_2012}'s individual fairness holds a similar lens; rather than looking at applicants in terms of their place in the cohort, this notion of fairness demands equal treatment of applicants as people. However, quantitative information makes possible considerations of distributive notions of justice and group fairness principles \cite{Olsaretti_2018}, as decision-makers have access to the supporting information needed to contextualise applications relative to other members of protected groups. Notably, programs with different ontologies governing what they consider fair will thus have different preferences considering the balance of quantitative and qualitative information in their systems. (This relationship is not absolute, though, as other differences in programs may lead to differing priorities.)

\subsection{Evaluate Real Change in Addition to Subjective Satisfaction}\label{ssec:real_change}
\textcite{Lipton} critiques explainable AI (xAI) systems on the grounds that they risk satisfying the subjective desires of the users while failing to improve objective outcomes. In Chapter \ref{ch:xai}, we confirm that this critique applies to some post-hoc justifications of model recommendations, as the justifications were found to yield an unwarranted increase in trust in the human decision-makers. Thus, it is important to define and evaluate measures of the social values that DSTs intend to support; when evaluating these DSTs, they should not be evaluated human-centrically (i.e., according to their users' satisfaction), but should instead be evaluated on whether their employment improves social outcomes.

The fundamental challenge with evaluating ``real change'' in a selection context is the lack of ground truth; i.e., there are not, in general, ``correct'' or ``incorrect'' selection decisions, only those preferred by the organisation. In this thesis, we solve this problem by working with programs to define measurable criteria that act as a surrogate for ``correct selection decisions''. In Chapter \ref{ch:xai}, these criteria are arrived at through an Action Research (AR) process and expressed in Figure \ref{fig:desiderata_matrix}, while in Chapter \ref{ch:spf}, these criteria are supplied directly by \rise, as the program has its own internal metrics for both axes of the SPF. We recommend designers work with organisations to define surrogate criteria that can be used to evaluate the success of their systems.

\section{Implications}
\subsection{Algorithmic Fairness in a Selection Context}
The work in this thesis has implications for the broader discussion of algorithmic fairness in selection processes. The design of SOAI DSTs have the potential to impact the lives of many of the world's most vulnerable people; it is thus imperative that these systems are designed fairly. But the notion of fairness itself is complex and multifaceted. As \textcite{pmlr-v80-kearns18a} highlight, fairness can be understood in both procedural and distributive terms, and different methods of achieving fairness across different subgroups often conflict. A common form of procedural fairness, especially in the algorithmic fairness literature, is individual fairness \cite{dwork_fairness_2012}; meanwhile, this is often contrasted with the distributed ``group'' fairness \cite{fleisher_whats_nodate,binns_apparent_2019,barocas2023fairness,Friedler_Scheidegger_Venkatasubramanian_2016}. Despite attempts to reconcile these differing notions of fairness, such as those by \textcite{binns_apparent_2019}, contradictions remain between metrics used to measure different forms of fairness; that is, decisions that may be ruled more fair by certain individual or procedural fairness measures might create group unfairness. What's worse, scholars disagree even on the best implementations of distributive notions of fairness \cite{Friedler_Scheidegger_Venkatasubramanian_2016}, and differing interpretations conflict. 

It is worth noting, then, that the work on generative AI detection in Chapter \ref{ch:genai} is built on a desire for procedural fairness, while the diversity goals of Chapters \ref{ch:diversity} and \ref{ch:spf}, in practice, accord closely with distributive notions. This raises the possibility that, via SOAI methods, researchers could determine socially beneficial fairness metrics to uphold in DSTs, and build to support those.\footnote{This work, in particular, should not be done solely from the decision-maker's perspective. \textcite{10.1145/3351095.3372867} investigate applicant perceptions of appropriate fairness metrics; this work may be a good starting point for SOAI work in this field.}

\subsection{New Developments in AI for Selection}
The growing popularity of genAI has already dramatically increased the number of applications that job, university, and scholarship programs must select from \cite{Kaashoek2024Impact}. While a blanket ban on genAI in application-writing may solve this \cite{h_holden_thorp_chatgpt_2023}, we find in Chapter \ref{ch:genai} that such a ban is unenforcible at present. We note in Chapter \ref{ch:genai} that our research is complicated by the rapidly changing nature of both genAI and detectors. Here, we extend this complication to SOAI as a whole. It may be that, as genAI development moves beyond retrieval-augmented generation to more complex architectures \cite{lewis2020retrieval}, such as integrated reasoning systems or agentic AI \cite{Shavit_O’Keefe_Eloundou_McMillan_Agarwal_Brundage_Adler_Campbell_Lee_Mishkin_et}, these systems will once again fundamentally change the process of selection. In light of this, SOAI is necessary to ensure that new, more powerful AI systems further the social aims of selection processes.

Of particular interest would be the development of AI systems capable of encoding domain knowledge in their structures, which could support decisions in a fundamentally different way. This could be particularly useful in the AES domain, where domain-specific knowledge is a significant problem \cite{elijahthesis}. If this is the case, then the work done in this thesis may serve as a precursor to the development of these systems, and a guide for how to ensure that these systems are designed to support the social aims of selection processes.

% maybe relate this to other HCI discussions. If someone else is doing AR in a different area, is there anything I've done that shows strengths or weaknesses of those areas. What relates to HCAI research / value centred design / policy stuff?
% maybe there are also implications for either VSD or PD?

\section{Limitations}
In scenarios outside the selection context, human-computer interaction (HCI) research has generally sought to harmonize the conflicting needs of different user groups through participatory design. Yet, in the case of talent identification, the very nature of the selection process introduces a conflict of incentives between applicants and the selection organization. Applicants are primarily motivated by the desire to be chosen, while selection organizations, guided by practitioners, seek to optimize the selected cohort according to (biased) measurements of (sometimes idiosyncratic) organisational preferences. This inherent misalignment of objectives raises significant concerns about how to balance the needs of both groups without compromising the overarching social values that SOAI seeks to uphold. Our solution in this thesis has been to centre the organisations, and use data from past selections to evaluate their selection decisions based on stated and elicited preferences. However, this solution positions research from the position of the decision-makers, and a lack of engagement with decision subjects may limit the social benefit of the research.

Central to this solution is the notion that the broader social aims of the selection process (e.g., ensuring fairness, diversity, social benefit) both align more closely with practitioners than with applicants and ultimately supersede the immediate interests of both practitioners and applicants. However, we note that some overlap exists between the broader social goals of the selection process and the scholarship applicants' goals. We note that work already exists exploring this from a decision-subject standpoint; \textcite{10.1145/3351095.3372867} find that applicants prefer algorithmic decision-making to human decision-making according to both procedural and distributive notions of fairness. In light of this, SOAI's positioning as a paradigm for building DSTs, rather than a paradigm for building algorithmic decision-making systems, limits the scope of the research. Were it the case that algorithmic decision-making is preferable to algorithmically-supported human decision-making, the SOAI paradigm would not be the most effective way to achieve the social aims of selection.

Setting aside the social benefit of the SOAI paradigm as a whole, specifics of SOAI, as it is implemented in this thesis, may limit the work done here. The Decision Matrix framework (explained in Chapter \ref{ch:context}, hinted at in Chapter \ref{ch:xai}, and discovered through Action Research in Chapter \ref{ch:genai}) intentionally elides distinctions between decision points in favour of clarifying a focus on a decision's \emph{stage} and \emph{stakes}. This elision was arrived at in concert with participants from \rise, but it may not extend to other decision-making contexts. If it doesn't, though we still call for SOAI work in these contexts, our work may not be directly applicable.

\section{Future Work}
While the work in this thesis articulates SOAI as a paradigm for all AI design oriented around supporting selection problems, we develop and test this paradigm with respect to three specific families of decision points. A straightforward extension of this work would apply SOAI principles to other decision points in selection processes. Natural candidates include: supporting essay judgements with Automated Essay Scoring (AES), where a large body of literature already seeks to score applicant essays via algorithm \cite{cozma_automated_2018,ramesh_automated_2022,wang_use_2022,elijahthesis}, but automated approaches continue to struggle with marking top or bottom essays \cite{elijahthesis}; supporting testing and test evaluation with automated scoring systems \cite{organisciak_beyond_2023,condon2014international}; and supporting pre-application portions of the outreach process, which was requested by several participants in Chapter \ref{ch:diversity}.

Though SOAI, as we investigate it here, aligns most closely with the interests of selectors, there is a need for human-centric work seeking to determine applicant perceptions of positive social outcomes. While work exists examining applicant perceptions of decisions made about them, \cite{pandey_applicants_2022,horodyski_applicants_2023}, this work often approaches research from a fairness or decision-subject-empowerment perspective. No work exists approaching applicant perspectives from the perspective of the ultimate social benefit of selection. Future work should seek to understand how applicants perceive the social outcomes of selection decisions, and how these perceptions can be used to design more effective AI systems for selection.

Though the Decision Matrix provides a useful framework for categorizing decision points in selection processes, it is not exhaustive. Future work should seek to augment the Decision Matrix with additional axes that capture the complexity of selection decisions more fully. In particular, though selectors drew a distinction between individual- and group-level in-process decisions in Chapter \ref{ch:diversity} (and though the design prototypes reflect this distinction), the Decision Matrix does not capture this distinction. Future work should may seek to augment the Decision Matrix to distinguish individual- from group-level distinctions, and may seek to implement more individual-level decision support systems in practice.\footnote{Chapters \ref{ch:genai} and \ref{ch:spf} both avoid individual-level implementations in real decision-making pipelines due to risks associated with introduced unfairnesses or biases \cite{hartigan_fairness_1989,barocas2023fairness,pmlr-v80-kearns18a,Bastounis_Campodonico_vanderSchaar_Adcock_Hansen_2024,liang_gpt_2023}. Any work implementing tools at the individual-level should be consider these risks first.}

\section{Conclusion}
In this thesis, we pioneer a new paradigm of AI design for selection processes, Selection-Oriented AI (SOAI). Chapters \ref{ch:xai} and \ref{ch:genai} find that existing AI systems often fail to meet the needs of selection practitioners, particularly for in-process decision-making. In response, we propose a new paradigm, SOAI, which seeks to centre the design of AI DSTs not around the selection practitioners but around the social aims of selection they seek to practice. Chapters \ref{ch:diversity} and \ref{ch:spf} apply SOAI principles to design DSTs to support considerations of diversity in selection; we implement a prototype designed to satisfy selection practitioner desires and find that it improves both diversity and performance outcomes in selection. We then provide a set of design recommendations for SOAI designers, including focusing on specific social values, identifying decision points with the Decision Matrix, balancing quantitative and qualitative information, and evaluating real change in addition to subjective satisfaction.

More broadly, the use of SOAI to support scholarship-specific selection decisions implies the potential to support and improve related decision-making processes, from other selection contexts (e.g., admissions or hiring) to non-selection decision-making contexts (e.g., program outreach). With a technology-induced flattening of the world \cite{Friedman_2005}, more candidates from more parts of the world find themselves qualified for opportunities. Add to this the ease of application submission created by genAI assistants, and it is clear why applications to job, university, and scholarship opportunities have seen a dramatic increase in recent years \cite{Kaashoek2024Impact}. In light of this, conclude with a call for SOAI across selection contexts; the need has never been more pressing.