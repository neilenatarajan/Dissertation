% \begin{savequote}[8cm]
% \textlatin{Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

% There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...
%   \qauthor{--- Cicero's \textit{de Finibus Bonorum et Malorum}}
% \end{savequote}


\chapter{\label{ch:rw}Related Works} 
\minitoc

\section{Decision Support Tools for Scholarship Selection}
This thesis engages primarily with literature seeking to use AI tools to support decision-making in global scholarship selection processes. Unfortunately (or perhaps fortunately), this particular niche of literature is fairly sparse. While many tools do exist, especially those seeking to automate the job of the scholarship selector entirely, we are forced to look beyond this niche for a larger body of related literature. Primarily, we do this by considering other selection contexts (e.g., universities or recruiters). Note that while this body of literature is large, and contains work ranging from automated essay scoring to intellect testing \cite{cozma_automated_2018,condon2014international}, our work is only tangentially related to these fields. Thus, we instead focus primarily on existing attempts by explainable AI (xAI), generative AI detectors, and tools considering diversity, as these are the most relevant to our work.\footnote{Note that passages identifying work related to specific chapters appear again in those chapters.}
% to-do

\section{Explainable Artificial Intelligence}
\subsection{Types of Explainable Artificial Intelligence Publications}
The field of xAI can, broadly, be divided into four main bodies of research: reviews, methods, notions, and evaluations \cite{vilone_explainable_2020}. Review articles are either systematic investigations or literature reviews. Method articles introduce new explanation methods. Notion articles focus on defining notions or concepts related to explainability. Evaluation articles evaluate existing notions or paradigms.

\subsubsection{Review Articles}
Review articles offer meta-commentary on other articles listed below. A review article is so-characterised because its primary contribution is one of aggregation. Thus, a review article might also be a method, notion, or evaluation article, so long as it primarily draws from these articles. This paper is, primarily, a review article.

\subsubsection{Method Articles}
In most method articles, the task of explanation is taken to be fairly well-defined, and the research questions deal in how to most effectively generate that explanation. 

Consider two particular method articles: SHAP explanations, introduced by \textcite{lundberg_unified_2017}, and counterfactual explanations, introduced by \textcite{wachter_counterfactual_2017}. In both papers, we see heavy treatment of the mathematical properties of the explanation algorithm introduced. However, we see relatively light treatment of what the authors take to be a good explanation or proposed use cases for the novel methods. These concepts are instead the domain of notion articles.

\subsubsection{Notion Articles}
Where method articles deal with individual methods, notion articles deal with theory underlying a variety of methods. For example, \textcite{miller_explanation_2017} discussion of the importance of principles of explanation from the social sciences is a notion article.

\subsubsection{Evaluation Articles}
Evaluation articles consider which notions and methods are best-suited to produce the best explanations. Evaluations of method articles, in particular, often compare them to notions introduced elsewhere, and where critiques are found, they are often critiques that the method fails to meet a standard set by the notion.

For the two method articles discussed above, we present two evaluation articles: \textcite{kumar_problems_2020} evaluation of the SHAP method, and \textcite{barocas_hidden_2020} evaluation of counterfactual explanations. Both articles rely on similar notions regarding the purpose of explanations, and critique the methods for failing to adhere to a notion. For instance, \textcite{barocas_hidden_2020} assume that counterfactual explanations are given to provide users actionable information. Similarly, \textcite{kumar_problems_2020} argue that SHAP cannot be used to inform users' actions.

\subsection{A Taxonomy of XAI Methods}
AI model explanations can broadly be categorised into two subgroups: intrinsically interpretable models and post-hoc explanations \cite{molnar_interpretable_2019}. Though intrinsically interpretable models are often not classed as ``xAI'' (as they do not make use of developments in xAI), they do offer offer explanations \cite{molnar_interpretable_2019}, since they are already interpretable; thus, we include them here.

Post-hoc methods can be further separated into global methods, which explain entire models, and local methods, which explain individual decisions \cite{molnar_interpretable_2019}. When assessing explanation methods, one can distinguish between local tests and global tests of model explanations \cite{molnar_interpretable_2019}. Local tests concentrate on the impact of the explanations at the level of a single prediction, global tests focus on the impact of explanations on overall model trust.

Rather than delineating between intrinsic and post-hoc or global and local, we could instead delineate by output type \cite{friedrich_taxonomy_2011}. Some interpretability devices offer feature importance statistics. Others offer explanations by contrast to another datapoint. Yet others offer rules that guide decision-making in regard to an individual case case.

\subsection{Use Case}
Consider the following potential use cases for xAI methods: to evaluate whether the AI is operating as intended, to examine an AI for bias and unfairness, to provide recourse, to provide a rule that would guarantee equality. This list is not exhaustive, and yet, even for these three use cases, explanations that satisfy some will not satisfy others \cite{natarajan_trust_2023}.

\subsection{Theories of Explanation}
In deciding which type of explanation will best suit a particular use case, we must consider the theory of explanation that underlies the explanation, and the desiderata that that theory demands of its explanations.

\textcite{miller_explanation_2017} summarises key findings from Social Sciences related to explanation. \textcite{miller_explanation_2017} works from a causal theory of explanation; he outlines and examines the theory, noting challenges to it, but does not consider a rejection of the theory. According to the theory, explanations are sought in response to counterfactual cases, i.e. the question asked is not of the form "Why \textit{P}" but rather "Why \textit{P} instead of \textit{Q}" \cite{miller_explanation_2017}. Such counterfactual theories of causality all argue that cause is a matter of what would have happened if the cause had not happened. Though they disagree on precisely how to model these counterfactual cases, many leading philosophical theories of causality agree that causality is best understood in terms of counterfactuals. Hume modeled these counterfactuals in terms of causes and events: in actuality, cause $C$ caused event $E$ to occur; but if cause $C'$ had replaced cause $C$, $E$ would not have occurred. We can call this sort of counterfactual a cause-counterfactual \cite{miller_explanation_2017}. Under this conception, explanations should present cause-counterfactuals as clearly as possible.

\textcite{miller_explanation_2017} also argues that explanations should be contrastive and selective. Contrastive explanations make use of counterfactuals, but these counterfactuals are not cause-counterfactuals. Instead, they are event-counterfactuals. In other words, contrastive explanations consider alternative events $E'$, and explain the data $E$ in relation to $E'$. Selective explanations, on the other hand, are ones that do not consist of a complete cause, but rather select one or two causes from the complete cause. 

\textcite{miller_explanation_2017} presents a powerful application of a theory of explanation to the field of xAI. However, causal theories of explanation are far from the only type of explanatory theories. Indeed, as we noted above with numerical evidence, there are theories of explanation who's desiderata contradict those of \textcite{miller_explanation_2017}'s causal theory \cite{woodward_scientific_2021}. 

In what follows, I present \textcite{woodward_scientific_2021}'s theory, the statistical relevance model of explanation. The statistical relevance theory rests heavily on the notion of statistical relevance. For events $A$, $B$, and $C$, we say $C$ is statistically relevant to $B$ in $A$ if and only if $P(B | A \land C) \neq P(B | A)$. I.e., an explanation is a member $C_i$ of a homogeneous partition $C$ of properties: that is, a set of properties that are exclusive and exhaustive of $A$, where there are no statistically relevant properties $D$ to $B$ in $A \land C_i$. The explanation also consists of $P(B | A)$, the probability $P(B | A \land C_i)$ of each cell withing the partition, and which of the $C_i$ contains the desired point to be explained, $x$ \cite{woodward_scientific_2021}.

On both accounts , explanations have an explainer and an explainee. Explanations in everyday use are an interaction. Unlike most modern algorithmic explanation methods, an explanation given by a human is often given in the form of a conversation. Thus, it obeys the rules of communication using language. In other words, these explanations are social. We may therefore apply theories governing social interactions, such as \textcite{Grice_1975}'s conversational maxims, to explanations \cite{miller_explanation_2017}.

\textcite{Grice_1975}'s four categories of conversational maxims are: quality, quantity, relation, and manner. That is, the quality of information conveyed in a cooperative conversation should be high (information should be likely and justifiable). The quantity of information should be neither too little nor too much. The information should be related to the conversation. The information should be conveyed in an appropriate manner. Note that the quantity bounds overlap quite heavily with selectivity.

\subsection{Trust}
Another important notion related to xAI is trust. Indeed, much of the xAI literature suggests that the purpose of xAI is to increase trust in AI. This is apparent from the titles of canonical papers in the field, such as \textcite{ribeiro_why_2016} and \textcite{pieters_explanation_2011}. So what does it mean to trust an AI system? A basic philosophical analysis of what trust consists in (in the general, non-algorithmic sense) is as follows: $A$ trusts $B$ if and only if $A$ believes that $B$ will act in $A$'s best interest, and $A$ accepts vulnerability to $B$'s actions \cite{jacovi_formalizing_2021}. Moreover, trust often does not have a blanket scope; typically, $A$ will trust $B$ regarding some particular actions or motivations.

An important distinction here can be drawn between whether someone or something is trusted and whether that trust is warranted; i.e. it is worthy of trust \cite{hardin_trust_2002}. In the context of AI, according to \textcite{jacovi_formalizing_2021}, we should only ever trust AI systems to fulfil their contracts, but some AI systems are not worthy of even this trust. Ideally, trust in a system should relate directly to that system's trustworthiness – we should place trust in any system when it warrants trust, but should not place unwarranted trust in any system. The problem of appropriately calibrating trust, however, is yet unsolved.

There are many methods that aim to calibrate trust in an AI algorithm. We could give some access to the patterns that distinguish correct and incorrect cases. The better some end-user is able to distinguish these, the better they know when they can trust the model to be correct. Knowledge of the performance of a model grants access to the patterns distinguishing correct and incorrect cases. Facts about the bias of the model grant access as well, as does information about the performance of the model on a subset of data. Explanations can be seen as one way of imparting such information. Because of this, algorithm explanations allow explainees to determine when an algorithm is trustworthy. 

\subsection{Relevant Evaluations of XAI}
There is a growing body of studies that empirically evaluate xAI methods. In some cases, an xAI method is evaluated not with human subject experiments but rather with some formally defined proxy for interpretability \cite{doshi-velez_towards_2017}. However, most evaluations involve humans using an AI system to perform some task, and the effect of an xAI method on task performance is measured.  For our purposes, namely the measurement of end-user trust and reliance, we need to measure lay people using a system in a simplified but realistic task.

It should also be noted that there is no standard evaluation or set of benchmarks explainability methods are judged by \cite{doshi-velez_towards_2017}. Indeed, though there are proposed methods for evaluating attribution-based explanations and model-based explanations, there are none for evaluating example-based explanations \cite{markus_role_2021}. As a result, there exist a variety of study designs, conditions, and variables measured by human-centred evaluation papers. We list some such studies here.

\textcite{ford_play_2020} run a study where they examine the impact of post-hoc explanations-by-example and error-rates on people's perceptions of a black-box classifier. They show that case-based explanations lead participants to perceive miss-classifications as more correct. In the case of their study, a case-based explanation is a series of three important data points in the training of the model (in other words, an “influential instances” explanation). They also show that classifiers with higher error rates lead participants to perceive the models as less trustworthy. They show participants a series of machine classifications of the MNIST dataset, where the model either correctly labels the data or commits an alternate labelling error. They then task the participants with rating correctness and reasonableness of each classification on a 5-point scale. Finally, at the end, the participants filled out global correctness and reasonableness, alongside global trust forms. The design was a 2x3 design, with explanations present or absent, and three accuracy levels. Participants report miss-classifications as being more correct when given a case-based explanation. They did not find significant findings for reasonableness, trust, or satisfaction across explanation-present or explanation-absent conditions. They use a MANOVA design to account for differences across conditions, but use a between-subjects design \cite{ford_play_2020}. 

\textcite{jacobs_how_2021} run a study in the medical diagnosis context, where participants are given a vignette of a patient and, in the experiment condition, a recommended treatment list and explanation. Participants were asked to make an antidepressant treatment selection, to rate their confidence, and to indicate the utility of the model on their decision (it appears on a 5-point scale). The study had statistically significant results only with respect to accuracy (measured as an average of 0s and 1s), not confidence or perceived utility. In the accuracy case, this indicated that an algorithmically generated treatment list, when wrong, would mislead the participants and lower their accuracy. In the confidence and utility cases, this would indicate either that accuracy is affected, but not confidence or utility, or that the Likert scale measurement method used in Jacobs et al.'s study is a weaker measurement tool than the accuracy. Furthermore, these findings are not isolated to the explanation. Rather, Jacobs et al. consider the recommendation and the explanation together. For example, they find that feature-based explanations, paired with incorrect recommendations, lowered accuracy compared to the baseline condition (no recommendation) \cite{jacobs_how_2021}. 

\textcite{bansal_does_2021} perform another similar study, looking at human-AI cooperation in a context where they have comparable performance. They use sentiment classification as their task. Furthermore, they measure effects with a variety of explanations. They find that explanations inspire increased trust in AI systems regardless of the correctness of the model. As explanation, they use a highlighting of the top predicted sentiment classes, alongside a highlighting of important words. Finally, they test expert-generated explanations to serve as an upper bound, as they found that humans were often confused when machine explanations made little sense. They note that explanations make the user more likely to have high accuracy when the AI is correct, but more likely to have low accuracy when the AI is incorrect. This would indicate that AI recommendations with explanations are capable of fostering mistrust, as they foster both overtrust and under trust \cite{bansal_does_2021}.

\textcite{mohseni_trust_nodate} measure user trust in an AI fake news detection system over time. They design a study in which users are shown true and fake news articles. The users are asked at three different points (each one additional third through the study) what their perceived accuracy of the explainable AI system is. They had three different conditions, segmented by type of explanation. There was a baseline condition with no explanation and two xAI conditions. They show that the user trust can be clustered into five profiles: consistent overtrust, consistent under trust, consistent trust, trust gains continually, and trust decreases continually. They show that more participants from the no explanation and attribute explanation conditions were consistently gaining trust, and that more participants from the attention explanation condition overshot their second perceived accuracy measurement \cite{mohseni_trust_nodate}.

\section{Generative AI Detectors}
GenAI models have quickly surpassed what was previously thought possible with a natural language model \cite{brown_language_2020,chowdhery_palm_2022,openai_gpt-4_2023}. While their ability to respond with natural language is impressive, their use in academic writing raises questions; of particular note are those of fairness and integrity \cite{hu_challenges_2023}. In part due to these questions, there are many approaches to detecting AI usage, a vast majority of them commercial. Researchers have developed methods such as DetectGPT and stylometric detection, while commercial approaches include AI Writing Check, CatchGPT, Copyleaks, GPT Radar, GPTZero,  Turnitin, and Originality.ai \cite{mitchell_detectgpt_2023,kalpesh_krishna_paraphrasing_2023,tharindu_kumarage_stylometric_2023,gptzero_gptzero_2023,kirchner_new_2023}. These genAI detectors, much like genAI itself, have advanced rapidly in capability.

Many research institutions make their stances on pedagogical integrity clear. \textcite{h_holden_thorp_chatgpt_2023}, in an editorial from \emph{Science}, declares that ``the word `original' is enough to signal that text written by ChatGPT is not acceptable: It is, after all, plagiarized from ChatGPT''. The journal adopts a policy that: ``text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools'' \cite{h_holden_thorp_chatgpt_2023}. However, while research institutions face pressure to publish universal guidelines, researchers themselves are free to debate the theoretical and ethical implications of genAI usage \cite{lav_r_varshney_limits_2020,h_holden_thorp_chatgpt_2023,yu_huang_reflection_2023,weber-wulff_testing_2023,otterbacher_why_2023}. \textcite{yu_huang_reflection_2023} argues that these guidelines emerge as a result of pressure placed on organisations, and that, in the pursuit of academic integrity, these institutions hamper learning. A better system would encourage use of these new technologies and instead make appropriate adjustments to teaching methods and examination standards 
\cite{yu_huang_reflection_2023}. 

While the complete ban of genAI systems is perhaps disagreeable and unrealistic, it is at least clear. \textcite{MikePerkins_JasperRoe_2023} point out a problematic lack of clarity in policies that allow genAI for some uses, but not others. Under many of these guidelines, detection of genAI is not sufficient to determine whether program policy has been violated, as the nature of the usage of genAI must be compared to the program guidelines (e.g., if a line or paragraph is rewritten by a genAI, is this a form of writing, or editing?). \textcite{MikePerkins_JasperRoe_2023} do not encourage bans of genAI usage, instead arguing the virtues of ``integrating technology, education, policy reform, and assessment restructuring''.

Despite these urgings, the field remains preoccupied with the question of detecting AI-based plagiarism. Many papers have undertaken the task of determining whether genAI detectors can help censure writers who use AI. Some studies conclude that detection algorithms can be effective across a variety of simple detection tasks \cite{dugan_raid_2024,weber-wulff_testing_2023,tharindu_kumarage_stylometric_2023,elkhatat_evaluating_2023,mitchell_detectgpt_2023}. Other evaluations find that, for more complicated tasks such as detecting paraphrased AI-generated text, these detectors perform less well \cite{kalpesh_krishna_paraphrasing_2023}. Ostensibly unrelated to plagiarism, researchers are beginning to evaluate potential implications of using imperfect AI detectors \cite{liang_gpt_2023} or the implications of genAI for content creation \cite{kalpesh_krishna_paraphrasing_2023}. For example, comparing detectors' False Positive Rates (FPRs) for genuinely human-generated essays by from a US-based essay competition ($N = 88$) against those from Chinese English-language test takers ($N = 91$), \textcite{liang_gpt_2023} conclude AI detectors are biased against non-native English writers. Even still, these works concern themselves primarily with the question of whether a single essay includes any AI-written content; in adopting this viewpoint, they ascribe value to this individual-level determination, inadvertently viewing this problem through the lens of plagiarism.

Though the problem of detecting AI-written plagiarism is interesting, it is only one of the many problems practitioners evaluating essays are faced with. While theoretical work explores the implications of these other problems \cite{otterbacher_why_2023,yu_huang_reflection_2023}, practical work often overlooks practitioners' real and present need for solutions.

\section{Approaches to Considerations of Diversity}
Algorithmic fairness literature has much to say on just processes for considering different applicants. Much of this work builds on the \emph{similar treatment principle} whereby individuals who perform similarly on some dimension(s) are treated equally in selection \cite{dwork_fairness_2012}.\footnote{Though not generally mentioned in this literature, the similar treatment principle is the normative basis on which differential treatment of demographic groups in audit studies is deemed wrong.} This approach has been criticized, however, on the grounds that in circumstances when performance and demographic factors are correlated, employing this principle could substantially reduce the diversity of the chosen cohort, which may not accord with a decision-makers fairness preferences \cite{MikePerkins_JasperRoe_2023}. Furthermore, when measurements of performance are biased in favor of certain demographic groups, similar treatment on the basis of these measurements leads to applicants with the same underlying ability being treated differently \cite{fleisher_whats_nodate}. An alternative perspective is to assume fairness and equity are captured by diversity, and instead to measure diversity directly.

A growing economics literature notes the impact of new screening technologies on the productivity and diversity of the personnel that organizations select. Modifications to screening allow us to retain the similar treatment principle while still improving organisational diveristy. \textcite{autor2008does} show that adding personality tests to the job applicant screening process in a national retail firm increased the productivity of selected workers without reducing minority representation among hires. More recently, \textcite{li2020hiring} demonstrate that, within a Fortune 500 tech company, diversity and productivity of hires can be improved by using exploration-based applicant screening algorithms that recommend applicants with rare sets of characteristics at higher rates in order to improve noisy predictions and, therefore, learn how to find promising applicants from atypical backgrounds. Similarly, \textcite{bergman2021seven} show that, across seven colleges, using a prediction algorithm instead of a test to decide whether to place students into remedial classes.

Some technologies attempt to tackle improvements in cohort diversity more directly. 
Three measures are commonly considered: (1) the majority-minority approach, (2) the generalized variance approach, and (3) the entropy statistic (see \textcite{budescu2012measure} for a summary). \textcite{kleinberg2018algorithmic} introduces an algorithm that colleges can use to calculate the most meritorious cohort at any threshold of representation for a single underrepresented group. Their algorithm is straightforward: define a minority group (e.g. black applicants) and the corresponding majority group (e.g. white applicants), rank applicants within each group by measure of merit (e.g. test scores, predicted college performance, etc.), set a threshold for the proportion of those selected who should be part of the minority group, select minority group members from the highest ranking down until the threshold is met, and finally, select the majority group members from the highest ranking down until all slots are filled. \textcite{huppenkothen2020entrofy} measure diversity using an entropy-based approach. Their method is dual to that of \textcite{kleinberg2018algorithmic}'s as, instead of first fixing minority groups of interest, they fix a threshold of acceptable applicants, and then optimise for diversity using this threshold. Similarly, this algorithm is fairly straightforward to implement, as it merely involves defining diversity preferences that can be optimised greedily, then optimising them greedily. We follow a similar approach in Chapter \ref{ch:spf}.