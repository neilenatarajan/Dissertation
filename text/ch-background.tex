% \begin{savequote}[8cm]
% \textlatin{Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}

% There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain...
%   \qauthor{--- Cicero's \textit{de Finibus Bonorum et Malorum}}
% \end{savequote}

\chapter{\label{ch:background}Background} 
In this chapter we examine: new advancements in  IAI, open problems in TI and their existing solutions, and human-centric methodologies that might help us applying AI to TI. We first engage in a survey of the field of IAI, including the types of articles prevalent in the field, the prominent paradigms for interpretability and explanation, and several key concepts in the development of new algorithms. Next, we examine open problems in diverse, equitable, and inclusive TI and note pre-existing solutions from prior works. Finally, we examine human-centred methodologies that we may draw on in crafting our own solutions. We further note that some problems are not well-suited to resolution through explanation, and suggest a variety of other solutions that could be attempted in address of these problems.

\minitoc

\section{Interpretable Artificial Intelligence}
\subsection{Types of IAI Publications}
The field of IAI can, broadly, be divided into four main bodies of research: reviews, methods, notions, and evaluations \cite{vilone_explainable_2020}. Review articles are either systematic investigations or literature reviews. Method articles introduce new explanation methods. Notion articles focus on defining notions or concepts related to explainability. Evaluation articles evaluate existing notions or paradigms.

\subsubsection{Review Articles}
Review articles offer meta-commentary on other articles listed below. A review article is so-characterised because its primary contribution is one of aggregation. Thus, a review article might also be a method, notion, or evaluation article, so long as it primarily draws from these articles. This paper is, primarily, a review article.

\subsubsection{Method Articles}
In most method articles, the task of explanation is taken to be fairly well-defined, and the research questions deal in how to most effectively generate that explanation. 

Consider two particular method articles: SHAP explanations, introduced by \textcite{lundberg_unified_2017}, and counterfactual explanations, introduced by \textcite{wachter_counterfactual_2017}. In both papers, we see heavy treatment of the mathematical properties of the explanation algorithm introduced. However, we see relatively light treatment of what the authors take to be a good explanation or proposed use cases for the novel methods. These concepts are instead the domain of notion articles.

\subsubsection{Notion Articles}
Where method articles deal with individual methods, notion articles deal with theory underlying a variety of methods. For example, \textcite{miller_explanation_2017} discussion of the importance of principles of explanation from the social sciences is a notion article.

\subsubsection{Evaluation Articles}
Evaluation articles consider which notions and methods are best-suited to produce the best explanations. Evaluations of method articles, in particular, often compare them to notions introduced elsewhere, and where critiques are found, they are often critiques that the method fails to meet a standard set by the notion.

For the two method articles discussed above, we present two evaluation articles: \textcite{kumar_problems_2020} evaluation of the SHAP method, and \textcite{barocas_hidden_2020} evaluation of counterfactual explanations. Both articles rely on similar notions regarding the purpose of explanations, and critique the methods for failing to adhere to a notion. For instance, \textcite{barocas_hidden_2020} assume that counterfactual explanations are given to provide users actionable information. Similarly, \textcite{kumar_problems_2020} argue that SHAP cannot be used to inform users' actions.

\subsection{A Taxonomy of IAI Methods}
AI model explanations can broadly be categorised into two subgroups: intrinsically interpretable models and post-hoc explanations \cite{molnar_interpretable_2019}. Though intrinsically interpretable models offer explanations and are meaningfully classed as forms of “interpretable AI”, they do not make use of the major developments in “explainable AI” \cite{molnar_interpretable_2019}, since they are already interpretable.

Post-hoc methods can be further separated into global methods, which explain entire models, and local methods, which explain individual decisions \cite{molnar_interpretable_2019}. When assessing explanation methods, one can distinguish between local tests and global tests of model explanations \cite{molnar_interpretable_2019}. Local tests concentrate on the impact of the explanations at the level of a single prediction, global tests focus on the impact of explanations on overall model trust.

Rather than delineating between intrinsic and post-hoc or global and local, we could instead delineate by output type \cite{friedrich_taxonomy_2011}. Some interpretability devices offer feature importance statistics. Others offer explanations by contrast to another datapoint. Yet others offer rules that guide decision-making in regard to an individual case case.

\subsubsection{Use Case}
Consider the following potential use cases for xAI methods: to evaluate whether the AI is operating as intended, to examine an AI for bias and unfairness, to provide recourse, to provide a rule that would guarantee equality. This list is not exhaustive, and yet, even for these three use cases, explanations that satisfy some will not satisfy others \cite{natarajan_trust_2023}.

\subsubsection{Theories of Explanation}
In deciding which type of explanation will best suit a particular use case, we must consider the theory of explanation that underlies the explanation, and the desiderata that that theory demands of its explanations.

\textcite{miller_explanation_2017} summarises key findings from Social Sciences related to explanation. \textcite{miller_explanation_2017} works from a causal theory of explanation; he outlines and examines the theory, noting challenges to it, but does not consider a rejection of the theory. According to the theory, explanations are sought in response to counterfactual cases, i.e. the question asked is not of the form "Why \textit{P}" but rather "Why \textit{P} instead of \textit{Q}" \cite{miller_explanation_2017}. Such counterfactual theories of causality all argue that cause is a matter of what would have happened if the cause had not happened. Though they disagree on precisely how to model these counterfactual cases, many leading philosophical theories of causality agree that causality is best understood in terms of counterfactuals. Hume modeled these counterfactuals in terms of causes and events: in actuality, cause $C$ caused event $E$ to occur; but if cause $C'$ had replaced cause $C$, $E$ would not have occurred. We can call this sort of counterfactual a cause-counterfactual \cite{miller_explanation_2017}. Under this conception, explanations should present cause-counterfactuals as clearly as possible.

\textcite{miller_explanation_2017} also argues that explanations should be contrastive and selective. Contrastive explanations make use of counterfactuals, but these counterfactuals are not cause-counterfactuals. Instead, they are event-counterfactuals. In other words, contrastive explanations consider alternative events $E'$, and explain the data $E$ in relation to $E'$. Selective explanations, on the other hand, are ones that do not consist of a complete cause, but rather select one or two causes from the complete cause. 

\textcite{miller_explanation_2017} presents a powerful application of a theory of explanation to the field of xAI. However, causal theories of explanation are far from the only type of explanatory theories. Indeed, as we noted above with numerical evidence, there are theories of explanation who's desiderata contradict those of \textcite{miller_explanation_2017}'s causal theory \cite{woodward_scientific_2021}. 

In what follows, I present \textcite{woodward_scientific_2021}'s theory, the statistical relevance model of explanation. The statistical relevance theory rests heavily on the notion of statistical relevance. For events $A$, $B$, and $C$, we say $C$ is statistically relevant to $B$ in $A$ if and only if $P(B | A \land C) \neq P(B | A)$. I.e., an explanation is a member $C_i$ of a homogeneous partition $C$ of properties: that is, a set of properties that are exclusive and exhaustive of $A$, where there are no statistically relevant properties $D$ to $B$ in $A \land C_i$. The explanation also consists of $P(B | A)$, the probability $P(B | A \land C_i)$ of each cell withing the partition, and which of the $C_i$ contains the desired point to be explained, $x$ \cite{woodward_scientific_2021}.

On both accounts , explanations have an explainer and an explainee. Explanations in everyday use are an interaction. Unlike most modern algorithmic explanation methods, an explanation given by a human is often given in the form of a conversation. Thus, it obeys the rules of communication using language. In other words, these explanations are social. We may therefore apply theories governing social interactions, such as \textcite{Grice_1975}'s conversational maxims, to explanations \cite{miller_explanation_2017}.

\textcite{Grice_1975}'s four categories of conversational maxims are: quality, quantity, relation, and manner. That is, the quality of information conveyed in a cooperative conversation should be high (information should be likely and justifiable). The quantity of information should be neither too little nor too much. The information should be related to the conversation. The information should be conveyed in an appropriate manner. Note that the quantity bounds overlap quite heavily with selectivity.

\subsection{Trust}
Another important notion related to xAI is trust. Indeed, much of the xAI literature suggests that the purpose of xAI is to increase trust in AI. This is apparent from the titles of canonical papers in the field, such as \textcite{ribeiro_why_2016} and \textcite{pieters_explanation_2011}. So what does it mean to trust an AI system? A basic philosophical analysis of what trust consists in (in the general, non-algorithmic sense) is as follows: $A$ trusts $B$ if and only if $A$ believes that $B$ will act in $A$'s best interest, and $A$ accepts vulnerability to $B$'s actions \cite{jacovi_formalizing_2021}. Moreover, trust often does not have a blanket scope; typically, $A$ will trust $B$ regarding some particular actions or motivations.

An important distinction here can be drawn between whether someone or something is trusted and whether that trust is warranted; i.e. it is worthy of trust \cite{hardin_trust_2002}. In the context of AI, according to \textcite{jacovi_formalizing_2021}, we should only ever trust AI systems to fulfil their contracts, but some AI systems are not worthy of even this trust. Ideally, trust in a system should relate directly to that system's trustworthiness – we should place trust in any system when it warrants trust, but should not place unwarranted trust in any system. The problem of appropriately calibrating trust, however, is yet unsolved.

There are many methods that aim to calibrate trust in an AI algorithm. We could give some access to the patterns that distinguish correct and incorrect cases. The better some end-user is able to distinguish these, the better they know when they can trust the model to be correct. Knowledge of the performance of a model grants access to the patterns distinguishing correct and incorrect cases. Facts about the bias of the model grant access as well, as does information about the performance of the model on a subset of data. Explanations can be seen as one way of imparting such information. Because of this, algorithm explanations allow explainees to determine when an algorithm is trustworthy. 

\subsection{Relevant Evaluations of XAI}
There is a growing body of studies that empirically evaluate xAI methods. In some cases, an xAI method is evaluated not with human subject experiments but rather with some formally defined proxy for interpretability \cite{doshi-velez_towards_2017}. However, most evaluations involve humans using an AI system to perform some task, and the effect of an xAI method on task performance is measured.  For our purposes, namely the measurement of end-user trust and reliance, we need to measure lay people using a system in a simplified but realistic task.

It should also be noted that there is no standard evaluation or set of benchmarks explainability methods are judged by \cite{doshi-velez_towards_2017}. Indeed, though there are proposed methods for evaluating attribution-based explanations and model-based explanations, there are none for evaluating example-based explanations \cite{markus_role_2021}. As a result, there exist a variety of study designs, conditions, and variables measured by human-centred evaluation papers. We list some such studies here.

\textcite{ford_play_2020} run a study where they examine the impact of post-hoc explanations-by-example and error-rates on people's perceptions of a black-box classifier. They show that case-based explanations lead participants to perceive miss-classifications as more correct. In the case of their study, a case-based explanation is a series of three important data points in the training of the model (in other words, an “influential instances” explanation). They also show that classifiers with higher error rates lead participants to perceive the models as less trustworthy. They show participants a series of machine classifications of the MNIST dataset, where the model either correctly labels the data or commits an alternate labelling error. They then task the participants with rating correctness and reasonableness of each classification on a 5-point scale. Finally, at the end, the participants filled out global correctness and reasonableness, alongside global trust forms. The design was a 2x3 design, with explanations present or absent, and three accuracy levels. Participants report miss-classifications as being more correct when given a case-based explanation. They did not find significant findings for reasonableness, trust, or satisfaction across explanation-present or explanation-absent conditions. They use a MANOVA design to account for differences across conditions, but use a between-subjects design \cite{ford_play_2020}. 

\textcite{jacobs_how_2021} run a study in the medical diagnosis context, where participants are given a vignette of a patient and, in the experiment condition, a recommended treatment list and explanation. Participants were asked to make an antidepressant treatment selection, to rate their confidence, and to indicate the utility of the model on their decision (it appears on a 5-point scale). The study had statistically significant results only with respect to accuracy (measured as an average of 0s and 1s), not confidence or perceived utility. In the accuracy case, this indicated that an algorithmically generated treatment list, when wrong, would mislead the participants and lower their accuracy. In the confidence and utility cases, this would indicate either that accuracy is affected, but not confidence or utility, or that the Likert scale measurement method used in Jacobs et al.'s study is a weaker measurement tool than the accuracy. Furthermore, these findings are not isolated to the explanation. Rather, Jacobs et al. consider the recommendation and the explanation together. For example, they find that feature-based explanations, paired with incorrect recommendations, lowered accuracy compared to the baseline condition (no recommendation) \cite{jacobs_how_2021}. 

\textcite{bansal_does_2021} perform another similar study, looking at human-AI cooperation in a context where they have comparable performance. They use sentiment classification as their task. Furthermore, they measure effects with a variety of explanations. They find that explanations inspire increased trust in AI systems regardless of the correctness of the model. As explanation, they use a highlighting of the top predicted sentiment classes, alongside a highlighting of important words. Finally, they test expert-generated explanations to serve as an upper bound, as they found that humans were often confused when machine explanations made little sense. They note that explanations make the user more likely to have high accuracy when the AI is correct, but more likely to have low accuracy when the AI is incorrect. This would indicate that AI recommendations with explanations are capable of fostering mistrust, as they foster both overtrust and under trust \cite{bansal_does_2021}.

\textcite{mohseni_trust_nodate} measure user trust in an AI fake news detection system over time. They design a study in which users are shown true and fake news articles. The users are asked at three different points (each one additional third through the study) what their perceived accuracy of the explainable AI system is. They had three different conditions, segmented by type of explanation. There was a baseline condition with no explanation and two xAI conditions. They show that the user trust can be clustered into five profiles: consistent overtrust, consistent under trust, consistent trust, trust gains continually, and trust decreases continually. They show that more participants from the no explanation and attribute explanation conditions were consistently gaining trust, and that more participants from the attention explanation condition overshot their second perceived accuracy measurement \cite{mohseni_trust_nodate}.

% This is a much more detailed account of the stuff I engage with in the intro
\section{Prior Applications of IAIDSTs to Talent Identification [WIP]}
We examine other approaches to designing decision support tools for talent identification professionals. We examine computational approaches to fairness here, including the 'similar treatment' principle. We pay special attention to measurements of diversity including the straightforward majority/minority group method and the Entrofy algorithm.

\section{The Need for Transparency in Applications of Artificial Intelligence to Talent Identification}
\subsection{The Problem of Transparency}
The problem of transparency is primarily motivated by the desire of stakeholders in an AI system to understand what a given system does and why. For example, in the above COMPAS example, a defendant may desire to understand why their bail was set where it was. In this case, we might consider it insufficient to simply say that COMPAS was used, as we might desire to understand why COMPAS set the recidivism prediction as it is. For most complicated AI systems, this desire cannot be met outright.

An ancillary concern here is trust. While the COMPAS example is not one in which the defendant need trust the system, there are many potential implementations of AI systems where the system's ability to work is contingent on the user trusting the system. For example, consider a system that analyses patients diagnosed with depression and suggests antidepressants as in \textcite{jacobs_how_2021}. This work is traditionally done by clinicians, but with the assistance of a machine, the work could potentially be made both easier and more accurate. However, these benefits are contingent on the clinicians' appropriate trust in the system assisting them. That is, they should neither blindly trust nor blindly distrust the machine. Rather, they should trust the machine appropriately, relying upon the machine when it is reliable and ignoring it when it is not. In order to achieve this, transparency into the AI system's reasoning process is desirable.

There are, however, times where total transparency is undesirable. Most obviously, scoring algorithms that are completely transparent to their users can be gamed. However, there are other reasons for algorithms to be less-than-transparent to their users. For example, if a system is optimised around a given condition, explaining this reasoning to a decision-maker may make them more likely to override given decisions, which will only lead to a less optimal system.

\subsection{Addressing Transparency}
Unlike bias and fairness, transparency can be directly addressed through explainability methods. Indeed, any explanation of a system or that system's outputs will increase that system's transparency. As such, all explanation will have some impact on transparency. However, this does not imply that explanation constitutes a complete solution to the problem of transparency. 

Consider local explanations such as \textcite{lundberg_unified_2017}'s SHAP and \textcite{ribeiro_anchors_2018}'s Scoped Anchors. Both explanations yield insights into a particular prediction made by a model. However, in their most straightforward application, neither provides transparency into the model in general – rather, they provide transparency into the model in this particular use case. Scoped Anchors, in particular, is designed to give transparency into a single use case – by bounding the given feature with an Anchor, we can replace the model locally with a decision rule \cite{ribeiro_anchors_2018}. SHAP, on the other hand, can be extended to provide transparency into a model in general by taking the average feature importances across the distribution of the training set, yielding feature importances for the model overall \cite{lundberg_unified_2017}.

It is also worth noting that not all explanations yield equivalent forms of transparency. For example, while a question like “why does the model yield a particular prediction in a particular case” is well-answered with Scoped Anchors, it is not well-answered by SHAP \cite{lundberg_unified_2017,ribeiro_anchors_2018}. Evaluations such as those by \textcite{binns_human_2022} and \textcite{rader_explanations_2018} indicate that different forms of explanation answer vastly different questions. Thus, the particular problem of transparency in question will have a strong bearing on what explanation method should be used to solve it.


\section{Problems of Diversity, Equity, and Inclusion in Talent Identification}
We highlight several open problems in TI that touch on concerns of DEI. To begin with, we define DEI alongside related concepts such as JEDI, EDI, DEIB, and IDEA. Then we spotlight the potential equity challenges posed by the integration of DSTs into TI workflows. In particular, we note that any bias and fairness issues built into a DST will create bias and fairness issues for the corresponding TI workflow. However, we note that problems of bias and fairness in TI exist even in the absence of DSTs. We move on to discuss fairness issues posed by applicant use of Generative AI. Finally, we note that TI professionals consistently struggle to balance selecting talented individuals with constructing a diverse cohort and note the implications for equity that this struggle brings.  

\subsection{Bias}
\subsubsection{The Problem of Bias}
We can define the problem of bias as the liability of systems to exhibit different behaviour on groups segregated across a “special” partition. “Special” here refers to partitions across protected classes, or partitions across which we would expect a system to perform equally. For example, gender and race are usually special partitions in the context of talent identification, but academic ability and teachability are not. 

To illustrate the issue of bias in AI systems, we turn to \textcite{mattu_how_nodate}'s analysis of the COMPAS system. The COMPAS system was an expert-system used to predict recidivism of criminals in the US. When defendants accused of crimes were booked in jail, they responded to a COMPAS questionnaire. These answers were then fed into the COMPAS software to produce predictions of the “Risk of Recidivism” and “Risk of Violent Recidivism.” These predictions were used, among other things, to determine if a defendant could be set bail (as likelihood of reoffence is a significant factor in the setting of bail). When \textcite{mattu_how_nodate} analysed the COMPAS tool's predictions relative to actual recidivism rates, they found that the recidivism predictions had roughly equal accuracy for White and Black defendants, but that, when errors were made, these errors acted in very different directions. That is, the Black defendants were more likely to be falsely predicted to reoffend, and the White defendants were more likely to be falsely predicted to not reoffend. As a result, the implementation of the COMPAS system had significantly greater negative externalities on the Black community than on the White community \cite{mattu_how_nodate}.

\textcite{barocas_big_2016} illustrate how data mining yields exactly this sort of bias. In the example of the COMPAS set, much of this bias comes from the underlying training data, but the software's predictions were, on average, \emph{more} biased than the underlying data, as the process the software used to optimise its predictions exacerbated any biases in the underlying data \cite{barocas_big_2016}.

\subsubsection{How Modifying Training Data Addresses Bias}
In some cases, algorithmic bias introduced via latent biases in the training data can be corrected rather simply by altering training data before the training process. One case of bias arises when one group across a special partition is much smaller than other groups \cite{barocas_big_2016}. Due to this sample size imbalance, optimising for accuracy across the total group leads to overemphasis on accuracy on the large group, and allows for marginal improvements in performance on the larger groups at the cost of performance on the smaller groups. While we might address this problem with explainability methods, in this case, the most straightforward solution rather involves increasing the attention paid to the small groups in training, either by increasing the rate at which we sample from this group, or by increasing the weight applied to each sample \cite{barocas_big_2016}. 

However, though simple, this solution is not always effective. Sometimes, for example, biases are introduced in the content of the training data, rather than their distribution. In this case, heterogenously biases errors in the variable to be predicted induce those same biases in the predictive algorithm. In this case, the solution is to correct the biases in the data itself, but this is often either prohibitively difficult, or outright impossible \cite{barocas_big_2016}. 

\subsubsection{How IAI Addresses Bias}
When bias cannot be removed, it instead might be managed. Where biased behavioural patterns exist in an AI system, simply making these systems more interpretable will not alter these patterns themselves. However, with humans in the loop, spotlighting algorithmic bias might allow humans to address it.

Consider the COMPAS example again, except suppose now that COMPAS implements a counterfactual explanation, which points to a similar hypothetical input that would have a different output. Suppose a defendant has a high calculated risk of reoffense. The actual defendant's race is “black”, but the counterfactual explanation reveals that a similar white defendant would have had a low calculated risk of reoffense. That is, COMPAS is, in this case, clearly discriminating based on race. If this were used in a courtroom setting, a judge might see this information and subsequently choose to disregard the prediction yielded by COMPAS. In this way, an explanation could indicate that an algorithm was biased and encourage scrutiny of that algorithm \cite{mothilal_explaining_2019,wachter_counterfactual_2017}.

Furthermore, in certain special circumstances, implementing techniques from IAI at training time can actually remove bias. For example, we could imagine imposing explanation-based constraints on a model at the time of training. One such constraint would be \textcite{wang_deontological_2020}'s `monotonicity' constraint , which would require that a particular input i relate monotonically to the model's output. Thus, if feature sets X and X' differ only in i such that i in X' is greater than i in X, and M is a model that is monotonic in i, then M(X') should be greater than M(X). This constraint would prevent a model from making decisions that penalise applicants based on, for example, membership in a minority group.

\subsection{Fairness}
\subsubsection{The Problem of Fairness}
Fairness is closely tied to, though still distinct from, Bias. Where bias deals with groups, fairness deals with individuals. We can define the problem of fairness as the liability of AI systems to exhibit different behaviour on different individuals that are similar in relevant dimensions. We can note this follows the given principle, entitled the “similar treatment” principle: if two individuals are similar across relevant dimensions, they should be treated similarly \cite{Fleisher_2021,dwork_fairness_2012}. 

Consider again the COMPAS example. Where the disproportional treatment of groups is a form of bias, an individual decision yielding a false positive in one individual, and a false negative in another individual is a problem of fairness. We can call this form of fairness “Individual Fairness”, and contrast it with other forms of fairness such as statistical parity fairness, where a system is fair if and only if it achieves parity between groups \cite{Fleisher_2021,dwork_fairness_2012}. We focus on individual fairness as a particularly important definition of fairness, and note that statistical parity fairness is more closely bias, as we have defined it. However, note that individual fairness is not a privileged form of fairness. \textcite{Fleisher_2021} argues this, and notes that individual fairness is undesirable when it conflicts with other notions of fairness. Thus, we also note that there are more general notions of fairness that we might address as well.

\subsubsection{Addressing Fairness}
Much like bias, fairness is not address directly via interpretability alone. Rather, they address fairness indirectly by highlighting unfair uses of AI systems. Indeed, xAI methods that address bias in this manner also address fairness. However, though the notion of individual fairness is not addressed, there are other forms of fairness that can be addressed by xAI methods.

One such method is the presentation of \textcite{ustun_actionable_2019}'s actionable recourse. Suppose some AI system $S$ makes yields some outcome $y$ for some person $P$ represented by feature set $X$. Suppose further that $y$ is an unfavourable outcome for $P$, and $y'$ would have instead been a favourable outcome. Actionable recourse information, in this case, is information that would allow $P$ to change their representation to feature set $X'$ such that $M(X')= y'$. In other words, recourse informs individuals what they would have to change in order to be received more favourably by an AI system. As \textcite{ustun_actionable_2019} argue, the mere presentation of this information makes a system more fair, as the decisions made by this system are accompanied by a means of achieving a more favourable decision, so individuals are always given the opportunity to improve their outcomes.

However, while this creates a form of fairness where individuals' outcomes are less fixed, it does not ensure individual fairness. This is by design – actionable recourse does not handle the differential treatment of similar individuals \cite{ustun_actionable_2019}. Indeed, one might contend, as \textcite{Fleisher_2021} does, that individual fairness is sometimes undesirable, especially when it conflicts with other notions of fairness.

\subsection{Diversity [WIP]}
[To-do]...

\section{Risks of Applicant Usage of Generative AI}
[To-do]...

\section{Methodologies [WIP]}
\subsection{Research Through Human-Centered Design}
This section enumerates research-through-design methods and explores relevant method papers. It focuses on the design methodologies employed in building the SHAP algorithm and the SPF algorithm.

\subsection{Quantitative Methods and Statistical Analysis}
This section covers both collecting information from users (i.e., online surveys) and robust statistical analyses applied both to those surveys and to the work in generative AI detection.

\subsection{Qualitative Methods and Thematic Analysis}
This section focuses on running interviews and group thinkaloud sessions. It also covers thematic analysis following Braun and Clarke.

\subsection{Case Studies}
\subsubsection{The Rise Program}
\subsubsection{The Ellison Scholars Program}

