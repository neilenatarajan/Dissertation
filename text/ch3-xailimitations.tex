% \begin{savequote}[8cm]
% Alles Gescheite ist schon gedacht worden.\\
% Man muss nur versuchen, es noch einmal zu denken.

% All intelligent thoughts have already been thought;\\
% what is necessary is only to try to think them again.
%   \qauthor{--- Johann Wolfgang von Goethe \cite{von_goethe_wilhelm_1829}}
% \end{savequote}

\chapter{\label{ch:3-xailimitations}Limitations of Post-Hoc Explainable AI}

\minitoc

\section{Introduction}
Increasingly, decisions affecting the lives of lay people are made by AI algorithms. And while these algorithms may be useful, they can also be dangerous. Users of AI systems desire an understanding of how these systems functions and why they yield the outputs they do, so that they may respond appropriately to the outputs \cite{binns_human_2022}. When AI systems make decisions impacting the user, user insight into the decision-making process allows for recourse \cite{ustun_actionable_2019}. However, when decision-making systems involve both an AI and human component, user insight into AI outputs is crucial in both (1) inducing appropriate reliance or scepticism in the AI as warranted and (2) optimising the overall performance of the decision-making system. This is especially true in high-stakes domains such as healthcare, finance, and criminal justice, where the decisions made by AI systems can have significant consequences for individuals \cite{binns_human_2022,ustun_actionable_2019,wachter_counterfactual_2017}.

The growing field of Explainable Artificial Intelligence (xAI) aims to develop methods for explaining algorithms. While some models are intrinsically interpretable \cite{rudin_interpretable_2021}, many more complicated machine learning algorithms are difficult to interpret. In order to explain these more complex models, various dedicated explanation algorithms have been proposed. Within the field of xAI, post-hoc, model-agnostic explainability methods remain popular due to their flexibility and ease-of-application \cite{molnar_interpretable_2019}. Indeed, the ability of methods like SHAP, LIME, and Scoped Anchors to treat any model as a black-box allows for applications of explainability to models that would be otherwise inscrutable \cite{lundberg_unified_2017,ribeiro_why_2016,ribeiro_anchors_2018}. However, there are several known limitations that can impact their utility as IAIDSTs; in some cases, these limitations undermine the purpose of explanation, or even to deceive and mislead users \cite{wang_transparency_2022}. 

In this Chapter, we explore a disconnect between user trust in an AI system and that system's trustworthiness as a result of post-hoc explainable AI tools. We begin by citing a series of papers yielding this result in a variety of contexts, and proceed to present our own work confirming that this disconnect persists in talent identification tasks. We then identify theories of when and why this disconnect occurs, and propose a series of experiments to test these theories. 

In Chapter \ref{ch:4-xaicasestudy}, we implement these experiments and present the results.

\section{Trust and Trustworthiness in Explainable AI}
User trust is central to the utility and limitations of any user-facing system. In the context of IAIDSTs, user trust in the system facilitates the system's adoption and use. Explainability is crucial here, as an oft-cited goal of transparency is increasing user trust in AI systems \cite{wang_transparency_2022}. However, though trust is much-discussed, trustworthiness is often overlooked.

\textcite{jacovi_formalizing_2021} set out rules for trust in an algorithmic context. They set out a basic philosophical analysis of trust (in the general, non-algorithmic sense): A trusts B if and only if A believes that B will act in A's best interest and accepts vulnerability to B's actions. Trust often has limited scope; typically, A will trust B regarding some particular actions or motivations, but not others.

In the context of trust in AI, this definition is only slightly changed. \textcite{jacovi_formalizing_2021} characterise trust in an AI system by two properties: ``the vulnerability of the user, and the ability to anticipate the impact of the AI model's decisions''; \textcite{vereschak_how_2021} similarly isolate three elements: ``trust is linked to a situation of vulnerability and positive expectations, and is an attitude''; \textcite{lee_trust_2004} give a similar definition of trust: ``An attitude that an agent will achieve an individual's goal in a situation characterized by uncertainty and vulnerability''. In all definitions, we see \emph{vulnerability} emerge as a key concept, and we variably also see that trust is characterised by \emph{uncertainty} and \emph{expectations}.

An important distinction here can be drawn between whether someone or something is trusted and whether that trust is well-placed; i.e. it is worthy of trust \cite{hardin_trust_2002}. In the machine context, Jacovi et al. argue, an algorithm is worthy of trust if and only if there exists some contract that the algorithm promises (or that the algorithm's creators and implementors promise) to uphold. They term this a `contractual' model of trust. We adopt the nomenclature of contractual trust and use it to frame breakdowns in trust in specific post-hoc explanations.

\section{A Series of Concerning Results}
\subsection{The Dangers of Transparency}
It is often assumed in the xAI literature that transparency of AI systems is always desirable \cite{molnar_interpretable_2019,miller_explanation_2017-1}. However, \textcite{wang_transparency_2022} dispel this claim. In particular, they demonstrate how, rather than empowering users and stakeholders, some implementations of algorithmic transparency serve the system's deployers, rather than its users, either through a false sense of understanding or the enforcement of norms and power structures. They examine the USA's FICO credit score, which releases the factors that go into the score, the data sources for these factors, and general guidelines on how these factors are used. They find specific information on the use of these factors lacking, and conclude that, rather than empowering users to debate the ethics of certain decisions, this transparency only serves to enforce certain behaviours among the users \cite{wang_transparency_2022}.

\textcite{ustun_actionable_2019} note a similar danger in transparency. They argue that explanations of AI systems making decisions about at end-users should empower those end users to alter the system's determinations. In particular, they introduce the concept of `Actionable Recourse', which consists of a specific set of actions an end-user can take to change the AI's determination, and demonstrate how to calculate these for linear models. Key to actionable recourse is that the actions an explainee should take in response to the explanation are clear in the explanation \cite{ustun_actionable_2019}.

We extend this concept to the context of IAIDSTs. Explanations aimed at decision-makers need not provide them \emph{recourse}, as decision-makers already posses the ability to override the AI system. However, they should still provide \emph{actionable} information, so that the explanations should contain information relevant to the decision-makers choice of what to do with the machine recommendation.

\subsection{Black Box Explanations Increase User Trust in AI Systems}
A number of studies exploring popular post-hoc, black-box explanation algorithms have found that these methods tend to increase overall user trust in the system being explained \cite{ford_play_2020,jacobs_how_2021,wang_transparency_2022}. 

\textcite{ford_play_2020} explore this effect in the context of a machine decision-maker with a human evaluator. They run a study examining the impact of post-hoc explanations-by-example and error-rates on people's perceptions of a black-box classifier classifying images from the MNIST dataset. They show that presenting `case-based explanations' (a series of three important data points in the training of the model; elsewhere, we term this an `influential instances' explanation and use `case-based' more broadly) lead participants to perceive miss-classifications as more correct \cite{ford_play_2020}.

\textcite{jacobs_how_2021} extend this result to a human-in-the-loop context. They study the effect of machine recommendation on a clinician's ability to select antidepressant treatments, and find that providing clinicians an incorrect algorithmically generated treatment list lowers the accuracy of clinicians' own treatment lists. Notably, they do not isolate this result to the presence of an explanation, but rather demonstrate how two explainable AI systems harm clinician antidepressant selection relative to a placebo group. However, they also find that feature-based explanations mislead clinicians more than heuristic-based explanations. Modern best practices outlined by \textcite{miller_explanation_2017-1} prefer the usage of feature-based explanations, so we find this particularly alarming \cite{jacobs_how_2021}.

\textcite{mccradden_when_2021} responds to \textcite{jacobs_how_2021} questioning the role of accuracy in explanations of IAIDSTs. They note the focus of \textcite{jacobs_how_2021} on improving \emph{accuracy} of the treatment lists, and aim to do this with IAIDSTs. However, though these systems may optimise for accuracy, they notably fail (at least in these specific instances) to facilitate clinicians' ability to \emph{help patients}, which is a clinicians' primary goal \cite{mccradden_when_2021}.

In a broader context, while `helping patients' doesn't describe the goal of all IAIDSTs, neither does `improving accuracy'. And, though increasing decision-maker trust regardless of system veracity may improve overall accuracy, it may hamper the broader goal of the decision-making system.

\subsection{Impacts on Talent Identification}
It should be noted that these effects are not universal. \textcite{mohseni_trust_nodate} measure user trust in an AI fake news detection system over time, and find the profile of the users to be just as important as the type of explanation. Though they demonstrate that different explanatory conditions have differential effects on which profile participants are likely to exhibit, they also cluster trust into five profiles by user: consistent over-trust, consistent under-trust, consistent trust, trust gains continually, and trust decreases continually. These profiles suggest that the same explanations will impact different groups performing different tasks differently. Thus, while an IAIDST may lead clinicians selecting antidepressants astray, it may have no effect (or even the opposite effect) on talent identification tasks \cite{mohseni_trust_nodate}.

In the next section, we undertake a series of experiments to test the implications of these limitations for talent identification.

\section{Misleading Explanations of AI Outputs in Talent Identification}
To-do: \emph{Explainable AI (xAI) methods are often motivated by the need to increase user trust in AI systems. However, increased trust may not always be desirable. While too little trust might lead people to neglect AI systems when they are correct, too much trust can also be a risk. Explanations which encourage misplaced trust in incorrect AI outputs might mislead humans into making bad decisions. We investigate the extent to which three different xAI methods create too much trust in their underlying AI systems. Examining Shapley-based Additive Explanations (SHAP), Scoped Anchors, and a confidence-based explanation that presents the model's confidence in an output, we conduct experiments where participants perform tasks with xAI assistance. We find that when the system is wrong, SHAP or Confidence explanations still increase trust. Scoped Anchors explanations, by contrast, increase participants' confidence in their own estimations regardless of the system's correctness. We discuss implications for the design and deployment of xAI in HitL tasks.}

\section{Trust Explanations to Do What They Say} % Is this part of intro or methods? It's almost more of a commentary on the state of the literature, so possibly fits better with background.
To-do: \emph{Trusting an algorithm without cause may lead to abuse, and mistrusting it may similarly lead to disuse. Trust in an AI is only desirable if it is warranted; thus, calibrating trust is critical to ensuring appropriate use. In the name of calibrating trust appropriately, AI developers should provide contracts specifying use cases in which an algorithm can and cannot be trusted. Automated explanation of AI outputs is often touted as a method by which trust can be built in the algorithm. However, automated explanations arise from algorithms themselves, so trust in these explanations is similarly only desirable if it is warranted. Developers of algorithms explaining AI outputs (xAI algorithms) should provide similar contracts, which should specify use cases in which an explanation can and cannot be trusted.}

% Clearly, post-hoc and intrinsically interpretable are very different. We clearly need to study them separately. That's what the next two chapters do