@book{hardin_trust_2002, address={New York, NY, US}, series={Trust and trustworthiness}, title={Trust and trustworthiness}, ISBN={978-0-87154-342-4}, abstractNote={In this book the author addresses the the standard theories of trust and articulates his own new idea: that much of what we call trust can be best described as “encapsulated interest.” Research into the roles of trust in society has offered a broad range of often conflicting theories. Some theorists maintain that trust is a social virtue that cannot be reduced to strategic self-interest; others claim that trusting another person is ultimately a rational calculation based on information about that person and his or her incentives and motivations. The author argues that we place our trust in persons whom we believe to have strong reasons to act in our best interests. He claims that we are correct when we assume that the main incentive of those whom we trust is to maintain a relationship with us--whether it be for reasons of economic benefit or for love and friendship. The author articulates his theory by using examples from a broad array of personal and social relationships, paying particular attention to explanations of the development of trusting relationships. He also examines trustworthiness and seeks to understand why people may behave in ways that violate their own self-interest in order to honor commitments they have made to others. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, publisher={Russell Sage Foundation}, author={Hardin, Russell}, year={2002}, pages={xxi, 234}, collection={Trust and trustworthiness} }


@inproceedings{ghadiri_socially_2021,
	address = {Virtual Event Canada},
	title = {Socially {Fair} k-{Means} {Clustering}},
	isbn = {978-1-4503-8309-7},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445906},
	doi = {10.1145/3442188.3445906},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Ghadiri, Mehrdad and Samadi, Samira and Vempala, Santosh},
	month = mar,
	year = {2021},
	pages = {438--448},
	file = {Markup:/Users/neilnatarajan/Zotero/storage/MKXAS2NL/Socially Fair k-Means Clustering.pdf:application/pdf;remarkable ver:/Users/neilnatarajan/Zotero/storage/FQ428NM3/Ghadiri et al. - 2021 - Socially Fair k-Means Clustering.pdf:application/pdf;Submitted Version:/Users/neilnatarajan/Zotero/storage/S89UG6FD/Ghadiri et al. - 2021 - Socially Fair k-Means Clustering.pdf:application/pdf},
}

@techreport{chari_specious_2021,
	type = {preprint},
	title = {The {Specious} {Art} of {Single}-{Cell} {Genomics}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.08.25.457696},
	abstract = {Abstract
          
            Dimensionality reduction is standard practice for filtering noise and identifying relevant dimensions in large-scale data analyses. In biology, single-cell expression studies almost always begin with reduction to two or three dimensions to produce ‘all-in-one’ visuals of the data that are amenable to the human eye, and these are subsequently used for qualitative and quantitative analysis of cell relationships. However, there is little theoretical support for this practice. We examine the theoretical and practical implications of low-dimensional embedding of single-cell data, and find extensive distortions incurred on the global and local properties of biological patterns relative to the high-dimensional, ambient space. In lieu of this, we propose semi-supervised dimension reduction to higher dimension, and show that such
            targeted
            reduction guided by the metadata associated with single-cell experiments provides useful latent space representations for hypothesis-driven biological discovery.},
	language = {en},
	urldate = {2021-09-16},
	institution = {Genomics},
	author = {Chari, Tara and Banerjee, Joeyta and Pachter, Lior},
	month = aug,
	year = {2021},
	doi = {10.1101/2021.08.25.457696},
	file = {Markup:/Users/neilnatarajan/Zotero/storage/5H7CDVB6/The Specious Art of Single-Cell Genomics.pdf:application/pdf;Submitted Version:/Users/neilnatarajan/Zotero/storage/9J97L62B/Chari et al. - 2021 - The Specious Art of Single-Cell Genomics.pdf:application/pdf},
}

@article{mccradden_when_2021,
	title = {When is accuracy off-target?},
	volume = {11},
	issn = {2158-3188},
	url = {http://www.nature.com/articles/s41398-021-01479-4},
	doi = {10.1038/s41398-021-01479-4},
	language = {en},
	number = {1},
	urldate = {2021-09-16},
	journal = {Translational Psychiatry},
	author = {McCradden, Melissa D.},
	month = jun,
	year = {2021},
	pages = {369},
	file = {Markup:/Users/neilnatarajan/Zotero/storage/P7MBUHQC/When is accuracy off-target.pdf:application/pdf;McCradden_2021_When is accuracy off-target.pdf:/Users/neilnatarajan/Zotero/storage/KISMSTP2/McCradden_2021_When is accuracy off-target.pdf:application/pdf},
}

@misc{noauthor_index_nodate,
	title = {Index - {Human}-{Learn}},
	url = {https://koaning.github.io/human-learn/},
	urldate = {2021-09-28},
	file = {Index - Human-Learn:/Users/neilnatarajan/Zotero/storage/PXG36WQR/human-learn.html:text/html},
}

@techreport{cowls_ai_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {AI} {Gambit} — {Leveraging} {Artificial} {Intelligence} to {Combat} {Climate} {Change}: {Opportunities}, {Challenges}, and {Recommendations}},
	shorttitle = {The {AI} {Gambit} — {Leveraging} {Artificial} {Intelligence} to {Combat} {Climate} {Change}},
	url = {https://papers.ssrn.com/abstract=3804983},
	abstract = {In this article we analyse the role that artificial intelligence (AI) could play, and is playing, to combat global climate change. We identify two crucial opportunities that AI offers in this domain: it can help improve and expand current understanding of climate change, and it can contribute to combatting the climate crisis effectively. However, the development of AI also raises two sets of problems when considering climate change: the possible exacerbation of social and ethical challenges already associated with AI, and the contribution to climate change of the greenhouse gases emitted by training data and computation-intensive AI systems. We assess the carbon footprint of AI research, and the factors that influence AI’s greenhouse gas (GHG) emissions in this domain. We find that the carbon footprint of AI research may be significant and highlight the need for more evidence concerning the trade-off between the GHG emissions generated by AI research and the energy and resource efficiency gains that AI can offer. In light of our analysis, we argue that leveraging the opportunities offered by AI for global climate change whilst limiting its risks is a gambit which requires responsive, evidence-based, and effective governance to become a winning strategy. We conclude by identifying the European Union as being especially well-placed to play a leading role in this policy response and provide 13 recommendations that are designed to identify and harness the opportunities of AI for combatting climate change, while reducing its impact on the environment.},
	language = {en},
	number = {ID 3804983},
	urldate = {2021-10-05},
	institution = {Social Science Research Network},
	author = {Cowls, Josh and Tsamados, Andreas and Taddeo, Mariarosaria and Floridi, Luciano},
	month = mar,
	year = {2021},
	doi = {10.2139/ssrn.3804983},
	keywords = {Artificial Intelligence, Carbon Footprint, Climate Change, Digital Ethics, Digital Governance, Environment, Sustainability},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/5678LH7E/Cowls et al. - 2021 - The AI Gambit — Leveraging Artificial Intelligence.pdf:application/pdf;Markup:/Users/neilnatarajan/Zotero/storage/NQZSQ28J/Cowls et al. - 2021 - The AI Gambit — Leveraging Artificial Intelligence.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/DPBRWVKN/papers.html:text/html},
}

@techreport{barocas_big_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Big {Data}'s {Disparate} {Impact}},
	url = {https://papers.ssrn.com/abstract=2477899},
	abstract = {Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm’s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.},
	language = {en},
	number = {ID 2477899},
	urldate = {2021-10-11},
	institution = {Social Science Research Network},
	author = {Barocas, Solon and Selbst, Andrew D.},
	year = {2016},
	doi = {10.2139/ssrn.2477899},
	keywords = {algorithms, big data, civil rights, data mining, discrimination, disparate impact, disparate treatment, employment discrimination, inequality, procedural fairness, substantive fairness, Title VII},
	file = {Barocas and Selbst - 2016 - Big Data's Disparate Impact.pdf:/Users/neilnatarajan/Zotero/storage/VGJXFHCQ/Barocas and Selbst - 2016 - Big Data's Disparate Impact.pdf:application/pdf;Barocas_Selbst_2016_Big Data's Disparate Impact.pdf:/Users/neilnatarajan/Zotero/storage/B3DAQ4R7/Barocas_Selbst_2016_Big Data's Disparate Impact.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/X8NTSPKV/papers.html:text/html},
}

@article{suresh_framework_2021,
	title = {A {Framework} for {Understanding} {Sources} of {Harm} throughout the {Machine} {Learning} {Life} {Cycle}},
	url = {http://arxiv.org/abs/1901.10002},
	abstract = {As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
	urldate = {2021-10-13},
	journal = {arXiv:1901.10002 [cs, stat]},
	author = {Suresh, Harini and Guttag, John V.},
	month = jun,
	year = {2021},
	note = {arXiv: 1901.10002},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/W9EYYBFW/1901.html:text/html;Suresh and Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf:/Users/neilnatarajan/Zotero/storage/S7MC52HY/Suresh and Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf:application/pdf;Suresh_Guttag_2021_A Framework for Understanding Sources of Harm throughout the Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/C6YZDUWC/Suresh_Guttag_2021_A Framework for Understanding Sources of Harm throughout the Machine Learning.pdf:application/pdf},
}

@misc{stein_philosophy_and_ethics_of_informationpdf_nodate,
	title = {Philosophy\_and\_ethics\_of\_information.pdf},
	author = {Stein, Jacob},
	file = {Markup:/Users/neilnatarajan/Zotero/storage/SH5FD7CE/1056156_Philosophy_and_ethics_of_information.pdf:application/pdf;Stein - Philosophy_and_ethics_of_information.pdf.pdf:/Users/neilnatarajan/Zotero/storage/X4KKG57C/Stein - Philosophy_and_ethics_of_information.pdf.pdf:application/pdf},
}

@article{bird_social_2010,
	title = {{SOCIAL} {KNOWING}: {THE} {SOCIAL} {SENSE} {OF} ‘{SCIENTIFIC} {KNOWLEDGE}’: {Social} {Knowing}},
	volume = {24},
	issn = {15208583},
	shorttitle = {{SOCIAL} {KNOWING}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1520-8583.2010.00184.x},
	doi = {10.1111/j.1520-8583.2010.00184.x},
	language = {en},
	number = {1},
	urldate = {2021-10-15},
	journal = {Philosophical Perspectives},
	author = {Bird, Alexander},
	month = dec,
	year = {2010},
	pages = {23--56},
	file = {Bird - 2010 - SOCIAL KNOWING THE SOCIAL SENSE OF ‘SCIENTIFIC KN.pdf:/Users/neilnatarajan/Zotero/storage/VA66UKK5/Bird - 2010 - SOCIAL KNOWING THE SOCIAL SENSE OF ‘SCIENTIFIC KN.pdf:application/pdf;markup:/Users/neilnatarajan/Zotero/storage/UQYJZ2CS/Bird - 2010 - SOCIAL KNOWING THE SOCIAL SENSE OF ‘SCIENTIFIC KN.pdf:application/pdf},
}

@misc{university_ai_2021,
	title = {{AI} empowers environmental regulators},
	url = {https://news.stanford.edu/2021/04/19/ai-empowers-environmental-regulators/},
	abstract = {Monitoring environmental compliance is a particular challenge for governments in poor countries. A new machine learning approach that uses satellite imagery to pinpoint highly polluting brick kilns in Bangladesh could provide a low-cost solution.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Stanford News},
	author = {University, Stanford},
	month = apr,
	year = {2021},
	note = {Section: Science \& Technology},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/LUVEPW3N/ai-empowers-environmental-regulators.html:text/html;University - 2021 - AI empowers environmental regulators.pdf:/Users/neilnatarajan/Zotero/storage/JNAAFLZX/University - 2021 - AI empowers environmental regulators.pdf:application/pdf},
}

@incollection{pietsch_causation_2017,
	title = {Causation, probability, and all that: {Data} science as a novel inductive paradigm},
	isbn = {978-1-315-15640-8},
	shorttitle = {Causation, probability, and all that},
	abstract = {A brief survey of the literature suggests that there is anything but a consensus on the and related issues concerning the foundations of data science. The fundamental distinction between enumerative and eliminative induction is briefly introduced, the former focusing on the mere repetition of phenomena, the latter on the variation of phenomena. The chapter argues that eliminative induction provides a much more plausible and realistic picture of actual scientific practice. Moreover, an account of causation is outlined that corresponds to eliminative induction and that allows establishing the crucial distinction between relationships that are purely accidental and those that allow for prediction and manipulation. The methodological framework sketched in section "Causation" relies on the assumption of determinism, which certainly cannot be upheld for most applications of data science. The chapter sketches an objective, nonfrequency interpretation of probability that relies on symmetries in the causal structure of probabilistic phenomena to establish probability values.},
	booktitle = {Frontiers in {Data} {Science}},
	publisher = {CRC Press},
	author = {Pietsch, Wolfgang ∗},
	year = {2017},
	note = {Num Pages: 25},
}

@article{pietsch_aspects_2015,
	title = {Aspects of {Theory}-{Ladenness} in {Data}-{Intensive} {Science}},
	volume = {82},
	issn = {0031-8248, 1539-767X},
	url = {https://www.journals.uchicago.edu/doi/10.1086/683328},
	doi = {10.1086/683328},
	abstract = {Recent claims, mainly from computer scientists, concerning a largely automated and modelfree data-intensive science have been countered by critical reactions from a number of philosophers of science. The debate suffers from a lack of detail in two respects, regarding (i) the actual methods used in data-intensive science and (ii) the specific ways in which these methods presuppose theoretical assumptions. I examine two widely-used algorithms, classificatory trees and non-parametric regression, and argue that these are theory-laden in an external sense, regarding the framing of research questions, but not in an internal sense concerning the causal structure of the examined phenomenon. With respect to the novelty of data-intensive science, I draw an analogy to exploratory as opposed to theory-directed experimentation.},
	language = {en},
	number = {5},
	urldate = {2021-10-15},
	journal = {Philosophy of Science},
	author = {Pietsch, Wolfgang},
	month = dec,
	year = {2015},
	pages = {905--916},
	file = {markup:/Users/neilnatarajan/Zotero/storage/YKB4W25Z/Pietsch - 2015 - Aspects of Theory-Ladenness in Data-Intensive Scie.pdf:application/pdf;Pietsch - 2015 - Aspects of Theory-Ladenness in Data-Intensive Scie.pdf:/Users/neilnatarajan/Zotero/storage/2SS27WGB/Pietsch - 2015 - Aspects of Theory-Ladenness in Data-Intensive Scie.pdf:application/pdf},
}

@inproceedings{jacovi_formalizing_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Formalizing {Trust} in {Artificial} {Intelligence}: {Prerequisites}, {Causes} and {Goals} of {Human} {Trust} in {AI}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Formalizing {Trust} in {Artificial} {Intelligence}},
	url = {https://doi.org/10.1145/3442188.3445923},
	doi = {10.1145/3442188.3445923},
	abstract = {Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.},
	urldate = {2021-10-15},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Jacovi, Alon and Marasović, Ana and Miller, Tim and Goldberg, Yoav},
	month = mar,
	year = {2021},
	keywords = {artificial intelligence, contractual trust, distrust, formalization, sociology, trust, trustworthy, warranted trust},
	pages = {624--635},
	file = {Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/R729TD7X/Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:application/pdf;Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/IGT2LYYQ/Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf:application/pdf},
}

@article{korb_introduction_2004,
	title = {Introduction: {Machine} {Learning} as {Philosophy} of {Science}},
	volume = {14},
	issn = {1572-8641},
	shorttitle = {Introduction},
	url = {https://doi.org/10.1023/B:MIND.0000045986.90956.7f},
	doi = {10.1023/B:MIND.0000045986.90956.7f},
	abstract = {I consider three aspects in which machine learning and philosophy of science can illuminate each other: methodology, inductive simplicity and theoretical terms. I examine the relations between the two subjects and conclude by claiming these relations to be very close.},
	language = {en},
	number = {4},
	urldate = {2021-10-15},
	journal = {Minds and Machines},
	author = {Korb, Kevin B.},
	month = nov,
	year = {2004},
	pages = {433--440},
	file = {Korb_2004_Introduction_annotated.pdf:/Users/neilnatarajan/Zotero/storage/E7RF8LYZ/Korb_2004_Introduction_annotated.pdf:application/pdf;Korb_2004_Introduction.pdf:/Users/neilnatarajan/Zotero/storage/E7RF8LYZ/Korb_2004_Introduction.pdf:application/pdf},
}

@inproceedings{karimi_algorithmic_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Algorithmic {Recourse}: from {Counterfactual} {Explanations} to {Interventions}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Algorithmic {Recourse}},
	url = {https://doi.org/10.1145/3442188.3445899},
	doi = {10.1145/3442188.3445899},
	abstract = {As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -"how the world would have (had) to be different for a desirable outcome to occur"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.},
	urldate = {2021-10-15},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Karimi, Amir-Hossein and Schölkopf, Bernhard and Valera, Isabel},
	month = mar,
	year = {2021},
	keywords = {algorithmic recourse, causal inference, consequential recommendations, contrastive explanations, counterfactual explanations, explainable artificial intelligence, minimal interventions},
	pages = {353--362},
	file = {Karimi et al_2021_Algorithmic Recourse.pdf:/Users/neilnatarajan/Zotero/storage/CQFZDLXK/Karimi et al_2021_Algorithmic Recourse.pdf:application/pdf},
}

@article{nyrup_water_2020,
	title = {‘{Of} {Water} {Drops} and {Atomic} {Nuclei}: {Analogies} and {Pursuit} {Worthiness} in {Science}’},
	volume = {71},
	issn = {0007-0882},
	shorttitle = {‘{Of} {Water} {Drops} and {Atomic} {Nuclei}},
	url = {https://www.journals.uchicago.edu/doi/full/10.1093/bjps/axy036},
	doi = {10.1093/bjps/axy036},
	abstract = {This article highlights a use of analogies in science that so far has received relatively little systematic discussion: providing reasons for pursuing a model or theory. Using the development of the liquid drop model as a test case, I critically assess two extant pursuit worthiness accounts: (i) that analogies justify pursuit by supporting plausibility arguments and (ii) that analogies can serve as a guide to potential theoretical unification. Neither of these fit the liquid drop model case. Instead, I develop an alternative account, based on the idea that analogies facilitate the transfer of a well-understood modelling strategy to a new domain.

1.  Introduction

2.  Case Study: The Development of the Liquid Drop Model

3.  Plausibility Accounts

3.1.  Bartha on plausibility and analogical inference

3.2.  Plausibility and the drop analogy

4.  Analogies as a Guide to Unification

5.  Generative Accounts

5.1.  Analogy-based modelling strategies

5.2.  Did analogies play a merely generative role?

6.  A New Pursuit Worthiness Account of Analogies

6.1.  Transferring understanding-with through analogies

6.2.  Understanding-with and the liquid drop model

7.  Conclusion},
	number = {3},
	urldate = {2021-10-15},
	journal = {The British Journal for the Philosophy of Science},
	author = {Nyrup, Rune},
	month = sep,
	year = {2020},
	note = {Publisher: The University of Chicago Press},
	pages = {881--903},
	file = {Nyrup_2020_‘Of Water Drops and Atomic Nuclei.pdf:/Users/neilnatarajan/Zotero/storage/JT36FMWN/Nyrup_2020_‘Of Water Drops and Atomic Nuclei.pdf:application/pdf},
}

@inproceedings{ron_corporate_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Corporate {Social} {Responsibility} via {Multi}-{Armed} {Bandits}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445868},
	doi = {10.1145/3442188.3445868},
	abstract = {We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.},
	urldate = {2021-10-15},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Ron, Tom and Ben-Porat, Omer and Shalit, Uri},
	month = mar,
	year = {2021},
	pages = {26--40},
	file = {Ron et al_2021_Corporate Social Responsibility via Multi-Armed Bandits.pdf:/Users/neilnatarajan/Zotero/storage/WHH4SDZU/Ron et al_2021_Corporate Social Responsibility via Multi-Armed Bandits.pdf:application/pdf},
}

@article{king_functional_2004,
	title = {Functional genomic hypothesis generation and experimentation by a robot scientist},
	volume = {427},
	copyright = {2004 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature02236},
	doi = {10.1038/nature02236},
	abstract = {The question of whether it is possible to automate the scientific process is of both great theoretical interest1,2 and increasing practical importance because, in many scientific areas, data are being generated much faster than they can be effectively analysed. We describe a physically implemented robotic system that applies techniques from artificial intelligence3,4,5,6,7,8 to carry out cycles of scientific experimentation. The system automatically originates hypotheses to explain observations, devises experiments to test these hypotheses, physically runs the experiments using a laboratory robot, interprets the results to falsify hypotheses inconsistent with the data, and then repeats the cycle. Here we apply the system to the determination of gene function using deletion mutants of yeast (Saccharomyces cerevisiae) and auxotrophic growth experiments9. We built and tested a detailed logical model (involving genes, proteins and metabolites) of the aromatic amino acid synthesis pathway. In biological experiments that automatically reconstruct parts of this model, we show that an intelligent experiment selection strategy is competitive with human performance and significantly outperforms, with a cost decrease of 3-fold and 100-fold (respectively), both cheapest and random-experiment selection.},
	language = {en},
	number = {6971},
	urldate = {2021-10-15},
	journal = {Nature},
	author = {King, Ross D. and Whelan, Kenneth E. and Jones, Ffion M. and Reiser, Philip G. K. and Bryant, Christopher H. and Muggleton, Stephen H. and Kell, Douglas B. and Oliver, Stephen G.},
	month = jan,
	year = {2004},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6971
Primary\_atype: Research
Publisher: Nature Publishing Group},
	pages = {247--252},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/6VNL26IW/King et al. - 2004 - Functional genomic hypothesis generation and exper.pdf:application/pdf;King et al. (2004) Supplementary Material.pdf:/Users/neilnatarajan/Zotero/storage/DDXG2PDV/King et al. (2004) Supplementary Material.pdf:application/pdf;markup:/Users/neilnatarajan/Zotero/storage/UUNNIQMK/King et al. - 2004 - Functional genomic hypothesis generation and exper.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/4D2NUMUZ/nature02236.html:text/html},
}

@article{king_automation_2009,
	title = {The {Automation} of {Science}},
	volume = {324},
	url = {https://www.science.org/doi/10.1126/science.1165620},
	doi = {10.1126/science.1165620},
	number = {5923},
	urldate = {2021-10-15},
	journal = {Science},
	author = {King, Ross D. and Rowland, Jem and Oliver, Stephen G. and Young, Michael and Aubrey, Wayne and Byrne, Emma and Liakata, Maria and Markham, Magdalena and Pir, Pinar and Soldatova, Larisa N. and Sparkes, Andrew and Whelan, Kenneth E. and Clare, Amanda},
	month = apr,
	year = {2009},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {85--89},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/UZU2KRQS/King et al. - 2009 - The Automation of Science.pdf:application/pdf;markup:/Users/neilnatarajan/Zotero/storage/IFI7FHCS/King et al. - 2009 - The Automation of Science.pdf:application/pdf},
}

@article{jones_renewable_2021,
	chapter = {Business},
	title = {Renewable electricity deals investigated by {UK} government},
	issn = {0261-3077},
	url = {https://www.theguardian.com/business/2021/aug/15/renewable-electricity-deals-investigated-by-uk-government},
	abstract = {Plan is to tighten rules to stop energy firms exaggerating environmental benefits of green tariffs},
	language = {en-GB},
	urldate = {2021-10-15},
	journal = {The Guardian},
	author = {Jones, Rupert},
	month = aug,
	year = {2021},
	keywords = {Environment, Business, Consumer affairs, Energy, Energy industry, Money, Renewable energy, UK news},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/2QHG7DXF/renewable-electricity-deals-investigated-by-uk-government.html:text/html},
}

@misc{vizzuality_climate_nodate,
	title = {Climate {Change} {Laws} of the {World}},
	url = {https://climate-laws.org/litigation_cases},
	abstract = {Climate Change Laws of the World is a global database of climate change laws, policies, climate targets and litigation cases},
	urldate = {2021-10-15},
	author = {Vizzuality},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/6MKRG3VH/litigation_cases.html:text/html},
}

@misc{noauthor_climate_nodate,
	title = {Climate change governance, legislation and litigation {Archives}},
	url = {https://www.lse.ac.uk/granthaminstitute/research-areas/climate-change-governance-legislation-and-litigation/},
	abstract = {This research area covers the governance and political economy of transformations to low-carbon and climate-resilient societies at the international and domestic levels, including the implementation of the Paris Agreement. It also covers the specific roles of legislation, litigation, sub-national government and the private sector in these processes. Our Climate Change Laws of the World database is part of this research area.},
	language = {en-GB},
	urldate = {2021-10-15},
	journal = {Grantham Research Institute on climate change and the environment},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/9NXWLHZD/climate-change-governance-legislation-and-litigation.html:text/html},
}

@inproceedings{benami_distributive_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {The {Distributive} {Effects} of {Risk} {Prediction} in {Environmental} {Compliance}: {Algorithmic} {Design}, {Environmental} {Justice}, and {Public} {Policy}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {The {Distributive} {Effects} of {Risk} {Prediction} in {Environmental} {Compliance}},
	url = {https://doi.org/10.1145/3442188.3445873},
	doi = {10.1145/3442188.3445873},
	abstract = {Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.},
	urldate = {2021-10-15},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Benami, Elinor and Whitaker, Reid and La, Vincent and Lin, Hongjin and Anderson, Brandon R. and Ho, Daniel E.},
	month = mar,
	year = {2021},
	keywords = {environmental justice, environmental protection, fairness, government, risk models},
	pages = {90--105},
	file = {Benami et al_2021_The Distributive Effects of Risk Prediction in Environmental Compliance.pdf:/Users/neilnatarajan/Zotero/storage/NFH4JBFZ/Benami et al_2021_The Distributive Effects of Risk Prediction in Environmental Compliance.pdf:application/pdf},
}

@misc{university_assessing_2021,
	title = {Assessing regulatory fairness through machine learning},
	url = {https://news.stanford.edu/2021/03/08/assessing-regulatory-fairness-machine-learning/},
	abstract = {Applying machine learning to a U.S. Environmental Protection Agency initiative reveals how key design elements determine what communities bear the burden of pollution. The approach could help ensure fairness and accountability in machine learning used by government regulators.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Stanford News},
	author = {University, Stanford},
	month = mar,
	year = {2021},
	note = {Section: Science \& Technology},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/MYVIMTMC/assessing-regulatory-fairness-machine-learning.html:text/html},
}

@article{luccioni_using_nodate,
	title = {Using {Natural} {Language} {Processing} to {Analyze} {Financial} {Climate} {Disclosures}},
	abstract = {According to U.S. ﬁnancial legislation, companies traded on the stock market are obliged to regularly disclose risks and uncertainties that are likely to affect their operations or ﬁnancial position. Since 2010, these disclosures must also include climate-related risk projections. These disclosures therefore present a large quantity of textual information on which we can apply NLP techniques in order to pinpoint the companies that divulge their climate risks and those that do not, the types of vulnerabilities that are disclosed, and to follow the evolution of these risks over time.},
	language = {en},
	author = {Luccioni, Alexandra and Palacios, Hector},
	pages = {3},
	file = {Luccioni_Palacios_Using Natural Language Processing to Analyze Financial Climate Disclosures.pdf:/Users/neilnatarajan/Zotero/storage/2Y6ZYC8M/Luccioni_Palacios_Using Natural Language Processing to Analyze Financial Climate Disclosures.pdf:application/pdf},
}

@article{laville_ai_2021,
	chapter = {Environment},
	title = {{AI} reveals 1,000 'dark discharges' of untreated sewage in {England}},
	issn = {0261-3077},
	url = {https://www.theguardian.com/environment/2021/mar/12/ai-reveals-1000-dark-discharges-of-untreated-sewage-in-england},
	abstract = {Paper says machine learning could prove crucial tool in efforts to improve quality of country’s rivers},
	language = {en-GB},
	urldate = {2021-10-15},
	journal = {The Guardian},
	author = {Laville, Sandra},
	month = mar,
	year = {2021},
	keywords = {Environment, UK news, Artificial intelligence (AI), Pollution, Rivers, Technology},
	file = {Laville - 2021 - AI reveals 1,000 'dark discharges' of untreated se.pdf:/Users/neilnatarajan/Zotero/storage/VMGW3S4J/Laville - 2021 - AI reveals 1,000 'dark discharges' of untreated se.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/VPFLRMAS/ai-reveals-1000-dark-discharges-of-untreated-sewage-in-england.html:text/html},
}

@article{connolly_detecting_2017,
	title = {Detecting peatland drains with {Object} {Based} {Image} {Analysis} and {Geoeye}-1 imagery},
	volume = {12},
	doi = {10.1186/s13021-017-0075-z},
	abstract = {Background
Peatlands play an important role in the global carbon cycle. They provide important ecosystem services including carbon sequestration and storage. Drainage disturbs peatland ecosystem services. Mapping drains is difficult and expensive and their spatial extent is, in many cases, unknown. An object based image analysis (OBIA) was performed on a very high resolution satellite image (Geoeye-1) to extract information about drain location and extent on a blanket peatland in Ireland. Two accuracy assessment methods: Error matrix and the completeness, correctness and quality (CCQ) were used to assess the extracted data across the peatland and at several sub sites. The cost of the OBIA method was compared with manual digitisation and field survey. The drain maps were also used to assess the costs relating to blocking drains vs. a business-as-usual scenario and estimating the impact of each on carbon fluxes at the study site. ResultsThe OBIA method performed well at almost all sites. Almost 500 km of drains were detected within the peatland. In the error matrix method, overall accuracy (OA) of detecting the drains was 94\% and the kappa statistic was 0.66. The OA for all sub-areas, except one, was 95–97\%. The CCQ was 85\%, 85\% and 71\% respectively. The OBIA method was the most cost effective way to map peatland drains and was at least 55\% cheaper than either field survey or manual digitisation, respectively. The extracted drain maps were used constrain the study area CO2 flux which was 19\% smaller than the prescribed Peatland Code value for drained peatlands. Conclusions
The OBIA method used in this study showed that it is possible to accurately extract maps of fine scale peatland drains over large areas in a cost effective manner. The development of methods to map the spatial extent of drains is important as they play a critical role in peatland carbon dynamics. The objective of this study was to extract data on the spatial extent of drains on a blanket bog in the west of Ireland. The results show that information on drain extent and location can be extracted from high resolution imagery and mapped with a high degree of accuracy. Under Article 3.4 of the Kyoto Protocol Annex 1 parties can account for greenhouse gas emission by sources and removals by sinks resulting from “wetlands drainage and rewetting”. The ability to map the spatial extent, density and location of peatlands drains means that Annex 1 parties can develop strategies for drain blocking to aid reduction of CO2 emissions, DOC runoff and water discoloration. This paper highlights some uncertainty around using one-size-fits-all emission factors for GHG in drained peatlands and re-wetting scenarios. However, the OBIA method is robust and accurate and could be used to assess the extent of drains in peatlands across the globe aiding the refinement of peatland carbon dynamics .},
	journal = {Carbon Balance and Management},
	author = {Connolly, John and Holden, N.},
	month = mar,
	year = {2017},
	file = {Connolly_Holden_2017_Detecting peatland drains with Object Based Image Analysis and Geoeye-1 imagery.pdf:/Users/neilnatarajan/Zotero/storage/CNQC7SU6/Connolly_Holden_2017_Detecting peatland drains with Object Based Image Analysis and Geoeye-1 imagery.pdf:application/pdf},
}

@misc{noauthor_river_nodate,
	title = {River {Action} {UK}},
	url = {https://riveractionuk.com/},
	abstract = {River Action is a campaigning body committed to addressing the severe problem of river pollution across the UK.},
	language = {en-US},
	urldate = {2021-10-15},
	journal = {River Action UK},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/QHUVWVWT/riveractionuk.com.html:text/html},
}

@misc{noauthor_wye_nodate,
	title = {Wye {CitSci} {Water} {Quality} {Monitoring} {Map}},
	url = {https://www.brecon-and-radnor-cprw.wales/WyeCitSci/},
	urldate = {2021-10-15},
	file = {Wye CitSci Water Quality Monitoring Map:/Users/neilnatarajan/Zotero/storage/NCX6CY9B/WyeCitSci.html:text/html},
}

@article{monbiot_britains_2021,
	chapter = {Opinion},
	title = {Britain’s rivers are suffocating to death},
	issn = {0261-3077},
	url = {https://www.theguardian.com/commentisfree/2021/jul/21/britains-rivers-suffocating-industrial-farm-waste},
	abstract = {Water that should be crystal clear has become a green-brown slop of microscopic algae because of industrial farm waste, says Guardian columnist George Monbiot},
	language = {en-GB},
	urldate = {2021-10-15},
	journal = {The Guardian},
	author = {Monbiot, George},
	month = jul,
	year = {2021},
	keywords = {Environment, UK news, Pollution, Rivers, Animals, Biodiversity, Conservation, Farming, Plants, Wildlife},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/G5YMKKR4/britains-rivers-suffocating-industrial-farm-waste.html:text/html},
}

@article{minasny_digital_2019,
	title = {Digital mapping of peatlands – {A} critical review},
	volume = {196},
	issn = {00128252},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S001282521830360X},
	doi = {10.1016/j.earscirev.2019.05.014},
	abstract = {Peatlands offer a series of ecosystem services including carbon storage, biomass production, and climate regulation. Climate change and rapid land use change are degrading peatlands, liberating their stored carbon (C) into the atmosphere. To conserve peatlands and help in realising the Paris Agreement, we need to understand their extent, status, and C stocks. However, current peatland knowledge is vague—estimates of global peatland extent ranges from 1 to 4.6 million km2, and C stock estimates vary between 113 and 612 Pg (or billion tonne C). This uncertainty mostly stems from the coarse spatial scale of global soil maps. In addition, most global peatland estimates are based on rough country inventories and reports that use outdated data. This review shows that digital mapping using field observations combined with remotely-sensed images and statistical models is an avenue to more accurately map peatlands and decrease this knowledge gap. We describe peat mapping experiences from 12 countries or regions and review 90 recent studies on peatland mapping. We found that interest in mapping peat information derived from satellite imageries and other digital mapping technologies is growing. Many studies have delineated peat extent using land cover from remote sensing, ecology, and environmental field studies, but rarely perform validation, and calculating the uncertainty of prediction is rare. This paper then reviews various proximal and remote sensing techniques that can be used to map peatlands. These include geophysical measurements (electromagnetic induction, resistivity measurement, and gamma radiometrics), radar sensing (SRTM, SAR), and optical images (Visible and Infrared). Peatland is better mapped when using more than one covariate, such as optical and radar products using nonlinear machine learning algorithms. The proliferation of satellite data available in an open-access format, availability of machine learning algorithms in an open-source computing environment, and high-performance computing facilities could enhance the way peatlands are mapped. Digital soil mapping allows us to map peat in a cost-effective, objective, and accurate manner. Securing peatlands for the future, and abating their contribution to atmospheric C levels, means digitally mapping them now.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Earth-Science Reviews},
	author = {Minasny, Budiman and Berglund, Örjan and Connolly, John and Hedley, Carolyn and de Vries, Folkert and Gimona, Alessandro and Kempen, Bas and Kidd, Darren and Lilja, Harry and Malone, Brendan and McBratney, Alex and Roudier, Pierre and O'Rourke, Sharon and {Rudiyanto} and Padarian, José and Poggio, Laura and ten Caten, Alexandre and Thompson, Daniel and Tuve, Clint and Widyatmanti, Wirastuti},
	month = sep,
	year = {2019},
	pages = {102870},
	file = {Minasny et al_2019_Digital mapping of peatlands – A critical review.pdf:/Users/neilnatarajan/Zotero/storage/EGARA839/Minasny et al_2019_Digital mapping of peatlands – A critical review.pdf:application/pdf},
}

@article{banerjee_microcredit_2013,
	title = {Microcredit {Under} the {Microscope}: {What} {Have} {We} {Learned} in the {Past} {Two} {Decades}, and {What} {Do} {We} {Need} to {Know}?},
	volume = {5},
	issn = {1941-1383, 1941-1391},
	shorttitle = {Microcredit {Under} the {Microscope}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-economics-082912-110220},
	doi = {10.1146/annurev-economics-082912-110220},
	abstract = {Research on microcredit is now two decades old. There has been enormous progress in understanding both what microcredit does and how. Yet a lot of what we have learned has raised new and often quite fundamental questions about its nature: Is microcredit primarily about investment, consumption, or savings? Why is it that the investments financed by microcredit do not always lead to income growth, and does this have to do with the structure of microlending? What are the roles of social capital, reputation, and group lending? This article attempts to take stock of this significant body of work and tries to identify the most important questions for future research.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Annual Review of Economics},
	author = {Banerjee, Abhijit Vinayak},
	month = aug,
	year = {2013},
	pages = {487--519},
	file = {Banerjee - 2013 - Microcredit Under the Microscope What Have We Lea.pdf:/Users/neilnatarajan/Zotero/storage/NFHC8VCE/Banerjee - 2013 - Microcredit Under the Microscope What Have We Lea.pdf:application/pdf;Markup:/Users/neilnatarajan/Zotero/storage/CML4FSHM/Banerjee - 2013 - Microcredit Under the Microscope What Have We Lea.pdf:application/pdf},
}

@techreport{karlan_hoping_2012,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Hoping to {Win}, {Expected} to {Lose}: {Theory} and {Lessons} on {Microenterprise} {Development}},
	shorttitle = {Hoping to {Win}, {Expected} to {Lose}},
	url = {https://papers.ssrn.com/abstract=2226588},
	abstract = {We show how financial and managerial constraints impede experimentation and thus limit learning about the profitability of investments. Imperfect information about one’s own type, but willingness to experiment to learn one’s type, leads to short-run negative expected returns to investments, with some outliers succeeding. We find in an experiment that entrepreneurs invest randomized grants of cash and adopt advice from randomized grants of consulting services, but both lead to lower profits on average. In the long run, they revert back to their prior scale of operations. In a meta-analysis, results from 19 other experiments find mixed support for this theory.},
	language = {en},
	number = {ID 2226588},
	urldate = {2021-10-28},
	institution = {Social Science Research Network},
	author = {Karlan, Dean S. and Knight, Ryan and Udry, Christopher},
	month = nov,
	year = {2012},
	doi = {10.2139/ssrn.2226588},
	keywords = {business training, consulting, credit constraints, entrepreneurship, managerial capital},
	file = {Karlan et al_2012_Hoping to Win, Expected to Lose.pdf:/Users/neilnatarajan/Zotero/storage/JNU35XYZ/Karlan et al_2012_Hoping to Win, Expected to Lose.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/IZSAG65G/papers.html:text/html},
}

@inproceedings{gordon_disagreement_2021,
	address = {Yokohama Japan},
	title = {The {Disagreement} {Deconvolution}: {Bringing} {Machine} {Learning} {Performance} {Metrics} {In} {Line} {With} {Reality}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {The {Disagreement} {Deconvolution}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445423},
	doi = {10.1145/3411764.3445423},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Gordon, Mitchell L. and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S.},
	month = may,
	year = {2021},
	pages = {1--14},
	file = {Gordon et al_2021_The Disagreement Deconvolution.pdf:/Users/neilnatarajan/Zotero/storage/MYR9QZ5A/Gordon et al_2021_The Disagreement Deconvolution.pdf:application/pdf},
}

@misc{noauthor_chef_nodate,
	title = {Chef {Watson} - {IBM}},
	copyright = {© Copyright IBM Corp. 2016},
	url = {https://researcher.watson.ibm.com/researcher/researcher.watson.ibm.com/researcher/view_group.php},
	abstract = {IBM Research},
	language = {en-US},
	urldate = {2021-11-04},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/B8RXQEXV/view_group.html:text/html},
}

@article{miller_explanation_2017,
	title = {Explanation in {Artificial} {Intelligence}: {Insights} from the {Social} {Sciences}},
	shorttitle = {Explanation in {Artificial} {Intelligence}},
	url = {https://arxiv.org/abs/1706.07269v3},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language = {en},
	urldate = {2021-11-08},
	author = {Miller, Tim},
	month = jun,
	year = {2017},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/ATHVMUH4/Miller - 2017 - Explanation in Artificial Intelligence Insights f.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/L7F658C4/1706.html:text/html},
}

@inproceedings{smart_why_2020,
	address = {New York, NY, USA},
	series = {{AIES} '20},
	title = {Why {Reliabilism} {Is} not {Enough}: {Epistemic} and {Moral} {Justification} in {Machine} {Learning}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {Why {Reliabilism} {Is} not {Enough}},
	url = {https://doi.org/10.1145/3375627.3375866},
	doi = {10.1145/3375627.3375866},
	abstract = {In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of {\textbackslash}em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method {\textbackslash}citegoldman2012reliabilism. We argue that, in cases where model deployments require {\textbackslash}em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral "wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.},
	urldate = {2021-11-25},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Smart, Andrew and James, Larry and Hutchinson, Ben and Wu, Simone and Vallor, Shannon},
	month = feb,
	year = {2020},
	keywords = {epistemology, explainability, interpretability, machine learning, moral justification, neural networks, \_tablet},
	pages = {372--377},
	file = {Smart et al_2020_Why Reliabilism Is not Enough.pdf:/Users/neilnatarajan/Zotero/storage/V57Y3F9I/Smart et al_2020_Why Reliabilism Is not Enough.pdf:application/pdf},
}

@misc{preserve_knowledge_nips_2018,
	title = {{NIPS} 2017 {Test} of {Time} {Award} "{Machine} learning has become alchemy.” {\textbar} {Ali} {Rahimi}, {Google}},
	url = {https://www.youtube.com/watch?v=x7psGHgatGM},
	urldate = {2021-11-25},
	author = {{Preserve Knowledge}},
	month = mar,
	year = {2018},
}

@misc{pearl_book_nodate,
	title = {The {Book} of {Why}},
	url = {https://www.penguin.co.uk/books/289825/the-book-of-why/9780141982410},
	abstract = {The hugely influential book on how the understanding of causality revolutionized science and the world, by the pioneer of artificial intelligence 'Wonderful ... illuminating and fun to read' Daniel Kahneman, Nobel Prize-winner and author of Thinking, Fast and Slow 'Correlation does not imply causation.' For decades, this mantra was invoked by scientists in order to avoid taking positions as to whether one thing caused another, such as smoking and cancer, or carbon dioxide and global warming. But today, that taboo is dead. The causal revolution, sparked by world-renowned computer scientist Judea Pearl and his colleagues, has cut through a century of confusion and placed cause and effect on a firm scientific basis. Now, Pearl and science journalist Dana Mackenzie explain causal thinking to general readers for the first time, showing how it allows us to explore the world that is and the worlds that could have been. It is the essence of human and artificial intelligence. And just as Pearl's discoveries have enabled machines to think better, The Book of Why explains how we too can think better. 'Pearl's accomplishments over the last 30 years have provided the theoretical basis for progress in artificial intelligence and have redefined the term "thinking machine"' Vint Cerf},
	language = {en},
	urldate = {2021-11-25},
	author = {Pearl, Judea and Mackenzie, Dana},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/PZ2NS99C/9780141982410.html:text/html},
}

@article{sterkenburg_no-free-lunch_2021,
	title = {The no-free-lunch theorems of supervised learning},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/s11229-021-03233-1},
	doi = {10.1007/s11229-021-03233-1},
	abstract = {The no-free-lunch theorems promote a skeptical conclusion that all possible machine learning algorithms equally lack justification. But how could this leave room for a learning theory, that shows that some algorithms are better than others? Drawing parallels to the philosophy of induction, we point out that the no-free-lunch results presuppose a conception of learning algorithms as purely data-driven. On this conception, every algorithm must have an inherent inductive bias, that wants justification. We argue that many standard learning algorithms should rather be understood as model-dependent: in each application they also require for input a model, representing a bias. Generic algorithms themselves, they can be given a model-relative justification.},
	language = {en},
	urldate = {2021-11-25},
	journal = {Synthese},
	author = {Sterkenburg, Tom F. and Grünwald, Peter D.},
	month = jun,
	year = {2021},
	file = {Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning_annotated.pdf:/Users/neilnatarajan/Zotero/storage/5F83F632/Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning_annotated.pdf:application/pdf;Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning.pdf:/Users/neilnatarajan/Zotero/storage/5F83F632/Sterkenburg_Grünwald_2021_The no-free-lunch theorems of supervised learning.pdf:application/pdf},
}

@misc{university_7_2020,
	title = {7 - {Causal} {Inference}},
	url = {https://blog.ml.cmu.edu/2020/08/31/7-causality/},
	abstract = {The rules of causality play a role in almost everything we do. Criminal conviction is based on the principle of being the cause of a crime (guilt) as judged by a jury and most of us consider the effects of our actions before we make a decision. Therefore, it is reasonable to assume that considering},
	language = {en-US},
	urldate = {2021-11-25},
	journal = {Machine Learning Blog {\textbar} ML@CMU {\textbar} Carnegie Mellon University},
	author = {University, Carnegie Mellon, Machine Learning Department},
	month = aug,
	year = {2020},
	note = {Section: Educational},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/BTVK8Y82/7-causality.html:text/html},
}

@book{pearl_causal_2016,
	address = {Chichester, West Sussex},
	title = {Causal inference in statistics: a primer},
	isbn = {978-1-119-18684-7},
	shorttitle = {Causal inference in statistics},
	language = {en},
	publisher = {Wiley},
	author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
	year = {2016},
	keywords = {Causation, Mathematical statistics, Probabilities},
	file = {Pearl et al_2016_Causal inference in statistics.pdf:/Users/neilnatarajan/Zotero/storage/53URLYGC/Pearl et al_2016_Causal inference in statistics.pdf:application/pdf},
}

@article{lucas_causal_nodate,
	title = {Causal {Inference}},
	url = {https://blog.ml.cmu.edu/2020/08/31/7-causality/},
	author = {Lucas, Keane and Huang, Biwei and Stelmakh, Ivan},
}

@inproceedings{biran_explanation_2017,
	title = {Explanation and {Justification} in {Machine} {Learning} : {A} {Survey} {Or}},
	author = {Biran, Or and Cotton, Courtenay V.},
	year = {2017},
}

@article{chun_tie_grounded_2019,
	title = {Grounded theory research: {A} design framework for novice researchers},
	volume = {7},
	journal = {Open Medicine},
	author = {Chun Tie, Ylona and Birks, Melanie and Francis, Karen},
	month = jan,
	year = {2019},
	pages = {1--8},
}

@misc{dua_uci_2017,
	title = {{UCI} {Machine} {Learning} {Repository}},
	url = {http://archive.ics.uci.edu/ml},
	publisher = {University of California, Irvine, School of Information and Computer Sciences},
	author = {Dua, Dheeru and Graff, Casey},
	year = {2017},
}

@article{lundberg_consistent_2018,
	title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	volume = {abs/1802.03888},
	url = {http://arxiv.org/abs/1802.03888},
	journal = {CoRR},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	year = {2018},
	note = {\_eprint: 1802.03888},
}

@article{miller_explanation_2017-1,
	title = {Explanation in {Artificial} {Intelligence}: {Insights} from the {Social} {Sciences}},
	volume = {abs/1706.07269},
	url = {http://arxiv.org/abs/1706.07269},
	journal = {CoRR},
	author = {Miller, Tim},
	year = {2017},
	note = {\_eprint: 1706.07269},
}

@book{molnar_interpretable_2019,
	title = {Interpretable {Machine} {Learning}: {A} {Guide} for {Making} {Black} {Box} {Models} {Explainable}},
	author = {Molnar, Christoph},
	year = {2019},
}

@article{mothilal_explaining_2019,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	volume = {abs/1905.07697},
	url = {http://arxiv.org/abs/1905.07697},
	journal = {CoRR},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	year = {2019},
	note = {\_eprint: 1905.07697},
}

@article{rudin_why_2019,
	title = {Why {Are} {We} {Using} {Black} {Box} {Models} in {AI} {When} {We} {Don}’t {Need} {To}? {A} {Lesson} {From} {An} {Explainable} {AI} {Competition}},
	volume = {1},
	url = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
	doi = {10.1162/99608f92.5a8a3a3d},
	number = {2},
	journal = {Harvard Data Science Review},
	author = {Rudin, Cynthia and Radin, Joanna},
	month = nov,
	year = {2019},
}

@article{wachter_counterfactual_2017,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	volume = {abs/1711.00399},
	url = {http://arxiv.org/abs/1711.00399},
	journal = {CoRR},
	author = {Wachter, Sandra and Mittelstadt, Brent D. and Russell, Chris},
	year = {2017},
	note = {\_eprint: 1711.00399},
}

@incollection{woodward_scientific_2021,
	edition = {Spring 2021},
	title = {Scientific {Explanation}},
	url = {https://plato.stanford.edu/archives/spr2021/entries/scientific-explanation/},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Woodward, James},
	editor = {Zalta, Edward N.},
	year = {2021},
}

@article{malik_hierarchy_2020,
	title = {A {Hierarchy} of {Limitations} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/2002.05193},
	abstract = {"All models are wrong, but some are useful", wrote George E. P. Box (1979). Machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. This paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. Machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. The limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance.},
	urldate = {2021-11-25},
	journal = {arXiv:2002.05193 [cs, econ, math, stat]},
	author = {Malik, Momin M.},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.05193},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society, Economics - Econometrics, G.3, I.6.4, J.4, Mathematics - Statistics Theory},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/L9M4H87C/2002.html:text/html;Malik_2020_A Hierarchy of Limitations in Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/E8MDBZ9L/Malik_2020_A Hierarchy of Limitations in Machine Learning.pdf:application/pdf},
}

@article{halpern_causes_2005,
	title = {Causes and {Explanations}: {A} {Structural}-{Model} {Approach}, {Part} {I}: {Causes}},
	shorttitle = {Causes and {Explanations}},
	url = {http://arxiv.org/abs/cs/0011012},
	abstract = {We propose a new definition of actual cause, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account.},
	urldate = {2021-11-25},
	journal = {arXiv:cs/0011012},
	author = {Halpern, Joseph Y. and Pearl, Judea},
	month = nov,
	year = {2005},
	note = {arXiv: cs/0011012},
	keywords = {Computer Science - Artificial Intelligence, I.2.4},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/YBSIIHGN/0011012.html:text/html;Halpern_Pearl_2005_Causes and Explanations.pdf:/Users/neilnatarajan/Zotero/storage/STQSIK6S/Halpern_Pearl_2005_Causes and Explanations.pdf:application/pdf},
}

@article{halpern_causes_2005-1,
	title = {Causes and {Explanations}: {A} {Structural}-{Model} {Approach}. {Part} {II}: {Explanations}},
	shorttitle = {Causes and {Explanations}},
	url = {http://arxiv.org/abs/cs/0208034},
	abstract = {We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature.},
	urldate = {2021-11-25},
	journal = {arXiv:cs/0208034},
	author = {Halpern, Joseph Y. and Pearl, Judea},
	month = nov,
	year = {2005},
	note = {arXiv: cs/0208034},
	keywords = {Computer Science - Artificial Intelligence, I.2.4},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6GUZ9GVH/0208034.html:text/html;Halpern_Pearl_2005_Causes and Explanations.pdf:/Users/neilnatarajan/Zotero/storage/WJKCTBZL/Halpern_Pearl_2005_Causes and Explanations.pdf:application/pdf},
}

@article{tucci_introduction_2013,
	title = {Introduction to {Judea} {Pearl}'s {Do}-{Calculus}},
	url = {http://arxiv.org/abs/1305.5506},
	abstract = {This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl's do-calculus, including proofs of his 3 rules.},
	urldate = {2021-11-25},
	journal = {arXiv:1305.5506 [cs]},
	author = {Tucci, Robert R.},
	month = apr,
	year = {2013},
	note = {arXiv: 1305.5506},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/VPX3N5EA/1305.html:text/html;Tucci_2013_Introduction to Judea Pearl's Do-Calculus_annotated.pdf:/Users/neilnatarajan/Zotero/storage/22HM7TJ9/Tucci_2013_Introduction to Judea Pearl's Do-Calculus_annotated.pdf:application/pdf;Tucci_2013_Introduction to Judea Pearl's Do-Calculus.pdf:/Users/neilnatarajan/Zotero/storage/22HM7TJ9/Tucci_2013_Introduction to Judea Pearl's Do-Calculus.pdf:application/pdf},
}

@article{pearl_-calculus_2012,
	title = {The {Do}-{Calculus} {Revisited}},
	url = {http://arxiv.org/abs/1210.4852},
	abstract = {The do-calculus was developed in 1995 to facilitate the identification of causal effects in non-parametric models. The completeness proofs of [Huang and Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of [Tian and Shpitser, 2010] have laid this identification problem to rest. Recent explorations unveil the usefulness of the do-calculus in three additional areas: mediation analysis [Pearl, 2012], transportability [Pearl and Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the task of fusing empirical results from several diverse studies, conducted on heterogeneous populations and under different conditions, so as to synthesize an estimate of a causal relation in some target environment, potentially different from those under study. The talk surveys these results with emphasis on the challenges posed by meta-synthesis. For background material, see http://bayes.cs.ucla.edu/csl\_papers.html},
	urldate = {2021-11-25},
	journal = {arXiv:1210.4852 [cs, stat]},
	author = {Pearl, Judea},
	month = oct,
	year = {2012},
	note = {arXiv: 1210.4852},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/HVMV2WQD/1210.html:text/html;Pearl_2012_The Do-Calculus Revisited.pdf:/Users/neilnatarajan/Zotero/storage/Y4WUAN8H/Pearl_2012_The Do-Calculus Revisited.pdf:application/pdf},
}

@article{oswald_how_2021,
	title = {How do climate change skeptics engage with opposing views? {Understanding} mechanisms of social identity and cognitive dissonance in an online forum},
	shorttitle = {How do climate change skeptics engage with opposing views?},
	url = {http://arxiv.org/abs/2102.06516},
	abstract = {Does engagement with opposing views help break down ideological `echo chambers'; or does it backfire and reinforce them? This question remains critical as academics, policymakers and activists grapple with the question of how to regulate political discussion on social media. In this study, we contribute to the debate by examining the impact of opposing views within a major climate change skeptic online community on Reddit. A large sample of posts (N = 3000) was manually coded as either dissonant or consonant which allowed the automated classification of the full dataset of more than 50,000 posts, with codes inferred from linked websites. We find that ideologically dissonant submissions act as a stimulant to activity in the community: they received more attention (comments) than consonant submissions, even though they received lower scores through up-voting and down-voting. Users who engaged with dissonant submissions were also more likely to return to the forum. Consistent with identity theory, confrontation with opposing views triggered activity in the forum, particularly among users that are highly engaged with the community. In light of the findings, theory of social identity and echo chambers is discussed and enhanced.},
	urldate = {2021-12-08},
	journal = {arXiv:2102.06516 [cs]},
	author = {Oswald, Lisa and Bright, Jonathan},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.06516},
	keywords = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/5HAWKSMK/2102.html:text/html;Oswald_Bright_2021_How do climate change skeptics engage with opposing views.pdf:/Users/neilnatarajan/Zotero/storage/EZ7YB9BL/Oswald_Bright_2021_How do climate change skeptics engage with opposing views.pdf:application/pdf},
}

@article{lannelongue_green_2021,
	title = {Green {Algorithms}: {Quantifying} the {Carbon} {Footprint} of {Computation}},
	volume = {8},
	issn = {2198-3844},
	shorttitle = {Green {Algorithms}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/advs.202100707},
	doi = {10.1002/advs.202100707},
	abstract = {Climate change is profoundly affecting nearly all aspects of life on earth, including human societies, economies, and health. Various human activities are responsible for significant greenhouse gas (GHG) emissions, including data centers and other sources of large-scale computation. Although many important scientific milestones are achieved thanks to the development of high-performance computing, the resultant environmental impact is underappreciated. In this work, a methodological framework to estimate the carbon footprint of any computational task in a standardized and reliable way is presented and metrics to contextualize GHG emissions are defined. A freely available online tool, Green Algorithms (www.green-algorithms.org) is developed, which enables a user to estimate and report the carbon footprint of their computation. The tool easily integrates with computational processes as it requires minimal information and does not interfere with existing code, while also accounting for a broad range of hardware configurations. Finally, the GHG emissions of algorithms used for particle physics simulations, weather forecasts, and natural language processing are quantified. Taken together, this study develops a simple generalizable framework and freely available tool to quantify the carbon footprint of nearly any computation. Combined with recommendations to minimize unnecessary CO2 emissions, the authors hope to raise awareness and facilitate greener computation.},
	language = {en},
	number = {12},
	urldate = {2022-01-04},
	journal = {Advanced Science},
	author = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202100707},
	keywords = {climate change, computational research, green computing},
	pages = {2100707},
	file = {Lannelongue et al_2021_Green Algorithms.pdf:/Users/neilnatarajan/Zotero/storage/NALV4V6I/Lannelongue et al_2021_Green Algorithms.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/VRHJZI85/advs.html:text/html},
}

@article{ribeiro_nothing_2016,
	title = {Nothing {Else} {Matters}: {Model}-{Agnostic} {Explanations} {By} {Identifying} {Prediction} {Invariance}},
	shorttitle = {Nothing {Else} {Matters}},
	url = {http://arxiv.org/abs/1611.05817},
	abstract = {At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior. In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks.},
	urldate = {2022-01-05},
	journal = {arXiv:1611.05817 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05817},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/36RCH2M7/1611.html:text/html;Ribeiro et al_2016_Nothing Else Matters.pdf:/Users/neilnatarajan/Zotero/storage/NCWBRPW5/Ribeiro et al_2016_Nothing Else Matters.pdf:application/pdf},
}

@article{sokol_explainability_2020,
	title = {Explainability {Fact} {Sheets}: {A} {Framework} for {Systematic} {Assessment} of {Explainable} {Approaches}},
	shorttitle = {Explainability {Fact} {Sheets}},
	url = {http://arxiv.org/abs/1912.05100},
	doi = {10.1145/3351095.3372870},
	abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
	urldate = {2022-01-05},
	journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Sokol, Kacper and Flach, Peter},
	month = jan,
	year = {2020},
	note = {arXiv: 1912.05100},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {56--67},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/EF7VNKR6/1912.html:text/html;Sokol_Flach_2020_Explainability Fact Sheets.pdf:/Users/neilnatarajan/Zotero/storage/QEN8DP73/Sokol_Flach_2020_Explainability Fact Sheets.pdf:application/pdf},
}

@article{sundararajan_many_2020,
	title = {The many {Shapley} values for model explanation},
	url = {http://arxiv.org/abs/1908.08474},
	abstract = {The Shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing [16] showing that it is the {\textbackslash}emph\{unique\} method that satisfies certain good properties ({\textbackslash}emph\{axioms\}). There are, however, a multiplicity of ways in which the Shapley value is operationalized in the attribution problem. These differ in how they reference the model, the training data, and the explanation context. These give very different results, rendering the uniqueness result meaningless. Furthermore, we find that previously proposed approaches can produce counterintuitive attributions in theory and in practice---for instance, they can assign non-zero attributions to features that are not even referenced by the model. In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution, and propose a technique called Baseline Shapley (BShap) that is backed by a proper uniqueness result. We also contrast BShap with Integrated Gradients, another extension of Shapley value to the continuous setting.},
	urldate = {2022-01-05},
	journal = {arXiv:1908.08474 [cs, econ]},
	author = {Sundararajan, Mukund and Najmi, Amir},
	month = feb,
	year = {2020},
	note = {arXiv: 1908.08474},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Economics - Theoretical Economics},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/YLSBC8Y5/1908.html:text/html;Sundararajan_Najmi_2020_The many Shapley values for model explanation.pdf:/Users/neilnatarajan/Zotero/storage/552GV96A/Sundararajan_Najmi_2020_The many Shapley values for model explanation.pdf:application/pdf},
}

@article{barocas_hidden_2020,
	title = {The {Hidden} {Assumptions} {Behind} {Counterfactual} {Explanations} and {Principal} {Reasons}},
	url = {http://arxiv.org/abs/1912.04930},
	doi = {10.1145/3351095.3372830},
	abstract = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant--and withholding others. These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear. In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes. We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden. While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world--and the subjective choices necessary to compensate for this--must be understood before these techniques can be usefully implemented.},
	urldate = {2022-01-05},
	journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
	month = jan,
	year = {2020},
	note = {arXiv: 1912.04930},
	keywords = {Computer Science - Computers and Society},
	pages = {80--89},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/ID6DMXNL/1912.html:text/html;Barocas et al_2020_The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons.pdf:/Users/neilnatarajan/Zotero/storage/7UCITN3E/Barocas et al_2020_The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons.pdf:application/pdf},
}

@article{kumar_problems_2020,
	title = {Problems with {Shapley}-value-based explanations as feature importance measures},
	url = {http://arxiv.org/abs/2002.11097},
	abstract = {Game-theoretic formulations of feature importance have become popular as a way to "explain" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.},
	urldate = {2022-01-05},
	journal = {arXiv:2002.11097 [cs, stat]},
	author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.11097},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/HYXCE9H2/2002.html:text/html;Kumar et al_2020_Problems with Shapley-value-based explanations as feature importance measures.pdf:/Users/neilnatarajan/Zotero/storage/I4WGRVEL/Kumar et al_2020_Problems with Shapley-value-based explanations as feature importance measures.pdf:application/pdf},
}

@article{colquitt_justice_2001,
	title = {Justice at the {Millennium}: {A} {Meta}-{Analytic} {Review} of 25 {Years} of {Organizational} {Justice} {Research}},
	volume = {86},
	shorttitle = {Justice at the {Millennium}},
	doi = {10.1037//0021-9010.86.3.425},
	abstract = {The field of organizational justice continues to be marked by several important research questions, including the size of relationships among justice dimensions, the relative importance of different justice criteria, and the unique effects of justice dimensions on key outcomes. To address such questions, the authors conducted a meta-analytic review of 183 justice studies. The results suggest that although different justice dimensions are moderately to highly related, they contribute incremental variance explained in fairness perceptions. The results also illustrate the overall and unique relationships among distributive, procedural, interpersonal, and informational justice and several organizational outcomes (e.g., job satisfaction, organizational commitment, evaluation of authority, organizational citizenship behavior, withdrawal, performance). These findings are reviewed in terms of their implications for future research on organizational justice.},
	journal = {The Journal of applied psychology},
	author = {Colquitt, Jason and Conlon, Donald and Wesson, Michael and Porter, Christopher and Ng, K.},
	month = jul,
	year = {2001},
	pages = {425--45},
	file = {Colquitt et al_2001_Justice at the Millennium.pdf:/Users/neilnatarajan/Zotero/storage/KQKMQ8V3/Colquitt et al_2001_Justice at the Millennium.pdf:application/pdf},
}

@article{ustun_actionable_2019,
	title = {Actionable {Recourse} in {Linear} {Classification}},
	url = {http://arxiv.org/abs/1809.06514},
	doi = {10.1145/3287560.3287566},
	abstract = {Machine learning models are increasingly used to automate decisions that affect humans - deciding who should receive a loan, a job interview, or a social service. In such applications, a person should have the ability to change the decision of a model. When a person is denied a loan by a credit score, for example, they should be able to alter its input variables in a way that guarantees approval. Otherwise, they will be denied the loan as long as the model is deployed. More importantly, they will lack the ability to influence a decision that affects their livelihood. In this paper, we frame these issues in terms of recourse, which we define as the ability of a person to change the decision of a model by altering actionable input variables (e.g., income vs. age or marital status). We present integer programming tools to ensure recourse in linear classification problems without interfering in model development. We demonstrate how our tools can inform stakeholders through experiments on credit scoring problems. Our results show that recourse can be significantly affected by standard practices in model development, and motivate the need to evaluate recourse in practice.},
	urldate = {2022-01-05},
	journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
	month = jan,
	year = {2019},
	note = {arXiv: 1809.06514},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {10--19},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/Y6ISCE6J/1809.html:text/html;Ustun et al_2019_Actionable Recourse in Linear Classification.pdf:/Users/neilnatarajan/Zotero/storage/33INW2LS/Ustun et al_2019_Actionable Recourse in Linear Classification.pdf:application/pdf},
}

@article{friedrich_taxonomy_2011,
	title = {A {Taxonomy} for {Generating} {Explanations} in {Recommender} {Systems}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2365},
	doi = {10.1609/aimag.v32i3.2365},
	abstract = {In recommender systems, explanations serve as an additional type of information that can help users to better understand the system's output and promote objectives such as trust, confidence in decision making or utility. This article proposes a taxonomy to categorize and review the research in the area of explanations. It provides a unified view on the different recommendation paradigms, allowing similarities and differences to be clearly identified. Finally, the authors present their view on open research issues and opportunities for future work on this topic.},
	language = {en},
	number = {3},
	urldate = {2022-01-05},
	journal = {AI Magazine},
	author = {Friedrich, Gerhard and Zanker, Markus},
	month = jun,
	year = {2011},
	note = {Number: 3},
	pages = {90--98},
	file = {Friedrich_Zanker_2011_A Taxonomy for Generating Explanations in Recommender Systems.pdf:/Users/neilnatarajan/Zotero/storage/KNRSFJBW/Friedrich_Zanker_2011_A Taxonomy for Generating Explanations in Recommender Systems.pdf:application/pdf},
}

@article{vilone_explainable_2020,
	title = {Explainable {Artificial} {Intelligence}: a {Systematic} {Review}},
	shorttitle = {Explainable {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2006.00093},
	abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
	urldate = {2022-01-05},
	journal = {arXiv:2006.00093 [cs]},
	author = {Vilone, Giulia and Longo, Luca},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.00093},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.0, I.2.6, I.2.m},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GLMKTK42/2006.html:text/html;Vilone_Longo_2020_Explainable Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/KXUTCJLA/Vilone_Longo_2020_Explainable Artificial Intelligence.pdf:application/pdf},
}

@incollection{rader_explanations_2018,
	address = {New York, NY, USA},
	title = {Explanations as {Mechanisms} for {Supporting} {Algorithmic} {Transparency}},
	isbn = {978-1-4503-5620-6},
	url = {https://doi.org/10.1145/3173574.3173677},
	abstract = {Transparency can empower users to make informed choices about how they use an algorithmic decision-making system and judge its potential consequences. However, transparency is often conceptualized by the outcomes it is intended to bring about, not the specifics of mechanisms to achieve those outcomes. We conducted an online experiment focusing on how different ways of explaining Facebook's News Feed algorithm might affect participants' beliefs and judgments about the News Feed. We found that  . The explanations were less effective for helping participants evaluate the correctness of the system's output, and form opinions about how sensible and consistent its behavior is. We present implications for the design of transparency mechanisms in algorithmic decision-making systems based on these results.},
	urldate = {2022-01-05},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rader, Emilee and Cotter, Kelley and Cho, Janghee},
	month = apr,
	year = {2018},
	keywords = {algorithmic decision-making, explanations, transparency, \_tablet, User Study},
	pages = {1--13},
	file = {Rader et al_2018_Explanations as Mechanisms for Supporting Algorithmic Transparency.pdf:/Users/neilnatarajan/Zotero/storage/CY7PY3LM/Rader et al_2018_Explanations as Mechanisms for Supporting Algorithmic Transparency.pdf:application/pdf},
}

@article{binns_its_2018,
	title = {'{It}'s {Reducing} a {Human} {Being} to a {Percentage}'; {Perceptions} of {Justice} in {Algorithmic} {Decisions}},
	url = {http://arxiv.org/abs/1801.10408},
	doi = {10.1145/3173574.3173951},
	abstract = {Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions.},
	urldate = {2022-01-06},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	author = {Binns, Reuben and Van Kleek, Max and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
	month = apr,
	year = {2018},
	note = {arXiv: 1801.10408},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, H.5.m, K.4.1, \_tablet, User Study},
	pages = {1--14},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/UALDRMYZ/1801.html:text/html;Binns et al_2018_'It's Reducing a Human Being to a Percentage'\; Perceptions of Justice in.pdf:/Users/neilnatarajan/Zotero/storage/VIS3KNGK/Binns et al_2018_'It's Reducing a Human Being to a Percentage'\; Perceptions of Justice in.pdf:application/pdf},
}

@article{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2022-01-06},
	journal = {arXiv:1705.07874 [cs, stat]},
	author = {Lundberg, Scott and Lee, Su-In},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.07874},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/LS6VRCJC/1705.html:text/html;Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:/Users/neilnatarajan/Zotero/storage/AIQWBTDJ/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:application/pdf},
}

@inproceedings{ribeiro_anchors_2018,
	title = {Anchors: {High} {Precision} {Model}-{Agnostic} {Explanations}},
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufﬁcient” conditions for predictions. We propose an algorithm to efﬁciently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the ﬂexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	language = {en},
	booktitle = {{AAAI}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2018},
	pages = {9},
	file = {Ribeiro et al. - Anchors High Precision Model-Agnostic Explanation.pdf:/Users/neilnatarajan/Zotero/storage/B6X96F9E/Ribeiro et al. - Anchors High Precision Model-Agnostic Explanation.pdf:application/pdf},
}

@article{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	urldate = {2022-01-17},
	journal = {arXiv:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.08608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/RHR7VDNR/1702.html:text/html;Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/BCP6WVRJ/Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:application/pdf},
}

@inproceedings{ribeiro_why_2016,
	address = {San Francisco California USA},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2022-01-21},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	keywords = {\_tablet},
	pages = {1135--1144},
	file = {Ribeiro et al_2016_Why Should I Trust You.pdf:/Users/neilnatarajan/Zotero/storage/X6X63T57/Ribeiro et al_2016_Why Should I Trust You.pdf:application/pdf},
}

@article{pieters_explanation_2011,
	title = {Explanation and trust: what to tell the user in security and {AI}?},
	volume = {13},
	issn = {1572-8439},
	shorttitle = {Explanation and trust},
	url = {https://doi.org/10.1007/s10676-010-9253-3},
	doi = {10.1007/s10676-010-9253-3},
	abstract = {There is a common problem in artificial intelligence (AI) and information security. In AI, an expert system needs to be able to justify and explain a decision to the user. In information security, experts need to be able to explain to the public why a system is secure. In both cases, an important goal of explanation is to acquire or maintain the users’ trust. In this paper, I investigate the relation between explanation and trust in the context of computing science. This analysis draws on literature study and concept analysis, using elements from system theory as well as actor-network theory. I apply the conceptual framework to both AI and information security, and show the benefit of the framework for both fields by means of examples. The main focus is on expert systems (AI) and electronic voting systems (security). Finally, I discuss consequences of the analysis for ethics in terms of (un)informed consent and dissent, and the associated division of responsibilities.},
	language = {en},
	number = {1},
	urldate = {2022-01-21},
	journal = {Ethics and Information Technology},
	author = {Pieters, Wolter},
	month = mar,
	year = {2011},
	pages = {53--64},
	file = {Springer Full Text PDF:/Users/neilnatarajan/Zotero/storage/YIIRHZIQ/Pieters - 2011 - Explanation and trust what to tell the user in se.pdf:application/pdf},
}

@misc{zerilli_explaining_2020,
	type = {Preprint},
	title = {Explaining machine learning decisions},
	url = {http://philsci-archive.pitt.edu/19096/},
	abstract = {The operations of deep networks are widely acknowledged to be inscrutable. The growing field of “Explainable AI” (XAI) has emerged in direct response to this problem. However, owing to the nature of the opacity in question, XAI has been forced to prioritise interpretability at the expense of completeness, and even realism, so that its explanations are frequently interpretable without being underpinned by more comprehensive explanations faithful to the way a network computes its predictions. While this has been taken to be a shortcoming of the field of XAI, I argue that it is broadly the right approach to the problem.},
	language = {en},
	urldate = {2022-01-26},
	author = {Zerilli, John},
	year = {2020},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/PFXRZAPS/Zerilli - 2020 - Explaining machine learning decisions.html:text/html;Zerilli_2020_Explaining machine learning decisions.pdf:/Users/neilnatarajan/Zotero/storage/W9RBSPVU/Zerilli_2020_Explaining machine learning decisions.pdf:application/pdf;Zerilli_2020_Explaining machine learning decisions.pdf:/Users/neilnatarajan/Zotero/storage/W9RBSPVU/false:application/pdf},
}

@article{chou_counterfactuals_2022,
	title = {Counterfactuals and causability in explainable artificial intelligence: {Theory}, algorithms, and applications},
	volume = {81},
	issn = {1566-2535},
	shorttitle = {Counterfactuals and causability in explainable artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521002281},
	doi = {10.1016/j.inffus.2021.11.003},
	abstract = {Deep learning models have achieved high performance across different domains, such as medical decision-making, autonomous vehicles, decision support systems, among many others. However, despite this success, the inner mechanisms of these models are opaque because their internal representations are too complex for a human to understand. This opacity makes it hard to understand the how or the why of the predictions of deep learning models. There has been a growing interest in model-agnostic methods that make deep learning models more transparent and explainable to humans. Some researchers recently argued that for a machine to achieve human-level explainability, this machine needs to provide human causally understandable explanations, also known as causability. A specific class of algorithms that have the potential to provide causability are counterfactuals. This paper presents an in-depth systematic review of the diverse existing literature on counterfactuals and causability for explainable artificial intelligence (AI). We performed a Latent Dirichlet topic modelling analysis (LDA) under a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework to find the most relevant literature articles. This analysis yielded a novel taxonomy that considers the grounding theories of the surveyed algorithms, together with their underlying properties and applications to real-world data. Our research suggests that current model-agnostic counterfactual algorithms for explainable AI are not grounded on a causal theoretical formalism and, consequently, cannot promote causability to a human decision-maker. Furthermore, our findings suggest that the explanations derived from popular algorithms in the literature provide spurious correlations rather than cause/effects relationships, leading to sub-optimal, erroneous, or even biased explanations. Thus, this paper also advances the literature with new directions and challenges on promoting causability in model-agnostic approaches for explainable AI.},
	language = {en},
	urldate = {2022-01-26},
	journal = {Information Fusion},
	author = {Chou, Yu-Liang and Moreira, Catarina and Bruza, Peter and Ouyang, Chun and Jorge, Joaquim},
	month = may,
	year = {2022},
	keywords = {\_tablet, Causability, Causality, Counterfactuals, Deep learning, Explainable AI},
	pages = {59--83},
	file = {Chou et al_2022_Counterfactuals and causability in explainable artificial intelligence.pdf:/Users/neilnatarajan/Zotero/storage/B7Y459EC/Chou et al_2022_Counterfactuals and causability in explainable artificial intelligence.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/Z5ASSTWX/S1566253521002281.html:text/html},
}

@inproceedings{mohseni_trust_nodate,
	title = {Trust {Evolution} {Over} {Time} in {Explainable} {AI} for {Fake} {News} {Detection}},
	abstract = {The need for interpretable and accountable intelligent systems is strong as artiﬁcial intelligence (AI) becomes more prevalent in human life. We study the effects of interpretability on user’s trust in an AI assistant tool designed for fake news detection. In our study, we expose participants to different types of AI and Explainable AI (XAI) assistants, measure their perceived accuracy of algorithm, and cluster user trust changes over time into ﬁve types of trust evolution. We present quantitative results and analysis from human-subject studies and discuss our ﬁndings regarding how model explanations affect on user trust evolution over time.},
	language = {en},
	author = {Mohseni, Sina and Yang, Fan and Pentyala, Shiva and Du, Mengnan and Liu, Yi and Lupfer, Nic and Hu, Xia and Ji, Shuiwang and Ragan, Eric D},
	pages = {4},
	file = {Mohseni et al. - Trust Evolution Over Time in Explainable AI for Fa.pdf:/Users/neilnatarajan/Zotero/storage/SIN9YLNL/Mohseni et al. - Trust Evolution Over Time in Explainable AI for Fa.pdf:application/pdf},
}

@inproceedings{yang_how_2020,
	address = {New York, NY, USA},
	series = {{IUI} '20},
	title = {How do visual explanations foster end users' appropriate trust in machine learning?},
	isbn = {978-1-4503-7118-6},
	url = {https://doi.org/10.1145/3377325.3377480},
	doi = {10.1145/3377325.3377480},
	abstract = {We investigated the effects of example-based explanations for a machine learning classifier on end users' appropriate trust. We explored the effects of spatial layout and visual representation in an in-person user study with 33 participants. We measured participants' appropriate trust in the classifier, quantified the effects of different spatial layouts and visual representations, and observed changes in users' trust over time. The results show that each explanation improved users' trust in the classifier, and the combination of explanation, human, and classification algorithm yielded much better decisions than the human and classification algorithm separately. Yet these visual explanations lead to different levels of trust and may cause inappropriate trust if an explanation is difficult to understand. Visual representation and performance feedback strongly affect users' trust, and spatial layout shows a moderate effect. Our results do not support that individual differences (e.g., propensity to trust) affect users' trust in the classifier. This work advances the state-of-the-art in trust-able machine learning and informs the design and appropriate use of automated systems.},
	urldate = {2022-01-26},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Fumeng and Huang, Zhuanyi and Scholtz, Jean and Arendt, Dustin L.},
	month = mar,
	year = {2020},
	keywords = {trust, explainable artificial intelligence, classification, human-machine collaboration, information visualization, supervised-learning, trust calibration, User Study},
	pages = {189--201},
	file = {Yang et al_2020_How do visual explanations foster end users' appropriate trust in machine.pdf:/Users/neilnatarajan/Zotero/storage/WSQ8FI2J/Yang et al_2020_How do visual explanations foster end users' appropriate trust in machine.pdf:application/pdf;Yang et al_2020_How do visual explanations foster end users' appropriate trust in machine.pdf:/Users/neilnatarajan/Zotero/storage/WSQ8FI2J/false:application/pdf},
}

@article{jacobs_how_2021,
	title = {How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2158-3188},
	shorttitle = {How machine-learning recommendations influence clinician treatment selections},
	url = {https://www.nature.com/articles/s41398-021-01224-x},
	doi = {10.1038/s41398-021-01224-x},
	abstract = {Decision support systems embodying machine learning models offer the promise of an improved standard of care for major depressive disorder, but little is known about how clinicians’ treatment decisions will be influenced by machine learning recommendations and explanations. We used a within-subject factorial experiment to present 220 clinicians with patient vignettes, each with or without a machine-learning (ML) recommendation and one of the multiple forms of explanation. We found that interacting with ML recommendations did not significantly improve clinicians’ treatment selection accuracy, assessed as concordance with expert psychopharmacologist consensus, compared to baseline scenarios in which clinicians made treatment decisions independently. Interacting with incorrect recommendations paired with explanations that included limited but easily interpretable information did lead to a significant reduction in treatment selection accuracy compared to baseline questions. These results suggest that incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. More generally, our findings challenge the common assumption that clinicians interacting with ML tools will perform better than either clinicians or ML algorithms individually.},
	language = {en},
	number = {1},
	urldate = {2022-01-26},
	journal = {Translational Psychiatry},
	author = {Jacobs, Maia and Pradier, Melanie F. and McCoy, Thomas H. and Perlis, Roy H. and Doshi-Velez, Finale and Gajos, Krzysztof Z.},
	month = feb,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Depression;Scientific community
Subject\_term\_id: depression;scientific-community},
	keywords = {Depression, Scientific community, \_tablet, User Study},
	pages = {1--9},
	file = {Jacobs et al_2021_How machine-learning recommendations influence clinician treatment selections.pdf:/Users/neilnatarajan/Zotero/storage/6EBQT5RA/Jacobs et al_2021_How machine-learning recommendations influence clinician treatment selections.pdf:application/pdf;Jacobs et al. - 2021 - How machine-learning recommendations influence cli.pdf:/Users/neilnatarajan/Zotero/storage/MVJRDDP8/Jacobs et al. - 2021 - How machine-learning recommendations influence cli.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/T2M6TSIZ/s41398-021-01224-x.html:text/html},
}

@article{markus_role_2021,
	title = {The role of explainability in creating trustworthy artificial intelligence for health care: {A} comprehensive survey of the terminology, design choices, and evaluation strategies},
	volume = {113},
	issn = {1532-0464},
	shorttitle = {The role of explainability in creating trustworthy artificial intelligence for health care},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420302835},
	doi = {10.1016/j.jbi.2020.103655},
	abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
	language = {en},
	urldate = {2022-01-26},
	journal = {Journal of Biomedical Informatics},
	author = {Markus, Aniek F. and Kors, Jan A. and Rijnbeek, Peter R.},
	month = jan,
	year = {2021},
	keywords = {\_tablet, Explainable artificial intelligence, Explainable modelling, Interpretability, Post-hoc explanation, Trustworthy artificial intelligence},
	pages = {103655},
	file = {Markus et al_2021_The role of explainability in creating trustworthy artificial intelligence for.pdf:/Users/neilnatarajan/Zotero/storage/89XMUAIS/Markus et al_2021_The role of explainability in creating trustworthy artificial intelligence for.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/B2DJP4BI/S1532046420302835.html:text/html},
}

@article{dzindolet_role_2003,
	series = {Trust and {Technology}},
	title = {The role of trust in automation reliance},
	volume = {58},
	issn = {1071-5819},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581903000387},
	doi = {10.1016/S1071-5819(03)00038-7},
	abstract = {A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.},
	language = {en},
	number = {6},
	urldate = {2022-01-26},
	journal = {International Journal of Human-Computer Studies},
	author = {Dzindolet, Mary T. and Peterson, Scott A. and Pomranky, Regina A. and Pierce, Linda G. and Beck, Hall P.},
	month = jun,
	year = {2003},
	keywords = {\_tablet, Automation reliance, Automation trust, Disuse, Misuse, User Study},
	pages = {697--718},
	file = {Dzindolet et al_2003_The role of trust in automation reliance.pdf:/Users/neilnatarajan/Zotero/storage/BY6ELP6C/Dzindolet et al_2003_The role of trust in automation reliance.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/VYE77HHX/S1071581903000387.html:text/html},
}

@article{ford_play_2020,
	title = {Play {MNIST} {For} {Me}! {User} {Studies} on the {Effects} of {Post}-{Hoc}, {Example}-{Based} {Explanations} \& {Error} {Rates} on {Debugging} a {Deep} {Learning}, {Black}-{Box} {Classifier}},
	abstract = {Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct and as error rates increase above 4\%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. This paper reports two experiments (N=349) on the impact of post hoc explanations by example and error rates on peoples perceptions of a black box classifier. Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct. They also show that as error rates increase above 4\%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. The implications of these results for XAI are discussed.},
	journal = {ArXiv},
	author = {Ford, Courtney and Kenny, Eoin M. and Keane, Mark T.},
	year = {2020},
	keywords = {User Study},
	file = {Ford et al_2020_Play MNIST For Me.pdf:/Users/neilnatarajan/Zotero/storage/VS3NLDMT/Ford et al_2020_Play MNIST For Me.pdf:application/pdf;Ford et al_2020_Play MNIST For Me.pdf:/Users/neilnatarajan/Zotero/storage/VS3NLDMT/false:application/pdf},
}

@inproceedings{bansal_does_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Does the {Whole} {Exceed} its {Parts}? {The} {Effect} of {AI} {Explanations} on {Complementary} {Team} {Performance}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Does the {Whole} {Exceed} its {Parts}?},
	url = {https://doi.org/10.1145/3411764.3445717},
	doi = {10.1145/3411764.3445717},
	abstract = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
	urldate = {2022-01-26},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
	month = may,
	year = {2021},
	keywords = {\_tablet, Explainable AI, Augmented intelligence, Human-AI teams, User Study},
	pages = {1--16},
	file = {Bansal et al_2021_Does the Whole Exceed its Parts.pdf:/Users/neilnatarajan/Zotero/storage/L9AYG6A7/Bansal et al_2021_Does the Whole Exceed its Parts.pdf:application/pdf},
}

@inproceedings{lai_human_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {On {Human} {Predictions} with {Explanations} and {Predictions} of {Machine} {Learning} {Models}: {A} {Case} {Study} on {Deception} {Detection}},
	isbn = {978-1-4503-6125-5},
	shorttitle = {On {Human} {Predictions} with {Explanations} and {Predictions} of {Machine} {Learning} {Models}},
	url = {https://doi.org/10.1145/3287560.3287590},
	doi = {10.1145/3287560.3287590},
	abstract = {Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels ({\textgreater}20\% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.},
	urldate = {2022-01-26},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Lai, Vivian and Tan, Chenhao},
	month = jan,
	year = {2019},
	keywords = {explanations, \_tablet, human agency, human performance, predictions},
	pages = {29--38},
	file = {Lai_Tan_2019_On Human Predictions with Explanations and Predictions of Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/9UAJYVZJ/Lai_Tan_2019_On Human Predictions with Explanations and Predictions of Machine Learning.pdf:application/pdf},
}

@inproceedings{dwork_fairness_2012,
	address = {New York, NY, USA},
	series = {{ITCS} '12},
	title = {Fairness through awareness},
	isbn = {978-1-4503-1115-1},
	url = {https://doi.org/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	urldate = {2022-01-27},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	month = jan,
	year = {2012},
	pages = {214--226},
	file = {Submitted Version:/Users/neilnatarajan/Zotero/storage/ZCLN5LCA/Dwork et al. - 2012 - Fairness through awareness.pdf:application/pdf},
}

@article{fleisher_whats_nodate,
	title = {What's {Fair} about {Individual} {Fairness}?},
	abstract = {One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle I call “similar treatment,” which requires that similar individuals be treated similarly. IF offers a precise account of this deﬁnition using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct deﬁnition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a deﬁnition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a deﬁnition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufﬁcient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to deﬁne fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a deﬁnition of fairness, and instead should be seen as one tool among many for ameliorating algorithmic bias.},
	language = {en},
	author = {Fleisher, Will},
	pages = {12},
	file = {Fleisher - What's Fair about Individual Fairness.pdf:/Users/neilnatarajan/Zotero/storage/FVPG2475/Fleisher - What's Fair about Individual Fairness.pdf:application/pdf},
}

@article{krishna_disagreement_2022,
	title = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}: {A} {Practitioner}'s {Perspective}},
	shorttitle = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}},
	url = {https://arxiv.org/abs/2202.01602v2},
	abstract = {As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.},
	language = {en},
	urldate = {2022-02-08},
	author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
	month = feb,
	year = {2022},
	file = {Krishna et al_2022_The Disagreement Problem in Explainable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/UUVTJGUF/Krishna et al_2022_The Disagreement Problem in Explainable Machine Learning.pdf:application/pdf;Krishna et al_2022_The Disagreement Problem in Explainable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/UUVTJGUF/false:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/WP7DRH5K/2202.html:text/html},
}

@article{zerilli_how_nodate,
	title = {How {Transparency} {Modulates} {Trust} in {Artificial} {Intelligence}},
	language = {en},
	author = {Zerilli, John and Bhatt, Umang and Weller, Adrian},
	pages = {28},
	file = {Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence_annotated.pdf:/Users/neilnatarajan/Zotero/storage/DVWBI8HB/Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence_annotated.pdf:application/pdf;Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence.pdf:/Users/neilnatarajan/Zotero/storage/DVWBI8HB/Zerilli et al_How Transparency Modulates Trust in Artificial Intelligence.pdf:application/pdf},
}

@article{zerilli_explaining_2022,
	title = {Explaining {Machine} {Learning} {Decisions}},
	volume = {89},
	issn = {0031-8248, 1539-767X},
	url = {https://www.cambridge.org/core/product/identifier/S0031824821000131/type/journal_article},
	doi = {10.1017/psa.2021.13},
	abstract = {The operations of deep networks are widely acknowledged to be inscrutable. The growing field of “Explainable AI” (XAI) has emerged in direct response to this problem. However, owing to the nature of the opacity in question, XAI has been forced to prioritise interpretability at the expense of completeness, and even realism, so that its explanations are frequently interpretable without being underpinned by more comprehensive explanations faithful to the way a network computes its predictions. While this has been taken to be a shortcoming of the field of XAI, I argue that it is broadly the right approach to the problem.},
	language = {en},
	number = {1},
	urldate = {2022-02-09},
	journal = {Philosophy of Science},
	author = {Zerilli, John},
	month = jan,
	year = {2022},
	pages = {1--19},
	file = {Zerilli_2022_Explaining Machine Learning Decisions_annotated.pdf:/Users/neilnatarajan/Zotero/storage/NQ5H7HA4/Zerilli_2022_Explaining Machine Learning Decisions_annotated.pdf:application/pdf;Zerilli_2022_Explaining Machine Learning Decisions.pdf:/Users/neilnatarajan/Zotero/storage/NQ5H7HA4/Zerilli_2022_Explaining Machine Learning Decisions.pdf:application/pdf},
}

@article{ustun_learning_nodate,
	title = {Learning {Optimized} {Risk} {Scores}},
	abstract = {Risk scores are simple classiﬁcation models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are diﬃcult to learn from data because they need to be calibrated, sparse, use small integer coeﬃcients, and obey application-speciﬁc constraints. In this paper, we introduce a machine learning method to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a cutting plane algorithm to recover its optimal solution. We improve our algorithm with specialized techniques that generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our algorithm can train risk scores in a way that scales linearly in the number of samples in a dataset, and that allows practitioners to address application-speciﬁc constraints without parameter tuning or post-processing. We benchmark the performance of diﬀerent methods to learn risk scores on publicly available datasets, comparing risk scores produced by our method to risk scores built using methods that are used in practice. We also discuss the practical beneﬁts of our method through a real-world application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.},
	language = {en},
	author = {Ustun, Berk and Rudin, Cynthia},
	pages = {75},
	file = {Ustun_Rudin_Learning Optimized Risk Scores.pdf:/Users/neilnatarajan/Zotero/storage/5BAWEDEC/Ustun_Rudin_Learning Optimized Risk Scores.pdf:application/pdf},
}

@inproceedings{wang_deontological_2020,
	title = {Deontological {Ethics} {By} {Monotonicity} {Shape} {Constraints}},
	url = {https://proceedings.mlr.press/v108/wang20e.html},
	abstract = {We demonstrate how easy it is for modern machine-learned systems to violate common deontological ethical principles and social norms such as “favor the less fortunate,” and “do not penalize good attributes.” We propose that in some cases such ethical principles can be incorporated into a machine-learned model by adding shape constraints that constrain the model to respond only positively to relevant inputs. We analyze the relationship between these deontological constraints that act on individuals and the consequentialist group-based fairness goals of one-sided statistical parity and equal opportunity.  This strategy works with sensitive attributes that are Boolean or real-valued such as income and age, and can help produce more responsible and trustworthy AI.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wang, Serena and Gupta, Maya},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2043--2054},
	file = {Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints_annotated.pdf:/Users/neilnatarajan/Zotero/storage/VBSVHX6B/Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints_annotated.pdf:application/pdf;Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:/Users/neilnatarajan/Zotero/storage/VBSVHX6B/Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:application/pdf;Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:/Users/neilnatarajan/Zotero/storage/4GQ5G589/Wang_Gupta_2020_Deontological Ethics By Monotonicity Shape Constraints.pdf:application/pdf},
}

@article{kruger_problem_2022,
	title = {The problem with trust: on the discursive commodification of trust in {AI}},
	issn = {1435-5655},
	shorttitle = {The problem with trust},
	url = {https://doi.org/10.1007/s00146-022-01401-6},
	doi = {10.1007/s00146-022-01401-6},
	abstract = {This commentary draws critical attention to the ongoing commodification of trust in policy and scholarly discourses of artificial intelligence (AI) and society. Based on an assessment of publications discussing the implementation of AI in governmental and private services, our findings indicate that this discursive trend towards commodification is driven by the need for a trusting population of service users to harvest data at scale and leads to the discursive construction of trust as an essential good on a par with data as raw material. This discursive commodification is marked by a decreasing emphasis on trust understood as the expected reliability of a trusted agent, and increased emphasis on instrumental and extractive framings of trust as a resource. This tendency, we argue, does an ultimate disservice to developers, users, and systems alike, insofar as it obscures the subtle mechanisms through which trust in AI systems might be built, making it less likely that it will be.},
	language = {en},
	urldate = {2022-03-01},
	journal = {AI \& SOCIETY},
	author = {Krüger, Steffen and Wilson, Christopher},
	month = feb,
	year = {2022},
	file = {Springer Full Text PDF:/Users/neilnatarajan/Zotero/storage/MT7KIMAP/Krüger and Wilson - 2022 - The problem with trust on the discursive commodif.pdf:application/pdf},
}

@article{rambachan_economic_2020,
	title = {An {Economic} {Perspective} on {Algorithmic} {Fairness}},
	volume = {110},
	issn = {2574-0768, 2574-0776},
	url = {https://pubs.aeaweb.org/doi/10.1257/pandp.20201036},
	doi = {10.1257/pandp.20201036},
	abstract = {There are widespread concerns that the growing use of machine learning algorithms in important decisions may reproduce and reinforce existing discrimination against legally protected groups. Most of the attention to date on issues of “algorithmic bias” or “algorithmic fairness” has come from computer scientists and machine learning researchers. We argue that concerns about algorithmic fairness are at least as much about questions of how discrimination manifests itself in data, decision-making under uncertainty, and optimal regulation. To fully answer these questions, an economic framework is necessary--and as a result, economists have much to contribute.},
	language = {en},
	urldate = {2022-03-04},
	journal = {AEA Papers and Proceedings},
	author = {Rambachan, Ashesh and Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil},
	month = may,
	year = {2020},
	keywords = {\_tablet},
	pages = {91--95},
	file = {Rambachan et al_2020_An Economic Perspective on Algorithmic Fairness.pdf:/Users/neilnatarajan/Zotero/storage/N7ZEFMZX/Rambachan et al_2020_An Economic Perspective on Algorithmic Fairness.pdf:application/pdf},
}

@incollection{van_de_poel_translating_2013,
	address = {Dordrecht},
	series = {Philosophy of {Engineering} and {Technology}},
	title = {Translating {Values} into {Design} {Requirements}},
	isbn = {978-94-007-7762-0},
	url = {https://doi.org/10.1007/978-94-007-7762-0_20},
	abstract = {A crucial step in Value Sensitive Design (VSD) is the translation of values into design requirements. However, few research has been done on how this translation can be made. In this contribution, I first consider an example of this translation. I then introduce the notion of values hierarchy, a hierarchy structure of values, norms and design requirements. I discuss the relation of specification, by which values can be translated into design requirements, and the for the sake of relation which connects design requirements to underlying norms and values. I discuss conditions under which a certain specification of values into design requirements is adequate or at least tenable.},
	language = {en},
	urldate = {2022-03-11},
	booktitle = {Philosophy and {Engineering}: {Reflections} on {Practice}, {Principles} and {Process}},
	publisher = {Springer Netherlands},
	author = {van de Poel, Ibo},
	editor = {Michelfelder, Diane P and McCarthy, Natasha and Goldberg, David E.},
	year = {2013},
	doi = {10.1007/978-94-007-7762-0_20},
	keywords = {Design, Requirements, Specification, Value, Value sensitive design},
	pages = {253--266},
}

@misc{noauthor_ai_nodate,
	title = {{AI} {Alignment}, {Philosophical} {Pluralism}, and the {Relevance} of {Non}-{Western} {Philosophy} - {AI} {Alignment} {Forum}},
	url = {https://www.alignmentforum.org/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of},
	abstract = {This is an extended transcript of the talk I gave at EAGxAsiaPacific 2020. In the talk, I present a somewhat critical take on how AI alignment has grown as a field, and how, from my perspective, it d…},
	urldate = {2022-04-09},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/CQSQKGBB/ai-alignment-philosophical-pluralism-and-the-relevance-of.html:text/html},
}

@article{amodei_concrete_2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	urldate = {2022-04-09},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/I5LAWUP5/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GR4MDXXQ/1606.html:text/html},
}

@book{nkwake_credibility_2015,
	title = {Credibility, {Validity}, and {Assumptions} in {Program} {Evaluation} {Methodology}},
	isbn = {978-3-319-19020-4},
	abstract = {This book focuses on assumptions underlyingmethods choice in program evaluation. Credible program evaluation extends beyond the accuracy of research designs toinclude arguments justifying the appropriateness of methods. An important part of this justification is explaining the assumptions made about the validity of methods. This book provides a framework for understanding methodological assumptions, identifying the decisions made at each stage of the evaluation process, the major forms of validity affected by those decisions, and the preconditions for andassumptions about those validities.Though the selection of appropriate research methodology is not a new topic within social development research, previouspublications suggest only advantages and disadvantagesof using various methods and when to use them. Thisbook goes beyond other publications to analyze the assumptions underlying actual methodological choicesinevaluation studies and how these eventually influence evaluation quality.The analysis offered issupported by a collation of assumptions collected from acase study of 34 evaluations. Due to its in-depth analysis, strong theoretical basis, andpractice examples, Credibility, Validity and Assumptions is amust-have resource for researchers, students, university professors and practitioners in program evaluation.Importantly, it provides tools for the application ofappropriate research methods in program evaluation},
	author = {Nkwake, Apollo and Mayne, John},
	month = jan,
	year = {2015},
	doi = {10.1007/978-3-319-19021-1},
	note = {Journal Abbreviation: Credibility, Validity, and Assumptions in Program Evaluation Methodology
Pages: 166
Publication Title: Credibility, Validity, and Assumptions in Program Evaluation Methodology},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/AISL65I8/Nkwake and Mayne - 2015 - Credibility, Validity, and Assumptions in Program .pdf:application/pdf},
}

@incollection{dodge_new_2002,
	address = {Basel},
	title = {New {Selection} {Indices} for {University} {Admissions}: {A} {Quantile} {Approach}},
	isbn = {978-3-0348-9472-2 978-3-0348-8201-9},
	shorttitle = {New {Selection} {Indices} for {University} {Admissions}},
	url = {http://link.springer.com/10.1007/978-3-0348-8201-9_6},
	abstract = {All universities seek to admit a freshmen cohort of a speciﬁed size who will be successful in graduating from college. Previous work has shown a high correlation between students’ eventual success and their ability to do well in their ﬁrst term. This objective translates into admitting students whose ﬁrst term grade point average (GPA) exceeds a minimum acceptable level. Current admissions decisions are based however on a selection index that is constructed from a GPA regression that estimates expected GPA. We consider a new approach in which the selection index is based directly on the quantiles of the GPA distribution. The new approach realistically assumes that student characteristics have diﬀerential impacts at diﬀerent parts of the GPA distribution. Since impacts usually vary for the low, middle and upper parts of the GPA distribution, the quantile approach provides additional information. The quantile method also provides admissions oﬃcers the ﬂexibility to target diﬀerent GPA properties for the freshman class. They can directly implement a criterion that selects students with maximal probability of a ﬁrst term GPA that is better than any speciﬁed value. We illustrate the quantile method by application to actual admission practices at the University of Illinois at Chicago.},
	language = {en},
	urldate = {2022-04-12},
	booktitle = {Statistical {Data} {Analysis} {Based} on the {L1}-{Norm} and {Related} {Methods}},
	publisher = {Birkhäuser Basel},
	author = {Tam, Mo-Yin S. and Bassett, Gilbert W. and Sukhatme, Uday},
	editor = {Dodge, Yadolah},
	year = {2002},
	doi = {10.1007/978-3-0348-8201-9_6},
	keywords = {\_tablet},
	pages = {67--76},
	file = {Tam et al_2002_New Selection Indices for University Admissions.pdf:/Users/neilnatarajan/Zotero/storage/TM77DPPG/Tam et al_2002_New Selection Indices for University Admissions.pdf:application/pdf},
}

@article{chadi_selecting_2018,
	title = {Selecting successful students? {Undergraduate} grades as an admission criterion},
	volume = {50},
	issn = {0003-6846, 1466-4283},
	shorttitle = {Selecting successful students?},
	url = {https://www.tandfonline.com/doi/full/10.1080/00036846.2017.1418072},
	doi = {10.1080/00036846.2017.1418072},
	abstract = {In Europe's reformed education system, universities may be forced by law to consider undergraduate grade point average (UGPAJ as the primary admission criterion in the selection of graduate students. In this article, we investigate whether UGPA predicts graduate student performance in order to discuss its usefulness as an admission criterion. In our theoretical framework, we show that undergraduate students may choose slower study progress in favour of receiving higher grades and conclude that UGPA is a relatively good (weak) predictor for graduate grade point average (study progress). Having data from a cohort of students whose selection was in clear conflict with the legal requirement, we empirically confirm our theoretical predictions by exploiting a unique opportunity for assessing educationa l policies. Discussion of our findings leads to some important conc lusions concerning the Bologna reforms and the lawmakers' idea of giving some independence to universities, but not too much of it.},
	language = {en},
	number = {28},
	urldate = {2022-04-12},
	journal = {Applied Economics},
	author = {Chadi, Adrian and de Pinto, Marco},
	month = jun,
	year = {2018},
	keywords = {\_tablet},
	pages = {3089--3105},
	file = {Chadi_de Pinto_2018_Selecting successful students.pdf:/Users/neilnatarajan/Zotero/storage/J5G4G9CK/Chadi_de Pinto_2018_Selecting successful students.pdf:application/pdf},
}

@article{walser_quasi-experiments_nodate,
	title = {Quasi-{Experiments} in {Schools}: {The} {Case} for {Historical} {Cohort} {Control} {Groups}},
	shorttitle = {Quasi-{Experiments} in {Schools}},
	url = {https://scholarworks.umass.edu/pare/vol19/iss1/6/},
	doi = {10.7275/17HJ-1K58},
	abstract = {There is increased emphasis on using experimental and quasi-experimental methods to evaluate educational programs; however, educational evaluators and school leaders are often faced with challenges when implementing such designs in educational settings. Use of a historical cohort control group design provides a viable option for conducting quasi-experiments in school-based outcome evaluation. A cohort is a successive group that goes through some experience together, such as a grade level or a training program. A historical cohort comparison group is a cohort group selected from pre-treatment archival data and matched to a subsequent cohort currently receiving a treatment. Although prone to the same threats to study validity as any quasi-experiment, issues related to selection, history, and maturation can be particularly challenging. However, use of a historical cohort control group can reduce noncomparability of treatment and control conditions through local, focal matching. In addition, a historical cohort control group design can alleviate concerns about denying program access to students in order to form a control group, minimize resource requirements and disruption to school routines, and make use of archival data schools and school districts collect and find meaningful. Accessed 12,614 times on https://pareonline.net from June 23, 2014 to December 31, 2019. For downloads from January 1, 2020 forward, please click on the PlumX Metrics link to the right.},
	language = {en},
	urldate = {2022-04-12},
	author = {Walser, Tamara M.},
	note = {Publisher: University of Massachusetts Amherst},
	keywords = {\_tablet},
	file = {Walser_Quasi-Experiments in Schools.pdf:/Users/neilnatarajan/Zotero/storage/6FB35U2V/Walser_Quasi-Experiments in Schools.pdf:application/pdf},
}

@article{schumann_diverse_2019,
	title = {The {Diverse} {Cohort} {Selection} {Problem}},
	url = {http://arxiv.org/abs/1709.03441},
	abstract = {How should a firm allocate its limited interviewing resources to select the optimal cohort of new employees from a large set of job applicants? How should that firm allocate cheap but noisy resume screenings and expensive but in-depth in-person interviews? We view this problem through the lens of combinatorial pure exploration (CPE) in the multi-armed bandit setting, where a central learning agent performs costly exploration of a set of arms before selecting a final subset with some combinatorial structure. We generalize a recent CPE algorithm to the setting where arm pulls can have different costs and return different levels of information. We then prove theoretical upper bounds for a general class of arm-pulling strategies in this new setting. We apply our general algorithm to a real-world problem with combinatorial structure: incorporating diversity into university admissions. We take real data from admissions at one of the largest US-based computer science graduate programs and show that a simulation of our algorithm produces a cohort with hiring overall utility while spending comparable budget to the current admissions process at that university.},
	urldate = {2022-04-12},
	journal = {arXiv:1709.03441 [cs]},
	author = {Schumann, Candice and Counts, Samsara N. and Foster, Jeffrey S. and Dickerson, John P.},
	month = mar,
	year = {2019},
	note = {arXiv: 1709.03441},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/2TIBJVN5/1709.html:text/html;Schumann et al_2019_The Diverse Cohort Selection Problem.pdf:/Users/neilnatarajan/Zotero/storage/QVALQ3QA/Schumann et al_2019_The Diverse Cohort Selection Problem.pdf:application/pdf},
}

@article{huppenkothen_entrofy_2020,
	title = {Entrofy your cohort: {A} transparent method for diverse cohort selection},
	volume = {15},
	issn = {1932-6203},
	shorttitle = {Entrofy your cohort},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7384611/},
	doi = {10.1371/journal.pone.0231939},
	abstract = {Selecting a cohort from a set of candidates is a common task within and beyond academia. Admitting students, awarding grants, and choosing speakers for a conference are situations where human biases may affect the selection of any particular candidate, and, thereby the composition of the final cohort. In this paper, we propose a new algorithm, entrofy, designed to be part of a human-in-the-loop decision making strategy aimed at making cohort selection as just, transparent, and accountable as possible. We suggest embedding entrofy in a two-step selection procedure. During a merit review, the committee selects all applicants, submissions, or other entities that meet their merit-based criteria. This often yields a cohort larger than the admissible number. In the second stage, the target cohort can be chosen from this meritorious pool via a new algorithm and software tool called entrofy. entrofy optimizes differences across an assignable set of categories selected by the human committee. Criteria could include academic discipline, home country, experience with certain technologies, or other quantifiable characteristics. The entrofy algorithm then yields the approximation of pre-defined target proportions for each category by solving the tie-breaking problem with provable performance guarantees. We show how entrofy selects cohorts according to pre-determined characteristics in simulated sets of applications and demonstrate its use in a case study of Astro Hack Week. This two stage candidate and cohort selection process allows human judgment and debate to guide the assessment of candidates’ merit in step 1. Then the human committee defines relevant diversity criteria which will be used as computational parameters in entrofy. Once the parameters are defined, the set of candidates who meet the minimum threshold for merit are passed through the entrofy cohort selection procedure in step 2 which yields a cohort of a composition as close as possible to the computational parameters defined by the committee. This process has the benefit of separating the meritorious assessment of candidates from certain elements of their diversity and from some considerations around cohort composition. It also increases the transparency and auditability of the process, which enables, but does not guarantee, fairness. Splitting merit and diversity considerations into their own assessment stages makes it easier to explain why a given candidate was selected or rejected, though it does not eliminate the possibility of objectionable bias.},
	number = {7},
	urldate = {2022-04-12},
	journal = {PLoS ONE},
	author = {Huppenkothen, Daniela and McFee, Brian and Norén, Laura},
	month = jul,
	year = {2020},
	pmid = {32716929},
	pmcid = {PMC7384611},
	pages = {e0231939},
	file = {Full Text:/Users/neilnatarajan/Zotero/storage/8XZ33SGI/Huppenkothen et al. - 2020 - Entrofy your cohort A transparent method for dive.pdf:application/pdf},
}

@article{binns_human_2022,
	title = {Human {Judgment} in algorithmic loops: {Individual} justice and automated decision-making},
	volume = {16},
	issn = {1748-5991},
	shorttitle = {Human {Judgment} in algorithmic loops},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rego.12358},
	doi = {10.1111/rego.12358},
	abstract = {Arguments in favor of tempering algorithmic decision making with human judgment often appeal to concepts and criteria derived from legal philosophy about the nature of law and legal reasoning, arguing that algorithmic systems cannot satisfy them (but humans can). Such arguments often make implicit appeal to the notion that each case needs to be assessed on its own merits, without comparison to or generalization from previous cases. This article argues that this notion of individual justice can only be meaningfully served through human judgment. It distinguishes individual justice and considers how it relates to other dimensions of justice, namely consistency and fairness / nondiscrimination. Finally, it identifies and discussess two challenges: first, how individual justice can be accommodated alongside other dimensions of justice in the socio-technical contexts of humans-in-the-loop; and second, how inequities in individual justice may result from an uneven application of human judgment in algorithmic contexts.},
	language = {en},
	number = {1},
	urldate = {2022-04-14},
	journal = {Regulation \& Governance},
	author = {Binns, Reuben},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rego.12358},
	keywords = {\_tablet, algorithmic regulation, data protection, discretion, human-in-the-loop, justice},
	pages = {197--211},
	file = {Binns_2022_Human Judgment in algorithmic loops.pdf:/Users/neilnatarajan/Zotero/storage/8ENLVGM2/Binns_2022_Human Judgment in algorithmic loops.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/A4S8UTEQ/rego.html:text/html},
}

@article{weerts_human-grounded_2019,
	title = {A {Human}-{Grounded} {Evaluation} of {SHAP} for {Alert} {Processing}},
	url = {http://arxiv.org/abs/1907.03324},
	abstract = {In the past years, many new explanation methods have been proposed to achieve interpretability of machine learning predictions. However, the utility of these methods in practical applications has not been researched extensively. In this paper we present the results of a human-grounded evaluation of SHAP, an explanation method that has been well-received in the XAI and related communities. In particular, we study whether this local model-agnostic explanation method can be useful for real human domain experts to assess the correctness of positive predictions, i.e. alerts generated by a classifier. We performed experimentation with three different groups of participants (159 in total), who had basic knowledge of explainable machine learning. We performed a qualitative analysis of recorded reflections of experiment participants performing alert processing with and without SHAP information. The results suggest that the SHAP explanations do impact the decision-making process, although the model's confidence score remains to be a leading source of evidence. We statistically test whether there is a significant difference in task utility metrics between tasks for which an explanation was available and tasks in which it was not provided. As opposed to common intuitions, we did not find a significant difference in alert processing performance when a SHAP explanation is available compared to when it is not.},
	urldate = {2022-04-14},
	journal = {arXiv:1907.03324 [cs, stat]},
	author = {Weerts, Hilde J. P. and van Ipenburg, Werner and Pechenizkiy, Mykola},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.03324},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Human-Computer Interaction, \_tablet},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/8Y57BJUL/1907.html:text/html;Weerts et al_2019_A Human-Grounded Evaluation of SHAP for Alert Processing.pdf:/Users/neilnatarajan/Zotero/storage/PBXD8NP3/Weerts et al_2019_A Human-Grounded Evaluation of SHAP for Alert Processing.pdf:application/pdf},
}

@article{murdoch_definitions_2019,
	title = {Definitions, methods, and applications in interpretable machine learning},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1900654116},
	doi = {10.1073/pnas.1900654116},
	abstract = {Significance
            The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work.
          , 
            Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	language = {en},
	number = {44},
	urldate = {2022-04-14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
	month = oct,
	year = {2019},
	keywords = {\_tablet},
	pages = {22071--22080},
	file = {Murdoch et al_2019_Definitions, methods, and applications in interpretable machine learning.pdf:/Users/neilnatarajan/Zotero/storage/56TDM4D9/Murdoch et al_2019_Definitions, methods, and applications in interpretable machine learning.pdf:application/pdf},
}

@article{rudin_interpretable_2021,
	title = {Interpretable {Machine} {Learning}: {Fundamental} {Principles} and 10 {Grand} {Challenges}},
	shorttitle = {Interpretable {Machine} {Learning}},
	url = {http://arxiv.org/abs/2103.11251},
	abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the "Rashomon set" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.},
	urldate = {2022-04-14},
	journal = {arXiv:2103.11251 [cs, stat]},
	author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
	month = jul,
	year = {2021},
	note = {arXiv: 2103.11251},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, \_tablet, 68T01},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/XK8MDDNN/2103.html:text/html;Rudin et al_2021_Interpretable Machine Learning.pdf:/Users/neilnatarajan/Zotero/storage/HGA2DHWE/Rudin et al_2021_Interpretable Machine Learning.pdf:application/pdf},
}

@article{vereschak_how_2021,
	title = {How to {Evaluate} {Trust} in {AI}-{Assisted} {Decision} {Making}? {A} {Survey} of {Empirical} {Methodologies}},
	volume = {5},
	shorttitle = {How to {Evaluate} {Trust} in {AI}-{Assisted} {Decision} {Making}?},
	url = {https://doi.org/10.1145/3476068},
	doi = {10.1145/3476068},
	abstract = {The spread of AI-embedded systems involved in human decision making makes studying human trust in these systems critical. However, empirically investigating trust is challenging. One reason is the lack of standard protocols to design trust experiments. In this paper, we present a survey of existing methods to empirically investigate trust in AI-assisted decision making and analyse the corpus along the constitutive elements of an experimental protocol. We find that the definition of trust is not commonly integrated in experimental protocols, which can lead to findings that are overclaimed or are hard to interpret and compare across studies. Drawing from empirical practices in social and cognitive studies on human-human trust, we provide practical guidelines to improve the methodology of studying Human-AI trust in decision-making contexts. In addition, we bring forward research opportunities of two types: one focusing on further investigation regarding trust methodologies and the other on factors that impact Human-AI trust.},
	number = {CSCW2},
	urldate = {2022-04-14},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Vereschak, Oleksandra and Bailly, Gilles and Caramiaux, Baptiste},
	month = oct,
	year = {2021},
	keywords = {artificial intelligence, trust, \_tablet, decision making, methodology},
	pages = {327:1--327:39},
	file = {Vereschak et al_2021_How to Evaluate Trust in AI-Assisted Decision Making.pdf:/Users/neilnatarajan/Zotero/storage/TCP8T6PK/Vereschak et al_2021_How to Evaluate Trust in AI-Assisted Decision Making.pdf:application/pdf},
}

@article{shneiderman_human-centered_2020,
	title = {Human-{Centered} {Artificial} {Intelligence}: {Reliable}, {Safe} \& {Trustworthy}},
	shorttitle = {Human-{Centered} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2002.04087},
	abstract = {Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe \& Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.},
	urldate = {2022-04-14},
	journal = {arXiv:2002.04087 [cs]},
	author = {Shneiderman, Ben},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.04087},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, H.5.0},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/ZYZUQ3GP/Shneiderman - 2020 - Human-Centered Artificial Intelligence Reliable, .pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/PFVN9S2I/2002.html:text/html},
}

@misc{noauthor_how_nodate,
	title = {How to explain {AI} systems to end users: a systematic literature review and research agenda {\textbar} {Emerald} {Insight}},
	url = {https://www.emerald.com/insight/content/doi/10.1108/INTR-08-2021-0600/full/html},
	urldate = {2022-05-11},
	file = {How to explain AI systems to end users\: a systematic literature review and research agenda | Emerald Insight:/Users/neilnatarajan/Zotero/storage/96MA57PV/html.html:text/html},
}

@article{weerts_case-based_2019,
	title = {Case-{Based} {Reasoning} for {Assisting} {Domain} {Experts} in {Processing} {Fraud} {Alerts} of {Black}-{Box} {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/1907.03334},
	abstract = {In many contexts, it can be useful for domain experts to understand to what extent predictions made by a machine learning model can be trusted. In particular, estimates of trustworthiness can be useful for fraud analysts who process machine learning-generated alerts of fraudulent transactions. In this work, we present a case-based reasoning (CBR) approach that provides evidence on the trustworthiness of a prediction in the form of a visualization of similar previous instances. Different from previous works, we consider similarity of local post-hoc explanations of predictions and show empirically that our visualization can be useful for processing alerts. Furthermore, our approach is perceived useful and easy to use by fraud analysts at a major Dutch bank.},
	urldate = {2022-05-12},
	journal = {arXiv:1907.03334 [cs, stat]},
	author = {Weerts, Hilde J. P. and van Ipenburg, Werner and Pechenizkiy, Mykola},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.03334},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/RSKRRF88/Weerts et al. - 2019 - Case-Based Reasoning for Assisting Domain Experts .pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/IAJMXE6R/1907.html:text/html},
}

@misc{biyik_batch_2018,
	title = {Batch {Active} {Preference}-{Based} {Learning} of {Reward} {Functions}},
	url = {http://arxiv.org/abs/1810.04303},
	doi = {10.48550/arXiv.1810.04303},
	abstract = {Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Bıyık, Erdem and Sadigh, Dorsa},
	month = oct,
	year = {2018},
	note = {arXiv:1810.04303 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/BXV7S9IB/Bıyık and Sadigh - 2018 - Batch Active Preference-Based Learning of Reward F.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/65BCXHJD/1810.html:text/html},
}

@misc{noauthor_6_2021,
	title = {6 - {Debate} and {Imitative} {Generalization} with {Beth} {Barnes}},
	url = {https://axrp.net/episode/2021/04/08/episode-6-debate-beth-barnes.html},
	abstract = {Google Podcasts link},
	language = {en},
	urldate = {2022-08-03},
	journal = {AXRP - the AI X-risk Research Podcast},
	month = apr,
	year = {2021},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/3MTAV2JA/episode-6-debate-beth-barnes.html:text/html},
}

@misc{christiano_eliciting_2022,
	title = {Eliciting latent knowledge},
	url = {https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc},
	abstract = {(This is a repost from December 2021, linking to a google doc.)},
	language = {en},
	urldate = {2022-08-03},
	journal = {Medium},
	author = {Christiano, Paul},
	month = feb,
	year = {2022},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/9FNX6H4L/eliciting-latent-knowledge-f977478608fc.html:text/html},
}

@misc{noauthor_ai-written_2022,
	title = {{AI}-{Written} {Critiques} {Help} {Humans} {Notice} {Flaws}},
	url = {https://openai.com/blog/critiques/},
	abstract = {Showing model-generated critical comments to humans helps them find flaws in summaries.},
	language = {en},
	urldate = {2022-08-03},
	journal = {OpenAI},
	month = jun,
	year = {2022},
}

@misc{parrish_single-turn_2022,
	title = {Single-{Turn} {Debate} {Does} {Not} {Help} {Humans} {Answer} {Hard} {Reading}-{Comprehension} {Questions}},
	url = {http://arxiv.org/abs/2204.05212},
	abstract = {Current QA systems can generate reasonable-sounding yet false answers without explanation or evidence for the generated answer, which is especially problematic when humans cannot readily check the model's answers. This presents a challenge for building trust in machine learning systems. We take inspiration from real-world situations where difficult questions are answered by considering opposing sides (see Irving et al., 2018). For multiple-choice QA examples, we build a dataset of single arguments for both a correct and incorrect answer option in a debate-style set-up as an initial step in training models to produce explanations for two candidate answers. We use long contexts -- humans familiar with the context write convincing explanations for pre-selected correct and incorrect answers, and we test if those explanations allow humans who have not read the full context to more accurately determine the correct answer. We do not find that explanations in our set-up improve human accuracy, but a baseline condition shows that providing human-selected text snippets does improve accuracy. We use these findings to suggest ways of improving the debate set up for future data collection efforts.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Parrish, Alicia and Trivedi, Harsh and Perez, Ethan and Chen, Angelica and Nangia, Nikita and Phang, Jason and Bowman, Samuel R.},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05212 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/MFCPFMUZ/Parrish et al. - 2022 - Single-Turn Debate Does Not Help Humans Answer Har.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/PY666PZT/2204.html:text/html},
}

@misc{saunders_self-critiquing_2022,
	title = {Self-critiquing models for assisting human evaluators},
	url = {http://arxiv.org/abs/2206.05802},
	doi = {10.48550/arXiv.2206.05802},
	abstract = {We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
	month = jun,
	year = {2022},
	note = {arXiv:2206.05802 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/VFJIMKYC/Saunders et al. - 2022 - Self-critiquing models for assisting human evaluat.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/7SYGZ72F/2206.html:text/html},
}

@misc{leike_scalable_2018,
	title = {Scalable agent alignment via reward modeling: a research direction},
	shorttitle = {Scalable agent alignment via reward modeling},
	url = {http://arxiv.org/abs/1811.07871},
	doi = {10.48550/arXiv.1811.07871},
	abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
	month = nov,
	year = {2018},
	note = {arXiv:1811.07871 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/NXSBF35A/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/N3AD4J7F/1811.html:text/html},
}

@misc{irving_ai_2018,
	title = {{AI} safety via debate},
	url = {http://arxiv.org/abs/1805.00899},
	doi = {10.48550/arXiv.1805.00899},
	abstract = {To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
	month = oct,
	year = {2018},
	note = {arXiv:1805.00899 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/SE6MAM82/Irving et al. - 2018 - AI safety via debate.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/BMFVFFGN/1805.html:text/html},
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05862 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/HR26S4D9/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/VLIDQAEN/2204.html:text/html},
}

@misc{biyik_batch_2018-1,
	title = {Batch {Active} {Preference}-{Based} {Learning} of {Reward} {Functions}},
	url = {http://arxiv.org/abs/1810.04303},
	abstract = {Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Bıyık, Erdem and Sadigh, Dorsa},
	month = oct,
	year = {2018},
	note = {arXiv:1810.04303 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/B4UEJN5V/Bıyık and Sadigh - 2018 - Batch Active Preference-Based Learning of Reward F.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/EPHSYIX5/1810.html:text/html},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {Spinning} {Up} in {Deep} {RL}! — {Spinning} {Up} documentation},
	url = {https://spinningup.openai.com/en/latest/},
	urldate = {2022-08-03},
	file = {Welcome to Spinning Up in Deep RL! — Spinning Up documentation:/Users/neilnatarajan/Zotero/storage/5XCZV9R4/latest.html:text/html},
}

@misc{noauthor_cs_nodate,
	title = {{CS} 294 {Deep} {Reinforcement} {Learning}, {Fall} 2017},
	url = {http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html},
	urldate = {2022-08-03},
	file = {CS 294 Deep Reinforcement Learning, Fall 2017:/Users/neilnatarajan/Zotero/storage/S96LC5M8/index.html:text/html},
}

@inproceedings{sadigh_active_2017,
	title = {Active {Preference}-{Based} {Learning} of {Reward} {Functions}},
	isbn = {978-0-9923747-3-0},
	url = {http://www.roboticsproceedings.org/rss13/p53.pdf},
	doi = {10.15607/RSS.2017.XIII.053},
	abstract = {Our goal is to efﬁciently learn reward functions encoding a human’s preferences for how a dynamical system should act. There are two challenges with this. First, in many problems it is difﬁcult for people to provide demonstrations of the desired system trajectory (like a high-DOF robot arm motion or an aggressive driving maneuver), or to even assign how much numerical reward an action or trajectory should get. We build on work in label ranking and propose to learn from preferences (or comparisons) instead: the person provides the system a relative preference between two trajectories. Second, the learned reward function strongly depends on what environments and trajectories were experienced during the training phase. We thus take an active learning approach, in which the system decides on what preference queries to make. A novel aspect of our work is the complexity and continuous nature of the queries: continuous trajectories of a dynamical system in environments with other moving agents (humans or robots). We contribute a method for actively synthesizing queries that satisfy the dynamics of the system. Further, we learn the reward function from a continuous hypothesis space by maximizing the volume removed from the hypothesis space by each query. We assign weights to the hypothesis space in the form of a log-concave distribution and provide a bound on the number of iterations required to converge. We show that our algorithm converges faster to the desired reward compared to approaches that are not active or that do not synthesize queries in an autonomous driving domain. We then run a user study to put our method to the test with real people.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Robotics: {Science} and {Systems} {XIII}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Sadigh, Dorsa and Dragan, Anca and Sastry, Shankar and Seshia, Sanjit},
	month = jul,
	year = {2017},
	file = {Sadigh et al. - 2017 - Active Preference-Based Learning of Reward Functio.pdf:/Users/neilnatarajan/Zotero/storage/TIF4AZNQ/Sadigh et al. - 2017 - Active Preference-Based Learning of Reward Functio.pdf:application/pdf},
}

@article{sutton_reinforcement_nodate,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	language = {en},
	author = {Sutton, Richard S and Barto, Andrew G},
	pages = {352},
	file = {Sutton and Barto - Reinforcement Learning An Introduction.pdf:/Users/neilnatarajan/Zotero/storage/XKA9BDZ6/Sutton and Barto - Reinforcement Learning An Introduction.pdf:application/pdf},
}

@article{wang_transparency_2022,
	title = {Transparency as {Manipulation}? {Uncovering} the {Disciplinary} {Power} of {Algorithmic} {Transparency}},
	volume = {35},
	issn = {2210-5433, 2210-5441},
	shorttitle = {Transparency as {Manipulation}?},
	url = {https://link.springer.com/10.1007/s13347-022-00564-w},
	doi = {10.1007/s13347-022-00564-w},
	abstract = {Automated algorithms are silently making crucial decisions about our lives, but most of the time we have little understanding of how they work. To counter this hidden influence, there have been increasing calls for algorithmic transparency. Much ink has been spilled over the informational account of algorithmic transparency—about how much information should be revealed about the inner workings of an algorithm. But few studies question the power structure beneath the informational disclosure of the algorithm. As a result, the information disclosure itself can be a means of manipulation used by a group of people to advance their own interests. Instead of concentrating on information disclosure, this paper examines algorithmic transpar‑ency from the perspective of power, explaining how algorithmic transparency under a disciplinary power structure can be a technique of normalizing people’s behav‑ior. The informational disclosure of an algorithm can not only set up some de facto norms, but also build a scientific narrative of its algorithm to justify those norms. In doing so, people would be internally motivated to follow those norms with less criti‑cal analysis. This article suggests that we should not simply open the black box of an algorithm without challenging the existing power relations.},
	language = {en},
	number = {3},
	urldate = {2022-08-04},
	journal = {Philosophy \& Technology},
	author = {Wang, Hao},
	month = sep,
	year = {2022},
	pages = {69},
	file = {Wang_2022_Transparency as Manipulation_annotated.pdf:/Users/neilnatarajan/Zotero/storage/EM733RJZ/Wang_2022_Transparency as Manipulation_annotated.pdf:application/pdf;Wang_2022_Transparency as Manipulation.pdf:/Users/neilnatarajan/Zotero/storage/EM733RJZ/Wang_2022_Transparency as Manipulation.pdf:application/pdf},
}

@misc{liao_human-centered_2022,
	title = {Human-{Centered} {Explainable} {AI} ({XAI}): {From} {Algorithms} to {User} {Experiences}},
	shorttitle = {Human-{Centered} {Explainable} {AI} ({XAI})},
	url = {http://arxiv.org/abs/2110.10790},
	abstract = {In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question "what are human-centered approaches doing for XAI" and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users' explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.},
	urldate = {2022-08-18},
	publisher = {arXiv},
	author = {Liao, Q. Vera and Varshney, Kush R.},
	month = apr,
	year = {2022},
	note = {arXiv:2110.10790 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, \_tablet},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/Q3NRRGHW/2110.html:text/html;Liao_Varshney_2022_Human-Centered Explainable AI (XAI).pdf:/Users/neilnatarajan/Zotero/storage/VAJV5NEW/Liao_Varshney_2022_Human-Centered Explainable AI (XAI).pdf:application/pdf},
}

@misc{camburu_struggles_2020,
	title = {The {Struggles} of {Feature}-{Based} {Explanations}: {Shapley} {Values} vs. {Minimal} {Sufficient} {Subsets}},
	shorttitle = {The {Struggles} of {Feature}-{Based} {Explanations}},
	url = {http://arxiv.org/abs/2009.11023},
	doi = {10.48550/arXiv.2009.11023},
	abstract = {For neural models to garner widespread public trust and ensure fairness, we must have human-intelligible explanations for their predictions. Recently, an increasing number of works focus on explaining the predictions of neural models in terms of the relevance of the input features. In this work, we show that feature-based explanations pose problems even for explaining trivial models. We show that, in certain cases, there exist at least two ground-truth feature-based explanations, and that, sometimes, neither of them is enough to provide a complete view of the decision-making process of the model. Moreover, we show that two popular classes of explainers, Shapley explainers and minimal sufficient subsets explainers, target fundamentally different types of ground-truth explanations, despite the apparently implicit assumption that explainers should look for one specific feature-based explanation. These findings bring an additional dimension to consider in both developing and choosing explainers.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Camburu, Oana-Maria and Giunchiglia, Eleonora and Foerster, Jakob and Lukasiewicz, Thomas and Blunsom, Phil},
	month = dec,
	year = {2020},
	note = {arXiv:2009.11023 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/IU7MTK3S/2009.html:text/html;Camburu et al_2020_The Struggles of Feature-Based Explanations_annotated.pdf:/Users/neilnatarajan/Zotero/storage/KQ4TRXBG/Camburu et al_2020_The Struggles of Feature-Based Explanations_annotated.pdf:application/pdf;Camburu et al_2020_The Struggles of Feature-Based Explanations.pdf:/Users/neilnatarajan/Zotero/storage/KQ4TRXBG/Camburu et al_2020_The Struggles of Feature-Based Explanations.pdf:application/pdf},
}

@inproceedings{wan_explainabilitys_2022,
	address = {Oxford United Kingdom},
	title = {Explainability's {Gain} is {Optimality}'s {Loss}?: {How} {Explanations} {Bias} {Decision}-making},
	isbn = {978-1-4503-9247-1},
	shorttitle = {Explainability's {Gain} is {Optimality}'s {Loss}?},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534156},
	doi = {10.1145/3514094.3534156},
	abstract = {Decisions in organizations are about evaluating alternatives and choosing the one that would best serve organizational goals. To the extent that the evaluation of alternatives could be formulated as a predictive task with appropriate metrics, machine learning algorithms are increasingly being used to improve the efficiency of the process. Explanations help to facilitate communication between the algorithm and the human decision-maker, making it easier for the latter to interpret and make decisions on the basis of predictions by the former. Feature-based explanations’ semantics of causal models, however, induce leakage from the decision-maker’s prior beliefs. Our findings from a field experiment demonstrate empirically how this leads to confirmation bias and disparate impact on the decisionmaker’s confidence in the predictions. Such differences can lead to sub-optimal and biased decision outcomes.},
	language = {en},
	urldate = {2022-08-23},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Wan, Charles and Belo, Rodrigo and Zejnilovic, Leid},
	month = jul,
	year = {2022},
	pages = {778--787},
	file = {Wan et al_2022_Explainability's Gain is Optimality's Loss_annotated.pdf:/Users/neilnatarajan/Zotero/storage/UVJBBMTA/Wan et al_2022_Explainability's Gain is Optimality's Loss_annotated.pdf:application/pdf;Wan et al_2022_Explainability's Gain is Optimality's Loss.pdf:/Users/neilnatarajan/Zotero/storage/UVJBBMTA/Wan et al_2022_Explainability's Gain is Optimality's Loss.pdf:application/pdf},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/9836QDJ6/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/WZWW244S/2204.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/8ZEKLULT/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6N8ZF8SV/2005.html:text/html},
}

@article{ostergaard_does_2011,
	title = {Does a different view create something new? {The} effect of employee diversity on innovation},
	volume = {40},
	issn = {0048-7333},
	shorttitle = {Does a different view create something new?},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733310002398},
	doi = {10.1016/j.respol.2010.11.004},
	abstract = {A growing literature is analysing the relation between diversity in the knowledge base and the performance of firms; nevertheless, studies that investigate the impact of employee diversity on innovation are scarce. Innovation is an interactive process that often involves communication and interaction among employees in a firm and draws on their different qualities from all levels of the organisation. This paper investigates the relation between employee diversity and innovation in terms of gender, age, ethnicity, and education. The analyses draw on data from a recent innovation survey. This data is merged with a linked employer–employee dataset that allow us to identify the employee composition of each firm. We test the hypothesis that employee diversity is associated with better innovative performance. The econometric analysis reveals a positive relation between diversity in education and gender on the likelihood of introducing an innovation. Furthermore, we find a negative effect of age diversity and no significant effect of ethnicity on the firm's likelihood to innovate. In addition, the logistic regression reveals a positive relationship between an open culture towards diversity and innovative performance. We find no support of any curvilinear relation between diversity and innovation.},
	language = {en},
	number = {3},
	urldate = {2022-08-28},
	journal = {Research Policy},
	author = {Østergaard, Christian R. and Timmermans, Bram and Kristinsson, Kari},
	month = apr,
	year = {2011},
	keywords = {Diversity, Education, Ethnicity, Gender, Innovation},
	pages = {500--509},
	file = {ScienceDirect Full Text PDF:/Users/neilnatarajan/Zotero/storage/DDZXCUKX/Østergaard et al. - 2011 - Does a different view create something new The ef.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/T6UUJC7P/S0048733310002398.html:text/html},
}

@inproceedings{yin_understanding_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Understanding the {Effect} of {Accuracy} on {Trust} in {Machine} {Learning} {Models}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300509},
	doi = {10.1145/3290605.3300509},
	abstract = {We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.},
	urldate = {2022-09-05},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yin, Ming and Wortman Vaughan, Jennifer and Wallach, Hanna},
	month = may,
	year = {2019},
	keywords = {trust, machine learning, human-subject experiments},
	pages = {1--12},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/BKL5C96V/Yin et al. - 2019 - Understanding the Effect of Accuracy on Trust in M.pdf:application/pdf},
}

@inproceedings{kohavi_scaling_1996,
	address = {Portland, Oregon},
	series = {{KDD}'96},
	title = {Scaling up the accuracy of {Naive}-{Bayes} classifiers: a decision-tree hybrid},
	shorttitle = {Scaling up the accuracy of {Naive}-{Bayes} classifiers},
	abstract = {Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classifiers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classifiers that frequently outperform both constituents, especially in the larger databases tested.},
	urldate = {2022-09-12},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {AAAI Press},
	author = {Kohavi, Ron},
	month = aug,
	year = {1996},
	pages = {202--207},
}

@book{caldwell_power_nodate,
	title = {Power {Analysis} with {Superpower}},
	url = {https://aaroncaldwell.us/SuperpowerBook/index.html#preface},
	abstract = {This is a book describing the capabilities of the Superpower R package.},
	urldate = {2022-09-13},
	author = {Caldwell, Aaron R. and Lakens, Daniël and Parlett-Pelleriti, Chelsea M. and Prochilo, Guy and Aust, Frederik},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/LZ67UGSS/index.html:text/html},
}

@misc{noauthor_facebook_nodate,
	title = {Facebook},
	url = {https://www.facebook.com/facebook/videos/1292820370894105/},
	urldate = {2022-09-13},
	file = {Facebook:/Users/neilnatarajan/Zotero/storage/QGX7FSLV/1292820370894105.html:text/html},
}

@inproceedings{yin_understanding_2019-1,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Understanding the {Effect} of {Accuracy} on {Trust} in {Machine} {Learning} {Models}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300509},
	doi = {10.1145/3290605.3300509},
	abstract = {We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yin, Ming and Wortman Vaughan, Jennifer and Wallach, Hanna},
	month = may,
	year = {2019},
	keywords = {trust, machine learning, human-subject experiments},
	pages = {1--12},
}

@article{westen_quantifying_2003,
	title = {Quantifying construct validity: {Two} simple measures},
	volume = {84},
	issn = {1939-1315},
	shorttitle = {Quantifying construct validity},
	doi = {10.1037/0022-3514.84.3.608},
	abstract = {Construct validity is one of the most central concepts in psychology. Researchers generally establish the construct validity of a measure by correlating it with a number of other measures and arguing from the pattern of correlations that the measure is associated with these variables in theoretically predictable ways. This article presents 2 simple metrics for quantifying construct validity that provide effect size estimates indicating the extent to which the observed pattern of correlations in a convergent-discriminant validity matrix matches the theoretically predicted pattern of correlations. Both measures, based on contrast analysis, provide simple estimates of validity that can be compared across studies, constructs, and measures meta-analytically, and can be implemented without the use of complex statistical procedures that may limit their accessibility. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	journal = {Journal of Personality and Social Psychology},
	author = {Westen, Drew and Rosenthal, Robert},
	year = {2003},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Construct Validity, Discriminant Validity, Effect Size (Statistical), Measurement, Prediction, Statistical Significance, Statistical Validity},
	pages = {608--618},
	file = {Full Text:/Users/neilnatarajan/Zotero/storage/HQBEHIMP/Westen and Rosenthal - 2003 - Quantifying construct validity Two simple measure.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/JLJH3NZU/doiLanding.html:text/html},
}

@misc{noauthor_construct_nodate,
	title = {Construct {Validity} of {Likert} {Scales} through {Confirmatory} {Factor} {Analysis}: {A} {Simulation} {Study} {Comparing} {Different} {Methods} of {Estimation} {Based} on {Pearson} and {Polychoric} {Correlations} {\textbar} {Morata}-{Ramírez} {\textbar} {International} {Journal} of {Social} {Science} {Studies}},
	url = {https://redfame.com/journal/index.php/ijsss/article/view/27},
	urldate = {2022-09-15},
	file = {Construct Validity of Likert Scales through Confirmatory Factor Analysis\: A Simulation Study Comparing Different Methods of Estimation Based on Pearson and Polychoric Correlations | Morata-Ramírez | International Journal of Social Science Studies:/Users/neilnatarajan/Zotero/storage/989ER42R/27.html:text/html},
}

@article{morata-ramirez_construct_2013,
	title = {Construct {Validity} of {Likert} {Scales} through {Confirmatory} {Factor} {Analysis}: {A} {Simulation} {Study} {Comparing} {Different} {Methods} of {Estimation} {Based} on {Pearson} and {Polychoric} {Correlations}},
	volume = {1},
	copyright = {Copyright (c)},
	issn = {2324-8041},
	shorttitle = {Construct {Validity} of {Likert} {Scales} through {Confirmatory} {Factor} {Analysis}},
	url = {https://redfame.com/journal/index.php/ijsss/article/view/27},
	doi = {10.11114/ijsss.v1i1.27},
	abstract = {The widespread use of Pearson correlations and, by extension, the Maximum Likelihood estimation method, does not take into account the measurement properties of Likert scales observed variables when carrying out a construct validity process through Confirmatory Factor Analysis (CFA). This simulation study compares four estimation methods (Maximum Likelihood –ML-, Robust Maximum Likelihood –RML-, Robust Unweighted Least Squares –, RULS) according to two of the assumptions CFA is supposed to fulfil: multivariate normality and, especially, the continuous measurement nature of both latent and observed variables. Goodness of fit is diagnosed by X2 Likelihood Ratio Test and RMSEA indices. Results suggest ULS and RULS are preferable as polychoric correlations help to overcome grouping and transformation errors produced when using Pearson correlations for ordinal observed variables. Data measurement scale consideration enhances the ability of hypothesized models to reproduce accurately construct variables relationships.},
	language = {en},
	number = {1},
	urldate = {2022-09-15},
	journal = {International Journal of Social Science Studies},
	author = {Morata-Ramírez, María de los Ángeles and Holgado-Tello, Francisco Pablo},
	month = jan,
	year = {2013},
	note = {Number: 1},
	pages = {54--61},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/UVDDF45R/Morata-Ramírez and Holgado-Tello - 2013 - Construct Validity of Likert Scales through Confir.pdf:application/pdf},
}

@article{crites_measuring_1994,
	title = {Measuring the {Affective} and {Cognitive} {Properties} of {Attitudes}: {Conceptual} and {Methodological} {Issues}},
	volume = {20},
	issn = {0146-1672},
	shorttitle = {Measuring the {Affective} and {Cognitive} {Properties} of {Attitudes}},
	url = {https://doi.org/10.1177/0146167294206001},
	doi = {10.1177/0146167294206001},
	abstract = {Despite renewed interest in the affective and cognitive properties of attitudes, assessment of these constructs is plagued by a number of problems. Some techniques for overcoming these problems are outlined, and scales for assessing the affective and cognitive properties of attitudes are reported. Two studies examine the reliability and validity of these scales. Study 1 assesses the internal consistency and the discriminant and convergent validity of these scales and indicates that the scales are useful for assessing the affective and cognitive properties of attitudes toward a wide range of objects. In Study 2, the ability of the scales to differentiate attitudes that are based primarily on affective versus cognitive information is examined by experimentally creating affective or cognitive attitudes in subjects. Analyses reveal that the scales can differentiate between people whose attitudes are based primarily on either affective or cognitive information.},
	language = {en},
	number = {6},
	urldate = {2022-09-16},
	journal = {Personality and Social Psychology Bulletin},
	author = {Crites, Stephen L. and Fabrigar, Leandre R. and Petty, Richard E.},
	month = dec,
	year = {1994},
	note = {Publisher: SAGE Publications Inc},
	pages = {619--634},
	file = {SAGE PDF Full Text:/Users/neilnatarajan/Zotero/storage/ZFA8AAB5/Crites et al. - 1994 - Measuring the Affective and Cognitive Properties o.pdf:application/pdf},
}

@article{lee_trust_2004,
	title = {Trust in {Automation}: {Designing} for {Appropriate} {Reliance}},
	volume = {46},
	issn = {0018-7208},
	shorttitle = {Trust in {Automation}},
	url = {https://journals.sagepub.com/doi/abs/10.1518/hfes.46.1.50_30392},
	doi = {10.1518/hfes.46.1.50_30392},
	abstract = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
	number = {1},
	urldate = {2022-09-16},
	journal = {Human Factors},
	author = {Lee, John D. and See, Katrina A.},
	month = mar,
	year = {2004},
	note = {Publisher: SAGE Publications Inc},
	pages = {50--80},
	file = {SAGE PDF Full Text:/Users/neilnatarajan/Zotero/storage/3BIIJWJQ/Lee and See - 2004 - Trust in Automation Designing for Appropriate Rel.pdf:application/pdf},
}

@misc{burkner_bayesian_2020,
	title = {Bayesian {Item} {Response} {Modeling} in {R} with brms and {Stan}},
	url = {http://arxiv.org/abs/1905.09501},
	abstract = {Item Response Theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective prespecified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. We demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and post-processed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.},
	urldate = {2022-09-29},
	publisher = {arXiv},
	author = {Bürkner, Paul-Christian},
	month = feb,
	year = {2020},
	note = {arXiv:1905.09501 [stat]},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/AEDDX34D/Bürkner - 2020 - Bayesian Item Response Modeling in R with brms and.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GNA3E52G/Bürkner - 2020 - Bayesian Item Response Modeling in R with brms and.html:text/html},
}

@article{murdoch_definitions_2019-1,
	title = {Definitions, methods, and applications in interpretable machine learning},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1900654116},
	doi = {10.1073/pnas.1900654116},
	abstract = {Significance
            The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work.
          , 
            Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	language = {en},
	number = {44},
	urldate = {2022-10-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
	month = oct,
	year = {2019},
	pages = {22071--22080},
	file = {Murdoch et al. - 2019 - Definitions, methods, and applications in interpre.pdf:/Users/neilnatarajan/Zotero/storage/NYVPR69D/Murdoch et al. - 2019 - Definitions, methods, and applications in interpre.pdf:application/pdf},
}

@misc{mattu_how_nodate,
	title = {How {We} {Analyzed} the {COMPAS} {Recidivism} {Algorithm}},
	url = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm},
	abstract = {ProPublica is an independent, non-profit newsroom that produces investigative journalism in the public interest.},
	language = {en},
	urldate = {2022-11-09},
	journal = {ProPublica},
	author = {Mattu, Julia Angwin, Lauren Kirchner, Surya, Jeff Larson},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/5U5BTTY7/how-we-analyzed-the-compas-recidivism-algorithm.html:text/html},
}

@article{ustun_learning_nodate-1,
	title = {Learning {Optimized} {Risk} {Scores}},
	abstract = {Risk scores are simple classiﬁcation models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are diﬃcult to learn from data because they need to be calibrated, sparse, use small integer coeﬃcients, and obey application-speciﬁc constraints. In this paper, we introduce a machine learning method to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a cutting plane algorithm to recover its optimal solution. We improve our algorithm with specialized techniques that generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our algorithm can train risk scores in a way that scales linearly in the number of samples in a dataset, and that allows practitioners to address application-speciﬁc constraints without parameter tuning or post-processing. We benchmark the performance of diﬀerent methods to learn risk scores on publicly available datasets, comparing risk scores produced by our method to risk scores built using methods that are used in practice. We also discuss the practical beneﬁts of our method through a real-world application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.},
	language = {en},
	author = {Ustun, Berk and Rudin, Cynthia},
	keywords = {\_tablet},
	pages = {75},
	file = {Ustun_Rudin_Learning Optimized Risk Scores.pdf:/Users/neilnatarajan/Zotero/storage/SIBJ6IBP/Ustun_Rudin_Learning Optimized Risk Scores.pdf:application/pdf},
}

@misc{marcus_next_2020,
	title = {The {Next} {Decade} in {AI}: {Four} {Steps} {Towards} {Robust} {Artificial} {Intelligence}},
	shorttitle = {The {Next} {Decade} in {AI}},
	url = {http://arxiv.org/abs/2002.06177},
	doi = {10.48550/arXiv.2002.06177},
	abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Marcus, Gary},
	month = feb,
	year = {2020},
	note = {arXiv:2002.06177 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, I.2.6, \_tablet, I.2},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/IQYEFV7G/2002.html:text/html;Marcus_2020_The Next Decade in AI.pdf:/Users/neilnatarajan/Zotero/storage/ZTIP4HAA/Marcus_2020_The Next Decade in AI.pdf:application/pdf},
}

@article{kim_relation_2018,
	title = {Relation between attitudinal trust and behavioral trust: {An} exploratory study},
	volume = {26},
	shorttitle = {Relation between attitudinal trust and behavioral trust},
	abstract = {Previous studies have reported the low predictability of attitudinal trust measures for behavioral
trust outcomes. This study argues that there has been a mismatch of the trust construct by using
social trust attitude measures to predict materialistic trust behavioral outcomes. Through
exploratory pilot experiments, distributional preference measures were found to be related to the
materialistic trust decision in the game but not to the social trust decision in the scenario. These
findings can shed light on the validity issue of trust measures in future research. https://uiowa.edu/crisp/prior-issues-covered},
	journal = {Current Research in Social Psychology},
	author = {Kim, Bora},
	month = sep,
	year = {2018},
	pages = {39--54},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/PE9Q9N3V/Kim - 2018 - Relation between attitudinal trust and behavioral .pdf:application/pdf},
}

@article{ahmed_relationship_2009,
	title = {The {Relationship} between {Behavioral} and {Attitudinal} {Trust}: {A} {Cross}-cultural {Study}},
	volume = {67},
	issn = {0034-6764},
	shorttitle = {The {Relationship} between {Behavioral} and {Attitudinal} {Trust}},
	url = {https://www.jstor.org/stable/41288480},
	abstract = {We study the relationship between trust in an experiment and trust measured by means of popular survey items in different countries. Students from Chile, Colombia, India, Mexico and Sweden participate in a public goods game experiment and answer a set of standard attitudinal survey questions about trust. We find that behavioral trust and attitudinal trust significantly differ among countries. Behavioral trust is highest in Sweden, followed by Latin America, and lowest in India. Attitudinal trust is highest in Chile and Sweden, followed by India and Mexico, and lowest in Colombia. Further, the predictive power of survey items also differs among countries. Trust measured by survey items is significantly related to behavioral trust in some but not in all societies. No single survey item predicts actual trust across all countries. Plausible explanations of the inconsistent relationship between behavioral and attitudinal trust across countries are discussed.},
	number = {4},
	urldate = {2022-12-06},
	journal = {Review of Social Economy},
	author = {Ahmed, Ali M. and Salas, Osvaldo},
	year = {2009},
	note = {Publisher: Taylor \& Francis, Ltd.},
	pages = {457--482},
	file = {JSTOR Full Text PDF:/Users/neilnatarajan/Zotero/storage/EYD3TP56/Ahmed and Salas - 2009 - The Relationship between Behavioral and Attitudina.pdf:application/pdf},
}

@misc{noauthor_side_nodate,
	title = {Side effects project idea},
	url = {https://docs.google.com/document/d/1wy_ObScI1az0R3gZ4u1Y_XVJno2opj84-Egu8PHVOuI/edit?usp=embed_facebook},
	abstract = {Preventing catastrophe by avoiding unintended consequences   Joshua D Greene, Nikola Jurkovic, Neil Natarajan, Xavier Roberts-Gaal  Background  We want to build intelligent systems that can do a range of tasks well. But, as these systems get more capable of doing tasks, they become increasingly ...},
	language = {en},
	urldate = {2023-01-09},
	journal = {Google Docs},
}

@misc{destefano_why_2022,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Why {Providing} {Humans} with {Interpretable} {Algorithms} {May}, {Counterintuitively}, {Lead} to {Lower} {Decision}-making {Performance}},
	url = {https://papers.ssrn.com/abstract=4246077},
	doi = {10.2139/ssrn.4246077},
	abstract = {How is algorithmic model interpretability related to human acceptance of algorithmic recommendations and performance on decision-making tasks? We explored these questions in a multi-method field study of a large multinational fashion organization. We first conducted a quantitative field experiment to compare the use of two models—an interpretable versus an uninterpretable algorithmic model— designed to assist employees with decision making around how many products to send to each of its stores. Contrary to what the literature on interpretable algorithms would lead us to expect, under conditions of high perceived uncertainty, decision makers’ use of an uninterpretable algorithmic model was associated with higher acceptance of algorithmic recommendations and higher task performance than was their use of an interpretable algorithmic model with a similar level of performance. We next investigated this puzzling result using 31 interviews with 14 employees—2 algorithm developers, 2 managers, and 10 decision makers. We advance two concepts that suggest a refinement of theory on interpretable algorithms. First, overconfident troubleshooting—a decision maker rejecting a recommendation coming from an interpretable algorithm, because of their belief that they understand the inner workings of complex processes better than they actually do. Second, social proofing the algorithm—including respected peers in the algorithm development and testing process—may make it more likely that decision makers accept recommendations coming from an uninterpretable algorithm in situations characterized by high perceived uncertainty, because the decision makers may seek to reduce their uncertainty by incorporating the opinions of people with their own knowledge base and experience.},
	language = {en},
	urldate = {2023-01-16},
	author = {DeStefano, Timothy and Kellogg, Katherine and Menietti, Michael and Vendraminelli, Luca},
	month = oct,
	year = {2022},
	keywords = {Artificial Intelligence, AI Adoption, AI and Strategy, Algorithm Aversion, Firm Productivity, Human-in-the-loop Decision Making, Interpretable AI, Machine Learning},
	file = {Submitted Version:/Users/neilnatarajan/Zotero/storage/JLDXXTQ7/DeStefano et al. - 2022 - Why Providing Humans with Interpretable Algorithms.pdf:application/pdf},
}

@article{green_principles_2019,
	title = {The {Principles} and {Limits} of {Algorithm}-in-the-{Loop} {Decision} {Making}},
	volume = {3},
	issn = {2573-0142},
	url = {https://dl.acm.org/doi/10.1145/3359152},
	doi = {10.1145/3359152},
	abstract = {The rise of machine learning has fundamentally altered decision making: rather than being made solely by people, many important decisions are now made through an "algorithm-in-the-loop'' process where machine learning models inform people. Yet insufficient research has considered how the interactions between people and models actually influence human decisions. Society lacks both clear normative principles regarding how people should collaborate with algorithms as well as robust empirical evidence about how people do collaborate with algorithms. Given research suggesting that people struggle to interpret machine learning models and to incorporate them into their decisions---sometimes leading these models to produce unexpected outcomes---it is essential to consider how different ways of presenting models and structuring human-algorithm interactions affect the quality and type of decisions made. This paper contributes to such research in two ways. First, we posited three principles as essential to ethical and responsible algorithm-in-the-loop decision making. Second, through a controlled experimental study on Amazon Mechanical Turk, we evaluated whether people satisfy these principles when making predictions with the aid of a risk assessment. We studied human predictions in two contexts (pretrial release and financial lending) and under several conditions for risk assessment presentation and structure. Although these conditions did influence participant behaviors and in some cases improved performance, only one desideratum was consistently satisfied. Under all conditions, our study participants 1) were unable to effectively evaluate the accuracy of their own or the risk assessment's predictions, 2) did not calibrate their reliance on the risk assessment based on the risk assessment's performance, and 3) exhibited bias in their interactions with the risk assessment. These results highlight the urgent need to expand our analyses of algorithmic decision making aids beyond evaluating the models themselves to investigating the full sociotechnical contexts in which people and algorithms interact.},
	language = {en},
	number = {CSCW},
	urldate = {2023-01-25},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Green, Ben and Chen, Yiling},
	month = nov,
	year = {2019},
	pages = {1--24},
	file = {Green and Chen - 2019 - The Principles and Limits of Algorithm-in-the-Loop.pdf:/Users/neilnatarajan/Zotero/storage/UPPYMVGI/Green and Chen - 2019 - The Principles and Limits of Algorithm-in-the-Loop.pdf:application/pdf},
}

@inproceedings{zhang_effect_2020,
	address = {Barcelona Spain},
	title = {Effect of confidence and explanation on accuracy and trust calibration in {AI}-assisted decision making},
	isbn = {978-1-4503-6936-7},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372852},
	doi = {10.1145/3351095.3372852},
	abstract = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the signiicance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model’s to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-speciic model information can calibrate trust and improve the joint performance of the human and AI. Speciically, we study the efect of showing conidence score and local explanation for a particular prediction. Through two human experiments, we show that conidence score can help calibrate people’s trust in an AI model, but trust calibration alone is not suicient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI’s errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
	language = {en},
	urldate = {2023-01-25},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
	month = jan,
	year = {2020},
	pages = {295--305},
	file = {Zhang et al. - 2020 - Effect of confidence and explanation on accuracy a.pdf:/Users/neilnatarajan/Zotero/storage/USWMYA2K/Zhang et al. - 2020 - Effect of confidence and explanation on accuracy a.pdf:application/pdf},
}

@article{page_difference_2007,
	title = {The {Difference}: {How} the {Power} of {Diversity} {Creates} {Better} {Groups}, {Firms}, {Schools}, and {Societies}},
	abstract = {In this landmark book, Scott Page redefines the way we understand ourselves in relation to one another. The Difference is about how we think in groups--and how our collective wisdom exceeds the sum of its parts. Why can teams of people find better solutions than brilliant individuals working alone? And why are the best group decisions and predictions those that draw upon the very qualities that make each of us unique? The answers lie in diversity--not what we look like outside, but what we look like within, our distinct tools and abilities.The Difference reveals that progress and innovation may depend less on lone thinkers with enormous IQs than on diverse people working together and capitalizing on their individuality. Page shows how groups that display a range of perspectives outperform groups of like-minded experts. Diversity yields superior outcomes, and Page proves it using his own cutting-edge research. Moving beyond the politics that cloud standard debates about diversity, he explains why difference beats out homogeneity, whether you're talking about citizens in a democracy or scientists in the laboratory. He examines practical ways to apply diversity's logic to a host of problems, and along the way offers fascinating and surprising examples, from the redesign of the Chicago "El" to the truth about where we store our ketchup.Page changes the way we understand diversity--how to harness its untapped potential, how to understand and avoid its traps, and how we can leverage our differences for the benefit of all.},
	author = {Page, Scott E.},
	month = jan,
	year = {2007},
	note = {MAG ID: 1927360298},
}

@article{page_diversity_2010,
	title = {Diversity and {Complexity}},
	abstract = {This book provides an introduction to the role of diversity in complex adaptive systems. A complex system--such as an economy or a tropical ecosystem--consists of interacting adaptive entities that produce dynamic patterns and structures. Diversity plays a different role in a complex system than it does in an equilibrium system, where it often merely produces variation around the mean for performance measures. In complex adaptive systems, diversity makes fundamental contributions to system performance. Scott Page gives a concise primer on how diversity happens, how it is maintained, and how it affects complex systems. He explains how diversity underpins system level robustness, allowing for multiple responses to external shocks and internal adaptations; how it provides the seeds for large events by creating outliers that fuel tipping points; and how it drives novelty and innovation. Page looks at the different kinds of diversity--variations within and across types, and distinct community compositions and interaction structures--and covers the evolution of diversity within complex systems and the factors that determine the amount of maintained diversity within a system.Provides a concise and accessible introduction Shows how diversity underpins robustness and fuels tipping points Covers all types of diversity The essential primer on diversity in complex adaptive systems},
	author = {Page, Scott E.},
	month = nov,
	year = {2010},
	note = {MAG ID: 1653902796},
}

@article{schmidt_validity_1998,
	title = {The validity and utility of selection methods in personnel psychology: {Practical} and theoretical implications of 85 years of research findings.},
	volume = {124},
	doi = {10.1037/0033-2909.124.2.262},
	abstract = {This article summarizes the practical and theoretical implications of 85 years of research in personnel selection. On the basis of meta-analytic findings, this article presents the validity of 19 selection procedures for predicting job performance and training performance and the validity of paired combinations of general mental ability (GMA) and Ihe 18 other selection procedures. Overall, the 3 combinations with the highest multivariate validity and utility for job performance were GMA plus a work sample test (mean validity of .63), GMA plus an integrity test (mean validity of .65), and GMA plus a structured interview (mean validity of .63). A further advantage of the latter 2 combinations is that they can be used for both entry level selection and selection of experienced employees. The practical utility implications of these summary findings are substantial. The implications of these research findings for the development of theories of job performance are discussed. From the point of view of practical value, the most important property of a personnel assessment method is predictive validity: the ability to predict future job performance, job-related learning (such as amount of learning in training and development programs), and other criteria. The predictive validity coefficient is directly proportional to the practical economic value (utility) of the assessment method (Brogden, 1949; Schmidt, Hunter, McKenzie, \& Muldrow, 1979). Use of hiring methods with increased predictive validity leads to substantial increases in employee performance as measured in percentage increases in output, increased monetary value of output, and increased learning of job-related skills (Hunter, Schmidt, \& Judiesch, 1990). Today, the validity of different personnel measures can be determined with the aid of 85 years of research. The most wellknown conclusion from this research is that for hiring employees without previous experience in the job the most valid predictor of future performance and learning is general mental ability ([GMA], i.e., intelligence or general cognitive ability; Hunter \& Hunter, 1984; Ree \& Earles, 1992). GMA can be measured using commercially available tests. However, many other measures can also contribute to the overall validity of the selection process. These include, for example, measures of},
	number = {2},
	journal = {Psychological Bulletin},
	author = {Schmidt, Frank L. and Hunter, John E.},
	month = jan,
	year = {1998},
	doi = {10.1037/0033-2909.124.2.262},
	note = {MAG ID: 2136971664},
	pages = {262--274},
}

@article{kahneman_thinking_2011,
	title = {Thinking, {Fast} and {Slow}},
	abstract = {Daniel Kahneman, recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology challenging the rational model of judgment and decision making, is one of the world's most important thinkers. His ideas have had a profound impact on many fields - including business, medicine, and politics - but until now, he has never brought together his many years of research in one book. In "Thinking, Fast and Slow", Kahneman takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think and make choices. One system is fast, intuitive, and emotional; the other is slower, more deliberative, and more logical. Kahneman exposes the extraordinary capabilities - and also the faults and biases - of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behaviour. The importance of properly framing risks, the effects of cognitive biases on how we view others, the dangers of prediction, the right ways to develop skills, the pros and cons of fear and optimism, the difference between our experience and memory of events, the real components of happiness - each of these can be understood only by knowing how the two systems work together to shape our judgments and decisions. Drawing on a lifetime's experimental experience, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our professional and our personal lives-and how we can use different techniques to guard against the mental glitches that often get us into trouble. "Thinking, Fast and Slow" will transform the way you take decisions and experience the world.},
	author = {Kahneman, Daniel},
	month = jan,
	year = {2011},
	note = {MAG ID: 2752099845},
}

@article{highhouse_assessing_2002,
	title = {{ASSESSING} {THE} {CANDIDATE} {AS} {A} {WHOLE}: {A} {HISTORICAL} {AND} {CRITICAL} {ANALYSIS} {OF} {INDIVIDUAL} {PSYCHOLOGICAL} {ASSESSMENT} {FOR} {PERSONNEL} {DECISION} {MAKING}},
	volume = {55},
	doi = {10.1111/j.1744-6570.2002.tb00114.x},
	abstract = {Although individual assessment is a thriving area of professional practice in industry, it receives little, if any, attention from textbooks on industrial psychology or personnel management. This article is an attempt to establish individual assessment's place in the history of personnel selection, and to examine why the practice has survived despite receiving little attention in research and graduate training. It is argued that the clinical, holistic approach that has characterized individual-assessment practice has survived primarily because the “elementalistic” testing approach, focusing on traits and abilities, has often been dismissed as inadequate for addressing the complexities of the executive profile. Moreover, public displeasure with standard paper-and-pencil testing in the 1960s and 1970s made the holistic approach to assessment an attractive, alternative. The article contrasts individual assessment practice with the current state of knowledge on psychological assessment and personnel decision making. Like psychotherapy in the 1950s, individual psychological assessment appears to have achieved the status of functional autonomy within psychology.},
	number = {2},
	journal = {Personnel Psychology},
	author = {Highhouse, Scott},
	month = jun,
	year = {2002},
	doi = {10.1111/j.1744-6570.2002.tb00114.x},
	note = {MAG ID: 2063757465},
	pages = {363--396},
}

@article{highhouse_stubborn_2008,
	title = {Stubborn reliance on intuition and subjectivity in employee selection.},
	volume = {1},
	doi = {10.1111/j.1754-9434.2008.00058.x},
	abstract = {The focus of this article is on implicit beliefs that inhibit adoption of selection decision aids (e.g., paper-and-pencil tests, structured interviews, mechanical combination of predictors). Understanding these beliefs is just as important as understanding organizational constraints to the adoption of selection technologies and may be more useful for informing the design of successful interventions. One of these is the implicit belief that it is theoretically possible to achieve near-perfect precision in predicting performance on the job. That is, people have an inherent resistance to analytical approaches to selection because they fail to view selection as probabilistic and subject to error. Another is the implicit belief that prediction of human behavior is improved through experience. This myth of expertise results in an overreliance on intuition and a reluctance to undermine one’s own credibility by using a selection decision aid.},
	number = {3},
	journal = {Industrial and Organizational Psychology},
	author = {Highhouse, Scott},
	month = sep,
	year = {2008},
	doi = {10.1111/j.1754-9434.2008.00058.x},
	note = {MAG ID: 2171321543},
	pages = {333--342},
}

@article{page_diversity_2017,
	title = {The {Diversity} {Bonus}: {How} {Great} {Teams} {Pay} {Off} in the {Knowledge} {Economy}},
	abstract = {How businesses and other organizations can improve their performance by tapping the power of differences in how people think What if workforce diversity is more than simply the right thing to do in order to make society more integrated and just? What if diversity can also improve the bottom line of businesses and other organizations facing complex challenges in the knowledge economy? It can. And The Diversity Bonus shows how and why. Scott Page, a leading thinker, writer, and speaker whose ideas and advice are sought after by corporations, nonprofits, universities, and governments around the world, makes a clear and compellingly pragmatic case for diversity and inclusion. He presents overwhelming evidence that teams that include different kinds of thinkers outperform homogenous groups on complex tasks, producing what he calls diversity bonuses. These bonuses include improved problem solving, increased innovation, and more accurate predictions all of which lead to better performance and results. Page shows that various types of cognitive diversity differences in how people perceive, encode, analyze, and organize the same information and experiencesare linked to better outcomes. He then describes how these cognitive differences are influenced by other kinds of diversity, including racial and gender differencesin other words, identity diversity. Identity diversity, therefore, can also produce bonuses. Drawing on research in economics, psychology, computer science, and many other fields, The Diversity Bonus also tells the stories of people and organizations that have tapped the power of diversity to solve complex problems. And the book includes a challenging response from Katherine Phillips of the Columbia Business School. The result changes the way we think about diversity in the workplaceand far beyond it.},
	author = {Page, Scott E. and Lewis, Earl and Cantor, Nancy},
	month = sep,
	year = {2017},
	note = {MAG ID: 2885284796
S2ID: e47d67055846c132fc8274170fe9adf1250a7727},
}

@article{kuncel_mechanical_2013,
	title = {Mechanical versus clinical data combination in selection and admissions decisions: a meta-analysis.},
	volume = {98},
	doi = {10.1037/a0034156},
	abstract = {In employee selection and academic admission decisions, holistic (clinical) data combination methods continue to be relied upon and preferred by practitioners in our field. This meta-analysis examined and compared the relative predictive power of mechanical methods versus holistic methods in predicting multiple work (advancement, supervisory ratings of performance, and training performance) and academic (grade point average) criteria. There was consistent and substantial loss of validity when data were combined holistically--even by experts who are knowledgeable about the jobs and organizations in question--across multiple criteria in work and academic settings. In predicting job performance, the difference between the validity of mechanical and holistic data combination methods translated into an improvement in prediction of more than 50\%. Implications for evidence-based practice are discussed.},
	number = {6},
	journal = {Journal of Applied Psychology},
	author = {Kuncel, Nathan R. and Klieger, David M. and Connelly, Brian S. and Ones, Deniz S.},
	month = sep,
	year = {2013},
	doi = {10.1037/a0034156},
	pmid = {24041118},
	note = {MAG ID: 1989277811},
	pages = {1060--1072},
}

@article{wang_are_2021,
	title = {Are {Explanations} {Helpful}? {A} {Comparative} {Study} of the {Effects} of {Explanations} in {AI}-{Assisted} {Decision}-{Making}},
	doi = {10.1145/3397481.3450650},
	abstract = {This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy—improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.},
	author = {Wang, Xinru and Yin, Ming},
	month = apr,
	year = {2021},
	doi = {10.1145/3397481.3450650},
	note = {MAG ID: 3156106752},
	pages = {318--328},
}

@article{vivian_lai_why_2020,
	title = {"{Why} is '{Chicago}' deceptive?" {Towards} {Building} {Model}-{Driven} {Tutorials} for {Humans}},
	doi = {10.1145/10.1145/3313831.3376873},
	abstract = {To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a training phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically selected examples from training data with explanations. We use deceptive review detection as a testbed and conduct large-scale, randomized human-subject experiments to examine the effectiveness of such tutorials. We find that tutorials indeed improve human performance, with and without real-time assistance. In particular, although deep learning provides superior predictive performance than simple models, tutorials and explanations from simple models are more useful to humans. Our work suggests future directions for human-centered tutorials and explanations towards a synergy between humans and AI.},
	journal = {arXiv: Human-Computer Interaction},
	author = {{Vivian Lai} and Lai, Vivian and Liu, Han and Tan, Chenhao},
	month = jan,
	year = {2020},
	doi = {10.1145/10.1145/3313831.3376873},
	note = {MAG ID: 2999438821},
}

@article{budescu_how_2012,
	title = {How to measure diversity when you must.},
	volume = {17},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0027129},
	doi = {10.1037/a0027129},
	abstract = {Racial/ethnic diversity has become an increasingly important variable in the social sciences. Research from multiple disciplines consistently demonstrates the tremendous impact of ethnic diversity on individuals and organizations. Investigators use a variety of measures, and their choices can affect the conclusions that can be drawn and limit the ability to compare and generalize results across studies effectively. The current article reviews 3 popular approaches to the measurement of diversity: the simplistic majority-minority approach and 2 multiple categories variants, the generalized variance and the lesser used entropy statistic. We discuss the properties of each approach and reject the majority-minority approach. We provide 5 examples using the generalized variance and entropy statistics and illustrate their versatility and flexibility. We urge investigators to adopt these multicategory measures and to use our discussion to determine which measure of diversity is most appropriate given the nature of one's data set and research question.},
	language = {en},
	number = {2},
	urldate = {2023-02-14},
	journal = {Psychological Methods},
	author = {Budescu, David V. and Budescu, Mia},
	year = {2012},
	pages = {215--227},
}

@article{gillet_diversity_2011,
	title = {Diversity selection algorithms},
	volume = {1},
	issn = {1759-0884},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.33},
	doi = {10.1002/wcms.33},
	abstract = {Molecular diversity has been an important topic in chemoinformatics for the last two decades following the introduction of high-throughput screening and combinatorial chemistry techniques. This article reviews the main algorithms that have been developed for assessing the diversity of a set of compounds and for selecting a diverse subset of compounds from a larger library. Particular focus is given to recent trends including the use of scaffolds as a way of assessing molecular diversity and the importance now given to optimizing multiple properties simultaneously in an attempt to reduce late stage attrition during the drug development stage of drug discovery. © 2011 John Wiley \& Sons, Ltd. WIREs Comput Mol Sci 2011 1 580-589 DOI: 10.1002/wcms.33 This article is categorized under: Computer and Information Science {\textgreater} Chemoinformatics},
	language = {en},
	number = {4},
	urldate = {2023-02-14},
	journal = {WIREs Computational Molecular Science},
	author = {Gillet, Valerie J.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcms.33},
	pages = {580--589},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/8UG78XGU/Gillet - 2011 - Diversity selection algorithms.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/BQZF3I82/wcms.html:text/html},
}

@misc{dwork_fairness_2011,
	title = {Fairness {Through} {Awareness}},
	url = {http://arxiv.org/abs/1104.3913},
	doi = {10.48550/arXiv.1104.3913},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Rich},
	month = nov,
	year = {2011},
	note = {arXiv:1104.3913 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Computational Complexity},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/YZET5NJ4/Dwork et al. - 2011 - Fairness Through Awareness.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/T5HCTR8X/1104.html:text/html},
}

@incollection{bordeaux_submodular_2014,
	edition = {1},
	title = {Submodular {Function} {Maximization}},
	isbn = {978-1-107-02519-6 978-1-139-17780-1},
	url = {https://www.cambridge.org/core/product/identifier/CBO9781139177801A031/type/book_part},
	language = {en},
	urldate = {2023-02-15},
	booktitle = {Tractability},
	publisher = {Cambridge University Press},
	author = {Krause, Andreas and Golovin, Daniel},
	editor = {Bordeaux, Lucas and Hamadi, Youssef and Kohli, Pushmeet},
	month = feb,
	year = {2014},
	doi = {10.1017/CBO9781139177801.004},
	pages = {71--104},
	file = {Krause and Golovin - 2014 - Submodular Function Maximization.pdf:/Users/neilnatarajan/Zotero/storage/2NQBNXC9/Krause and Golovin - 2014 - Submodular Function Maximization.pdf:application/pdf},
}

@article{nemhauser_analysis_1978,
	title = {An analysis of approximations for maximizing submodular set functions—{I}},
	volume = {14},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/BF01588971},
	doi = {10.1007/BF01588971},
	abstract = {LetN be a finite set andz be a real-valued function defined on the set of subsets ofN that satisfies z(S)+z(T)≥z(S⋃T)+z(S⋂T) for allS, T inN. Such a function is called submodular. We consider the problem maxS⊂N\{a(S):{\textbar}S{\textbar}≤K,z(S) submodular\}.Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more thanK colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem.We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, whenz(S) is nondecreasing andz(0) = 0, we show that a “greedy” heuristic always produces a solution whose value is at least 1 −[(K − 1)/K]K times the optimal value. This bound can be achieved for eachK and has a limiting value of (e − 1)/e, where e is the base of the natural logarithm.},
	language = {en},
	number = {1},
	urldate = {2023-02-15},
	journal = {Mathematical Programming},
	author = {Nemhauser, G. L. and Wolsey, L. A. and Fisher, M. L.},
	month = dec,
	year = {1978},
	pages = {265--294},
}

@misc{noauthor_doing_nodate,
	title = {Doing {Design} {Ethnography} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/book/10.1007/978-1-4471-2726-0},
	urldate = {2023-02-24},
}

@misc{noauthor_contextual_nodate,
	title = {Contextual {Design} - an overview {\textbar} {ScienceDirect} {Topics}},
	url = {https://www.sciencedirect.com/topics/computer-science/contextual-design},
	urldate = {2023-02-24},
	file = {Contextual Design - an overview | ScienceDirect Topics:/Users/neilnatarajan/Zotero/storage/T8DUDCVS/contextual-design.html:text/html},
}

@misc{noauthor_thematic_nodate,
	title = {Thematic {Analysis} {\textbar} {SAGE} {Publications} {Ltd}},
	url = {https://uk.sagepub.com/en-gb/eur/thematic-analysis/book248481},
	urldate = {2023-02-24},
	file = {Thematic Analysis | SAGE Publications Ltd:/Users/neilnatarajan/Zotero/storage/NEKDU69G/book248481.html:text/html},
}

@incollection{lazar_chapter_2017,
	address = {Boston},
	title = {Chapter 9 - {Ethnography}},
	isbn = {978-0-12-805390-4},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128053904000091},
	abstract = {In-depth, longitudinal observation of complex contexts is often necessary for understanding systems requirements or the impact of systems in use. Ethnography refers to the use of in-depth observation, and often participation, of a human group, culture, or context, with the goal of developing a rich description of activities, interactions, beliefs, roles, and goals. Ethnographic research in human-computer interaction (HCI) is particularly useful for understanding environment where stakeholders interact to complete complex tasks involving the need for coordination and exchange of information. This chapter discusses the selection of sites, identification of research roles, practical issues for ethnographic studies, and processes involved in collecting and analyzing ethnographic data. Examples of ethnographic HCI studies in home, work, education, and other settings are provided.},
	language = {en},
	urldate = {2023-03-13},
	booktitle = {Research {Methods} in {Human} {Computer} {Interaction} ({Second} {Edition})},
	publisher = {Morgan Kaufmann},
	author = {Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
	editor = {Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
	month = jan,
	year = {2017},
	doi = {10.1016/B978-0-12-805390-4.00009-1},
	keywords = {Ethnography, Informants, Observation, Participation, Triangulation},
	pages = {229--261},
	file = {ScienceDirect Full Text PDF:/Users/neilnatarajan/Zotero/storage/B88RKS6Z/Lazar et al. - 2017 - Chapter 9 - Ethnography.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/ECYJCGI4/research-methods-in-human-computer-interaction.html:text/html},
}

@book{jacko_human_2012,
	address = {Baton Rouge, UNITED STATES},
	title = {Human {Computer} {Interaction} {Handbook}: {Fundamentals}, {Evolving} {Technologies}, and {Emerging} {Applications}, {Third} {Edition}},
	isbn = {978-1-4398-2944-8},
	shorttitle = {Human {Computer} {Interaction} {Handbook}},
	url = {http://ebookcentral.proquest.com/lib/oxford/detail.action?docID=911990},
	urldate = {2023-03-13},
	publisher = {Taylor \& Francis Group},
	author = {Jacko, Julie A.},
	year = {2012},
	keywords = {etc., Human-computer interaction -- Handbooks, manuals},
	file = {ProQuest Ebook Snapshot:/Users/neilnatarajan/Zotero/storage/FCEZFK2S/reader.html:text/html},
}

@misc{lum_closer_2021,
	title = {Closer than they appear: {A} {Bayesian} perspective on individual-level heterogeneity in risk assessment},
	shorttitle = {Closer than they appear},
	url = {http://arxiv.org/abs/2102.01135},
	abstract = {Risk assessment instruments are used across the criminal justice system to estimate the probability of some future behavior given covariates. The estimated probabilities are then used in making decisions at the individual level. In the past, there has been controversy about whether the probabilities derived from group-level calculations can meaningfully be applied to individuals. Using Bayesian hierarchical models applied to a large longitudinal dataset from the court system in the state of Kentucky, we analyze variation in individual-level probabilities of failing to appear for court and the extent to which it is captured by covariates. We ﬁnd that individuals within the same risk group vary widely in their probability of the outcome. In practice, this means that allocating individuals to risk groups based on standard approaches to risk assessment, in large part, results in creating distinctions among individuals who are not meaningfully different in terms of their likelihood of the outcome. This is because uncertainty about the probability that any particular individual will fail to appear is large relative to the difference in average probabilities among any reasonable set of risk groups.},
	language = {en},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Lum, Kristian and Dunson, David B. and Johndrow, James},
	month = feb,
	year = {2021},
	note = {arXiv:2102.01135 [stat]},
	keywords = {Statistics - Applications},
	file = {Lum et al. - 2021 - Closer than they appear A Bayesian perspective on.pdf:/Users/neilnatarajan/Zotero/storage/9Q8JR2KP/Lum et al. - 2021 - Closer than they appear A Bayesian perspective on.pdf:application/pdf},
}

@article{koivunen_pitfalls_2023,
	title = {Pitfalls and {Tensions} in {Digitalizing} {Talent} {Acquisition}: {An} {Analysis} of {HRM} {Professionals}’ {Considerations} {Related} to {Digital} {Ethics}},
	issn = {1873-7951},
	shorttitle = {Pitfalls and {Tensions} in {Digitalizing} {Talent} {Acquisition}},
	url = {https://doi.org/10.1093/iwc/iwad018},
	doi = {10.1093/iwc/iwad018},
	abstract = {The practices of organizational talent acquisition are rapidly transforming as a result of the proliferation of information systems that support decision-making, ranging from applicant tracking systems to recruitment chatbots. As part of human resource management (HRM), talent acquisition covers recruitment and team-assembly activities and is allegedly in dire need for digital aid. We analyze the pitfalls and tensions of digitalization in this area through a lens that builds on the interdisciplinary literature related to digital ethics. Using three relevant landmark papers, we analyzed qualitative data from 47 interviews of HRM professionals in Finland, including team-assembly facilitators and recruitment experts. The analysis highlights 14 potential tensions and pitfalls, such as the tension between requesting detailed data versus respecting privacy and the pitfall of unequal treatment across application channels. We identify that the values of autonomy, fairness and utility are often especially at risk of being compromised. We discuss the tendency of the binary considerations related to human and automated decision making, and the reasons for the incompatibility between current digital systems and organizations’ needs for talent acquisition.},
	urldate = {2023-03-18},
	journal = {Interacting with Computers},
	author = {Koivunen, Sami and Sahlgren, Otto and Ala-Luopa, Saara and Olsson, Thomas},
	month = mar,
	year = {2023},
	pages = {iwad018},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/KYB6DGCA/Koivunen et al. - 2023 - Pitfalls and Tensions in Digitalizing Talent Acqui.pdf:application/pdf},
}

@article{dawid_individual_2017,
	title = {On {Individual} {Risk}},
	volume = {194},
	issn = {0039-7857, 1573-0964},
	url = {http://arxiv.org/abs/1406.5540},
	doi = {10.1007/s11229-015-0953-4},
	abstract = {We survey a variety of possible explications of the term "Individual Risk." These in turn are based on a variety of interpretations of "Probability," including Classical, Enumerative, Frequency, Formal, Metaphysical, Personal, Propensity, Chance and Logical conceptions of Probability, which we review and compare. We distinguish between "groupist" and "individualist" understandings of Probability, and explore both "group to individual" (G2i) and "individual to group" (i2G) approaches to characterising Individual Risk. Although in the end that concept remains subtle and elusive, some pragmatic suggestions for progress are made.},
	number = {9},
	urldate = {2023-03-20},
	journal = {Synthese},
	author = {Dawid, A. Philip},
	month = sep,
	year = {2017},
	note = {arXiv:1406.5540 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications, 62A01},
	pages = {3445--3474},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/NEFVMUDE/Dawid - 2017 - On Individual Risk.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/I76KSLY2/1406.html:text/html},
}

@misc{experience_diary_nodate,
	title = {Diary {Studies}: {Understanding} {Long}-{Term} {User} {Behavior} and {Experiences}},
	shorttitle = {Diary {Studies}},
	url = {https://www.nngroup.com/articles/diary-studies/},
	abstract = {User logs (diaries) of daily activities as they occur give contextual insights about real-time user behaviors and needs, helping define UX feature requirements.},
	language = {en},
	urldate = {2023-03-31},
	journal = {Nielsen Norman Group},
	author = {Experience, World Leaders in Research-Based User},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/H484KAID/diary-studies.html:text/html},
}

@misc{liu_deid-gpt_2023,
	title = {{DeID}-{GPT}: {Zero}-shot {Medical} {Text} {De}-{Identification} by {GPT}-4},
	shorttitle = {{DeID}-{GPT}},
	url = {http://arxiv.org/abs/2303.11032},
	doi = {10.48550/arXiv.2303.11032},
	abstract = {The digitization of healthcare has facilitated the sharing and re-using of medical data but has also raised concerns about confidentiality and privacy. HIPAA (Health Insurance Portability and Accountability Act) mandates removing re-identifying information before the dissemination of medical records. Thus, effective and efficient solutions for de-identifying medical data, especially those in free-text forms, are highly needed. While various computer-assisted de-identification methods, including both rule-based and learning-based, have been developed and used in prior practice, such solutions still lack generalizability or need to be fine-tuned according to different scenarios, significantly imposing restrictions in wider use. The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework ("DeID-GPT") to automatically identify and remove the identifying information. Compared to existing commonly used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original structure and meaning of the text. This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, which provides insights for further research and solution development on the use of LLMs such as ChatGPT/GPT-4 in healthcare. Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.},
	urldate = {2023-04-05},
	publisher = {arXiv},
	author = {Liu, Zhengliang and Yu, Xiaowei and Zhang, Lu and Wu, Zihao and Cao, Chao and Dai, Haixing and Zhao, Lin and Liu, Wei and Shen, Dinggang and Li, Quanzheng and Liu, Tianming and Zhu, Dajiang and Li, Xiang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11032 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language, \_tablet},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/KSFDDDXM/2303.html:text/html;Liu et al_2023_DeID-GPT.pdf:/Users/neilnatarajan/Zotero/storage/292WR3K5/Liu et al_2023_DeID-GPT.pdf:application/pdf},
}

@inproceedings{viswanathan_unlockme_2021,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '21},
	title = {{UnlockMe}: {Social} {Interactions} when {Co}-located in {Online} {Activities}},
	isbn = {978-1-4503-8095-9},
	shorttitle = {{UnlockMe}},
	url = {https://dl.acm.org/doi/10.1145/3411763.3451757},
	doi = {10.1145/3411763.3451757},
	abstract = {All the activities that we do online, by either preference or obligation, deprive us of social interactions especially the impromptu ones. We present UnlockMe, a concept that aims at preserving the social link, the disposition to come across other people serendipitously when engaged in online activities such as purchasing goods, working from home or when viewing media and entertainment. Our concept relies on virtual co-location detection (both synchronous and asynchronous), to allow users to engage with other people with whom they would have been likely to interact when doing the same activities offline in the physical world. We developed and illustrated UnlockMe with six scenarios and low-fidelity prototyping to test it with 10 participants who were isolated due to the coronavirus disease (COVID-19) pandemic. Our findings reveal multimedia recommendations from close social connections to be the best scenario for UnlockMe followed by online shopping and connecting with the local community.},
	urldate = {2023-04-05},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Viswanathan, Sruthi and Legras, Christophe},
	month = may,
	year = {2021},
	keywords = {Collaborative Decision Making, Network Segregation, Recommendations, Serendipitous Messaging, Social Connections, Virtual Co-location},
	pages = {1--7},
	file = {Viswanathan_Legras_2021_UnlockMe_annotated.pdf:/Users/neilnatarajan/Zotero/storage/IGZIP3NB/Viswanathan_Legras_2021_UnlockMe_annotated.pdf:application/pdf;Viswanathan_Legras_2021_UnlockMe.pdf:/Users/neilnatarajan/Zotero/storage/IGZIP3NB/Viswanathan_Legras_2021_UnlockMe.pdf:application/pdf},
}

@inproceedings{viswanathan_situational_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {Situational {Recommender}: {Are} {You} {On} the {Spot}, {Refining} {Plans}, or {Just} {Bored}?},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Situational {Recommender}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501909},
	doi = {10.1145/3491102.3501909},
	abstract = {When people engage in urban exploration, the tool they are most likely to use today is a mobile phone. In this paper, we present observations of users’ “home” and “away” conducted to refine our understanding of situational Point-of-Interest (POI) needs. Our findings suggest three distinct categories of situations in which users seek POI information: On-the-spot, Refining plans, and Moments of boredom. Based on the similarities and differences of these three situations in five observed underlying constraints – distance of interest, engagement threshold, ambiguity of the search, profile matching, and other imperative constraints, we derive implications for designing and ranking POIs for a Situational Recommender. To further access our concept, we designed and prototyped Situational Recommender by providing an interactional representation of the situation, and ran a Wizard-of-Oz concept validation study. Our results suggest that participants understood the concept without much effort and appreciated its usefulness.},
	urldate = {2023-04-05},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Viswanathan, Sruthi and Boulard, Cecile and Bruyat, Adrien and Maria Grasso, Antonietta},
	month = apr,
	year = {2022},
	keywords = {CAI, Interaction Design, Mobile Devices, POI Recommender System, Prototyping, Situation Aware, Situation modelling, User Studies},
	pages = {1--19},
	file = {Viswanathan et al_2022_Situational Recommender_annotated.pdf:/Users/neilnatarajan/Zotero/storage/Y5UBUJJ7/Viswanathan et al_2022_Situational Recommender_annotated.pdf:application/pdf;Viswanathan et al_2022_Situational Recommender.pdf:/Users/neilnatarajan/Zotero/storage/Y5UBUJJ7/Viswanathan et al_2022_Situational Recommender.pdf:application/pdf},
}

@inproceedings{warren_categorical_2023,
	address = {New York, NY, USA},
	series = {{IUI} '23},
	title = {Categorical and {Continuous} {Features} in {Counterfactual} {Explanations} of {AI} {Systems}},
	isbn = {9798400701061},
	url = {https://dl.acm.org/doi/10.1145/3581641.3584090},
	doi = {10.1145/3581641.3584090},
	abstract = {Recently, eXplainable AI (XAI) research has focused on the use of counterfactual explanations to address interpretability, algorithmic recourse, and bias in AI system decision-making. The proponents of these algorithms claim they meet users’ requirements for counterfactual explanations. For instance, many claim that the output of their algorithms work as explanations because they prioritise "plausible", "actionable" or "causally important" features in their generated counterfactuals. However, very few of these claims have been tested in controlled psychological studies, and we know very little about which aspects of counterfactual explanations help users to understand AI system decisions. Furthermore, we do not know whether counterfactual explanations are an advance on more traditional causal explanations that have a much longer history in AI (in explaining expert systems and decision trees). Accordingly, we carried out two user studies to (i) test a fundamental distinction in feature-types, between categorical and continuous features, and (ii) compare the relative effectiveness of counterfactual and causal explanations. The studies used a simulated, automated decision-making app that determined safe driving limits after drinking alcohol, based on predicted blood alcohol content, and user responses were measured objectively (users’ predictive accuracy) and subjectively (users’ satisfaction and trust judgments). Study 1 (N=127) showed that users understand explanations referring to categorical features more readily than those referring to continuous features. It also discovered a dissociation between objective and subjective measures: counterfactual explanations elicited higher accuracy of predictions than no-explanation control descriptions but no higher accuracy than causal explanations, yet counterfactual explanations elicited greater satisfaction and trust judgments than causal explanations. Study 2 (N=211) found that users were more accurate for categorically-transformed features compared to continuous ones, and also replicated the results of Study 1. The findings delineate important boundary conditions for current and future counterfactual explanation methods in XAI.},
	urldate = {2023-04-05},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Warren, Greta and Byrne, Ruth M. J. and Keane, Mark T.},
	month = mar,
	year = {2023},
	keywords = {counterfactual, explanation, user study, XAI},
	pages = {171--187},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/MZFMHMGC/Warren et al. - 2023 - Categorical and Continuous Features in Counterfact.pdf:application/pdf},
}

@misc{noauthor_ai_nodate-1,
	title = {{AI} {Overreliance} {Is} a {Problem}. {Are} {Explanations} a {Solution}?},
	url = {https://hai.stanford.edu/news/ai-overreliance-problem-are-explanations-solution},
	abstract = {Stanford researchers show that shifting the cognitive costs and benefits of engaging with AI explanations could result in fewer erroneous decisions due to AI overreliance.},
	language = {en},
	urldate = {2023-04-05},
	journal = {Stanford HAI},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/5L84WL9L/ai-overreliance-problem-are-explanations-solution.html:text/html},
}

@misc{miller_explainable_2023,
	title = {Explainable {AI} is {Dead}, {Long} {Live} {Explainable} {AI}! {Hypothesis}-driven decision support},
	url = {http://arxiv.org/abs/2302.12389},
	doi = {10.48550/arXiv.2302.12389},
	abstract = {In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.},
	urldate = {2023-04-05},
	publisher = {arXiv},
	author = {Miller, Tim},
	month = mar,
	year = {2023},
	note = {arXiv:2302.12389 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/FDVSR8XQ/2302.html:text/html;Miller_2023_Explainable AI is Dead, Long Live Explainable AI_annotated.pdf:/Users/neilnatarajan/Zotero/storage/5GLPJWA2/Miller_2023_Explainable AI is Dead, Long Live Explainable AI_annotated.pdf:application/pdf;Miller_2023_Explainable AI is Dead, Long Live Explainable AI.pdf:/Users/neilnatarajan/Zotero/storage/5GLPJWA2/Miller_2023_Explainable AI is Dead, Long Live Explainable AI.pdf:application/pdf},
}

@article{zellers_defending_2019,
	title = {Defending {Against} {Neural} {Fake} {News}},
	volume = {32},
	abstract = {Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73\% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92\% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.},
	author = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
	month = may,
	year = {2019},
	note = {MAG ID: 2971008823},
	pages = {9051--9062},
}

@article{ippolito_human_2019,
	title = {Human and {Automatic} {Detection} of {Generated} {Text}.},
	abstract = {With the advent of generative models with a billion parameters or more, it is now possible to automatically generate vast amounts of human-sounding text. This raises questions into just how human-like is the machine-generated text, and how long does a text excerpt need to be for both humans and automatic discriminators to be able reliably detect that it was machine-generated. In this paper, we conduct a thorough investigation of how choices such as sampling strategy and text excerpt length can impact the performance of automatic detection methods as well as human raters. We find that the sampling strategies which result in more human-like text according to human raters create distributional differences from human-written text that make detection easy for automatic discriminators.},
	journal = {arXiv: Computation and Language},
	author = {Ippolito, Daphne and Duckworth, Daniel and Callison-Burch, Chris and Eck, Douglas},
	month = nov,
	year = {2019},
	note = {MAG ID: 2988675894
S2ID: dee33aa8d7f8ff4458a65379ebce8bd8860d0100},
}

@article{catherine_a_gao_comparing_2022,
	title = {Comparing scientific abstracts generated by {ChatGPT} to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers},
	doi = {10.1101/2022.12.23.521610},
	abstract = {Background Large language models such as ChatGPT can produce increasingly realistic text, with unknown information on the accuracy and integrity of using these models in scientific writing. Methods We gathered ten research abstracts from five high impact factor medical journals (n=50) and asked ChatGPT to generate research abstracts based on their titles and journals. We evaluated the abstracts using an artificial intelligence (AI) output detector, plagiarism detector, and had blinded human reviewers try to distinguish whether abstracts were original or generated. Results All ChatGPT-generated abstracts were written clearly but only 8\% correctly followed the specific journal’s formatting requirements. Most generated abstracts were detected using the AI output detector, with scores (higher meaning more likely to be generated) of median [interquartile range] of 99.98\% [12.73, 99.98] compared with very low probability of AI-generated output in the original abstracts of 0.02\% [0.02, 0.09]. The AUROC of the AI output detector was 0.94. Generated abstracts scored very high on originality using the plagiarism detector (100\% [100, 100] originality). Generated abstracts had a similar patient cohort size as original abstracts, though the exact numbers were fabricated. When given a mixture of original and general abstracts, blinded human reviewers correctly identified 68\% of generated abstracts as being generated by ChatGPT, but incorrectly identified 14\% of original abstracts as being generated. Reviewers indicated that it was surprisingly difficult to differentiate between the two, but that the generated abstracts were vaguer and had a formulaic feel to the writing. Conclusion ChatGPT writes believable scientific abstracts, though with completely generated data. These are original without any plagiarism detected but are often identifiable using an AI output detector and skeptical human reviewers. Abstract evaluation for journals and medical conferences must adapt policy and practice to maintain rigorous scientific standards; we suggest inclusion of AI output detectors in the editorial process and clear disclosure if these technologies are used. The boundaries of ethical and acceptable use of large language models to help scientific writing remain to be determined.},
	journal = {bioRxiv},
	author = {{Catherine A. Gao} and {Frederick M. Howard} and {N. Markov} and {E. Dyer} and {S. Ramesh} and {Yuan Luo} and {Alexander T. Pearson}},
	year = {2022},
	doi = {10.1101/2022.12.23.521610},
	note = {S2ID: b36acdfc67612d707c95d1ed282672d3ca262be7},
}

@article{kalpesh_krishna_paraphrasing_2023,
	title = {Paraphrasing evades detectors of {AI}-generated text, but retrieval is an effective defense},
	abstract = {To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3\% to 4.6\% (at a constant false positive rate of 1\%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80\% to 97\% of paraphrased generations across different settings, while only classifying 1\% of human-written sequences as AI-generated. We will open source our code, model and data for future research.},
	author = {{Kalpesh Krishna} and {Yixiao Song} and {Marzena Karpinska} and {J. Wieting} and {Mohit Iyyer}},
	year = {2023},
	note = {ARXIV\_ID: 2303.13408
S2ID: 2969e8a14237f8244d3c825ff19bdfb3cc7fddf1},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/26Z4LSTB/2303.html:text/html;Krishna et al_2023_Paraphrasing evades detectors of AI-generated text, but retrieval is an.pdf:/Users/neilnatarajan/Zotero/storage/7HN24CJJ/Krishna et al_2023_Paraphrasing evades detectors of AI-generated text, but retrieval is an.pdf:application/pdf},
}

@article{vinu_sankar_sadasivan_can_2023,
	title = {Can {AI}-{Generated} {Text} be {Reliably} {Detected}?},
	doi = {10.48550/arxiv.2303.11156},
	abstract = {The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, both empirically and theoretically, we show that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of the generative text model, can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors and zero-shot classifiers. We then provide a theoretical impossibility result indicating that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden watermarking signatures and add them to their generated text to be detected as text generated by the LLMs, potentially causing reputational damages to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.},
	journal = {ArXiv},
	author = {{Vinu Sankar Sadasivan} and {Aounon Kumar} and {S. Balasubramanian} and {Wenxiao Wang} and {S. Feizi}},
	year = {2023},
	doi = {10.48550/arxiv.2303.11156},
	note = {ARXIV\_ID: 2303.11156
S2ID: fb47aa3c541fc2a9b340c9d2a3572860811767d6},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/5DDI8322/2303.html:text/html;Sadasivan et al_2023_Can AI-Generated Text be Reliably Detected.pdf:/Users/neilnatarajan/Zotero/storage/Z7NQAG3L/Sadasivan et al_2023_Can AI-Generated Text be Reliably Detected.pdf:application/pdf},
}

@article{tharindu_kumarage_stylometric_2023,
	title = {Stylometric {Detection} of {AI}-{Generated} {Text} in {Twitter} {Timelines}},
	doi = {10.48550/arxiv.2303.03697},
	abstract = {Recent advancements in pre-trained language models have enabled convenient methods for generating human-like text at a large scale. Though these generation capabilities hold great potential for breakthrough applications, it can also be a tool for an adversary to generate misinformation. In particular, social media platforms like Twitter are highly susceptible to AI-generated misinformation. A potential threat scenario is when an adversary hijacks a credible user account and incorporates a natural language generator to generate misinformation. Such threats necessitate automated detectors for AI-generated tweets in a given user's Twitter timeline. However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline. In this paper, we present a novel algorithm using stylometric signals to aid detecting AI-generated tweets. We propose models corresponding to quantifying stylistic changes in human and AI tweets in two related tasks: Task 1 - discriminate between human and AI-generated tweets, and Task 2 - detect if and when an AI starts to generate tweets in a given Twitter timeline. Our extensive experiments demonstrate that the stylometric features are effective in augmenting the state-of-the-art AI-generated text detectors.},
	journal = {ArXiv},
	author = {{Tharindu Kumarage} and {Joshua Garland} and {Amrita Bhattacharjee} and {K. Trapeznikov} and {Scott W. Ruston} and {Huan Liu}},
	year = {2023},
	doi = {10.48550/arxiv.2303.03697},
	note = {ARXIV\_ID: 2303.03697
S2ID: 18e0b11dd5b1b413d33308da0379836752aaaec1},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/I3GLTGM6/2303.html:text/html;Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines_annotated.pdf:/Users/neilnatarajan/Zotero/storage/QYB42FR6/Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines_annotated.pdf:application/pdf;Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines.pdf:/Users/neilnatarajan/Zotero/storage/QYB42FR6/Kumarage et al_2023_Stylometric Detection of AI-Generated Text in Twitter Timelines.pdf:application/pdf},
}

@article{hu_challenges_2023,
	title = {Challenges for enforcing editorial policies on {AI}-generated papers},
	volume = {0},
	issn = {0898-9621},
	url = {https://doi.org/10.1080/08989621.2023.2184262},
	doi = {10.1080/08989621.2023.2184262},
	abstract = {ChatGPT, a chatbot released by OpenAI in November 2022, has rocked academia with its capacity to generate papers “good enough” for academic journals. Major journals such as Nature and professional societies such as the World Association of Medical Editors have moved fast to issue policies to ban or curb AI-written papers. Amid the flurry of policy initiatives, one important challenge seems to be overlooked: AI-generated papers are not easily discernible to the human eye, and we lack the right tools to implement the policies. Without such tools, the well-intentioned policies are likely to remain on paper.},
	number = {0},
	urldate = {2023-04-06},
	journal = {Accountability in Research},
	author = {Hu, Guangwei},
	month = feb,
	year = {2023},
	pmid = {36840450},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08989621.2023.2184262},
	keywords = {ChatGPT; editorial policies; AI-generated papers; challenges},
	pages = {1--3},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/46N2DI8B/Hu - 2023 - Challenges for enforcing editorial policies on AI-.pdf:application/pdf},
}

@article{anderson_ai_2023,
	title = {{AI} did not write this manuscript, or did it? {Can} we trick the {AI} text detector into generated texts? {The} potential future of {ChatGPT} and {AI} in {Sports} \&amp; {Exercise} {Medicine} manuscript generation},
	volume = {9},
	copyright = {© Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {2055-7647},
	shorttitle = {{AI} did not write this manuscript, or did it?},
	url = {https://bmjopensem.bmj.com/content/9/1/e001568},
	doi = {10.1136/bmjsem-2023-001568},
	abstract = {Researching a topic and generating an academic paper is a nuanced skill. It can take months or years to produce and publish one, if it is ever published at all. What if there were a way to make this happen instantly? Artificial intelligence (AI) may hold a flame to quickly analyse a research topic and generate an academic paper. There are many forms of AI; this editorial discusses natural language model-based AI, such as ChatGPT, and their potential ability to generate academic papers.

Natural language model-based AI, in particular ChatGPT, is generating new content and a lot of controversies. This AI software is innovative. It generates, de novo, content that has a natural conversational flow. It can quickly answer questions and write poems, fan fiction and children’s books.1 ChatGPT has even passed the United States Medical Licensing Examination theory section with no additional training and/or years of studying medicine.2

Language-based AI has already entered the scientific community. Nature reported that four manuscripts in preprint credit ChatGPT as an author.3 Also, an article reported that AI had been used to generate an academic paper.4

In this editorial, we discuss the pros and cons of AI for manuscript generation in sports and exercise medicine (SEM), generate an academic paper using AI and bypass AI-generation detection, and discuss potential concerns regarding natural language model-based AI. We aim to get insights on how AI, in particular ChatGPT and similar language model-based AI, will impact the future of manuscript generation in SEM. To achieve such purpose, we ought to consider what is an academic paper, whether AI should write academic papers, what the issues are, what our stance should be on AI-generated texts and how we deal with them.

An academic paper has a thesis and aims to persuade readers of …},
	language = {en},
	number = {1},
	urldate = {2023-04-06},
	journal = {BMJ Open Sport \& Exercise Medicine},
	author = {Anderson, Nash and Belavy, Daniel L. and Perle, Stephen M. and Hendricks, Sharief and Hespanhol, Luiz and Verhagen, Evert and Memon, Aamir R.},
	month = feb,
	year = {2023},
	note = {Publisher: BMJ Specialist Journals
Section: Editorial},
	keywords = {Medical Ethics, Position statement, Research, Sports, Sports \& exercise medicine},
	pages = {e001568},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/ANISXSR5/Anderson et al. - 2023 - AI did not write this manuscript, or did it Can w.pdf:application/pdf},
}

@article{kobis_artificial_2021,
	title = {Artificial intelligence versus {Maya} {Angelou}: {Experimental} evidence that people cannot differentiate {AI}-generated from human-written poetry},
	volume = {114},
	issn = {0747-5632},
	shorttitle = {Artificial intelligence versus {Maya} {Angelou}},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563220303034},
	doi = {10.1016/j.chb.2020.106553},
	abstract = {The release of openly available, robust natural language generation algorithms (NLG) has spurred much public attention and debate. One reason lies in the algorithms' purported ability to generate humanlike text across various domains. Empirical evidence using incentivized tasks to assess whether people (a) can distinguish and (b) prefer algorithm-generated versus human-written text is lacking. We conducted two experiments assessing behavioral reactions to the state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal = 830). Using the identical starting lines of human poems, GPT-2 produced samples of poems. From these samples, either a random poem was chosen (Human-out-of-theloop) or the best one was selected (Human-in-the-loop) and in turn matched with a human-written poem. In a new incentivized version of the Turing Test, participants failed to reliably detect the algorithmicallygenerated poems in the Human-in-the-loop treatment, yet succeeded in the Human-out-of-the-loop treatment. Further, people reveal a slight aversion to algorithm-generated poetry, independent on whether participants were informed about the algorithmic origin of the poem (Transparency) or not (Opacity). We discuss what these results convey about the performance of NLG algorithms to produce human-like text and propose methodologies to study such learning algorithms in human-agent experimental settings.},
	language = {en},
	urldate = {2023-04-06},
	journal = {Computers in Human Behavior},
	author = {Köbis, Nils and Mossink, Luca D.},
	month = jan,
	year = {2021},
	keywords = {Computational creativity, Creativity, Machine behavior, Natural language generation, Test, Turing},
	pages = {106553},
	file = {ScienceDirect Full Text PDF:/Users/neilnatarajan/Zotero/storage/ZUVRGDDI/Köbis and Mossink - 2021 - Artificial intelligence versus Maya Angelou Exper.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/ALUIBRXG/S0747563220303034.html:text/html},
}

@misc{mitchell_detectgpt_2023,
	title = {{DetectGPT}: {Zero}-{Shot} {Machine}-{Generated} {Text} {Detection} using {Probability} {Curvature}},
	shorttitle = {{DetectGPT}},
	url = {http://arxiv.org/abs/2301.11305},
	doi = {10.48550/arXiv.2301.11305},
	abstract = {The fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is machine-written. For example, students may use LLMs to complete written assignments, leaving instructors unable to accurately assess student learning. In this paper, we first demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g, T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11305 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/5FSXCBTN/Mitchell et al. - 2023 - DetectGPT Zero-Shot Machine-Generated Text Detect.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GFYNS2GZ/2301.html:text/html},
}

@article{alkaissi_artificial_2023,
	title = {Artificial {Hallucinations} in {ChatGPT}: {Implications} in {Scientific} {Writing}},
	issn = {2168-8184},
	shorttitle = {Artificial {Hallucinations} in {ChatGPT}},
	url = {https://www.cureus.com/articles/138667-artificial-hallucinations-in-chatgpt-implications-in-scientific-writing},
	doi = {10.7759/cureus.35179},
	abstract = {While still in its infancy, ChatGPT (Generative Pretrained Transformer), introduced in November 2022, is bound to hugely impact many industries, including healthcare, medical education, biomedical research, and scientific writing. Implications of ChatGPT, that new chatbot introduced by OpenAI on academic writing, is largely unknown. In response to the Journal of Medical Science (Cureus) Turing Test - call for case reports written with the assistance of ChatGPT, we present two cases one of homocystinuria-associated osteoporosis, and the other is on late-onset Pompe disease (LOPD), a rare metabolic disorder. We tested ChatGPT to write about the pathogenesis of these conditions. We documented the positive, negative, and rather troubling aspects of our newly introduced chatbot’s performance.},
	language = {en},
	urldate = {2023-04-08},
	journal = {Cureus},
	author = {Alkaissi, Hussam and McFarlane, Samy I},
	month = feb,
	year = {2023},
	file = {Alkaissi and McFarlane - 2023 - Artificial Hallucinations in ChatGPT Implications.pdf:/Users/neilnatarajan/Zotero/storage/ZQPY9WXN/Alkaissi and McFarlane - 2023 - Artificial Hallucinations in ChatGPT Implications.pdf:application/pdf},
}

@article{tetlock_psychology_2000,
	title = {The psychology of the unthinkable: {Taboo} trade-offs, forbidden base rates, and heretical counterfactuals.},
	volume = {78},
	issn = {1939-1315, 0022-3514},
	shorttitle = {The psychology of the unthinkable},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.78.5.853},
	doi = {10.1037/0022-3514.78.5.853},
	language = {en},
	number = {5},
	urldate = {2023-04-13},
	journal = {Journal of Personality and Social Psychology},
	author = {Tetlock, Philip E. and Kristel, Orie V. and Elson, S. Beth and Green, Melanie C. and Lerner, Jennifer S.},
	year = {2000},
	pages = {853--870},
	file = {Tetlock et al. - 2000 - The psychology of the unthinkable Taboo trade-off.pdf:/Users/neilnatarajan/Zotero/storage/P89ZLI2Z/Tetlock et al. - 2000 - The psychology of the unthinkable Taboo trade-off.pdf:application/pdf},
}

@article{flanigan_fair_2021,
	title = {Fair algorithms for selecting citizens’ assemblies},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03788-6},
	doi = {10.1038/s41586-021-03788-6},
	abstract = {Globally, there has been a recent surge in ‘citizens’ assemblies’1, which are a form of civic participation in which a panel of randomly selected constituents contributes to questions of policy. The random process for selecting this panel should satisfy two properties. First, it must produce a panel that is representative of the population. Second, in the spirit of democratic equality, individuals would ideally be selected to serve on this panel with equal probability2,3. However, in practice these desiderata are in tension owing to differential participation rates across subpopulations4,5. Here we apply ideas from fair division to develop selection algorithms that satisfy the two desiderata simultaneously to the greatest possible extent: our selection algorithms choose representative panels while selecting individuals with probabilities as close to equal as mathematically possible, for many metrics of ‘closeness to equality’. Our implementation of one such algorithm has already been used to select more than 40 citizens’ assemblies around the world. As we demonstrate using data from ten citizens’ assemblies, adopting our algorithm over a benchmark representing the previous state of the art leads to substantially fairer selection probabilities. By contributing a fairer, more principled and deployable algorithm, our work puts the practice of sortition on firmer foundations. Moreover, our work establishes citizens’ assemblies as a domain in which insights from the field of fair division can lead to high-impact applications.},
	language = {en},
	number = {7873},
	urldate = {2023-05-17},
	journal = {Nature},
	author = {Flanigan, Bailey and Gölz, Paul and Gupta, Anupam and Hennig, Brett and Procaccia, Ariel D.},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {\_tablet, Computer science, Economics},
	pages = {548--552},
	file = {Flanigan et al_2021_Fair algorithms for selecting citizens’ assemblies.pdf:/Users/neilnatarajan/Zotero/storage/HJESJVT9/Flanigan et al_2021_Fair algorithms for selecting citizens’ assemblies.pdf:application/pdf},
}

@misc{chen_machine_2023,
	title = {Machine {Explanations} and {Human} {Understanding}},
	url = {http://arxiv.org/abs/2202.04092},
	doi = {10.48550/arXiv.2202.04092},
	abstract = {Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, but they cannot improve human understanding of task decision boundary or model error. To achieve complementary human-AI performance, we articulate possible ways on how explanations need to work with human intuitions. For instance, human intuitions about the relevance of features (e.g., education is more important than age in predicting a person's income) can be critical in detecting model error. We validate the importance of human intuitions in shaping the outcome of machine explanations with empirical human-subject studies. Overall, our work provides a general framework along with actionable implications for future algorithmic development and empirical experiments of machine explanations.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Chen, Chacha and Feng, Shi and Sharma, Amit and Tan, Chenhao},
	month = may,
	year = {2023},
	note = {arXiv:2202.04092 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/U8C5BYB2/Chen et al. - 2023 - Machine Explanations and Human Understanding.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/GV6N3KA5/2202.html:text/html},
}

@article{sweidan_probabilistic_nodate,
	title = {Probabilistic {Prediction} in scikit-learn},
	abstract = {Adding conﬁdence measures to predictive models should increase the trustworthiness, but only if the models are well-calibrated. Historically, some algorithms like logistic regression, but also neural networks, have been considered to produce well-calibrated probability estimates oﬀ-the-shelf. Other techniques, like decision trees and Naive Bayes, on the other hand, are infamous for being signiﬁcantly overconﬁdent in their probabilistic predictions. In this paper, a large experimental study is conducted to investigate how well-calibrated models produced by a number of algorithms in the scikit-learn library are out-of-the-box, but also if either the built-in calibration techniques Platt scaling and isotonic regression, or Venn-Abers, can be used to improve the calibration. The results show that of the seven algorithms evaluated, the only one obtaining well-calibrated models without the external calibration is logistic regression. All other algorithms, i.e., decision trees, adaboost, gradient boosting, kNN, naive Bayes and random forest beneﬁt from using any of the calibration techniques. In particular, decision trees, Naive Bayes and the boosted models are substantially improved using external calibration. From a practitioner’s perspective, the obvious recommendation becomes to incorporate calibration when using probabilistic prediction. Comparing the diﬀerent calibration techniques, Platt scaling and VennAbers generally outperform isotonic regression, on these rather small datasets. Finally, the unique ability of Venn-Abers to output not only well-calibrated probability estimates, but also the conﬁdence in these estimates is demonstrated.},
	language = {en},
	author = {Sweidan, Dirar and Johansson, Ulf},
	file = {Sweidan_Johansson_Probabilistic Prediction in scikit-learn_annotated.pdf:/Users/neilnatarajan/Zotero/storage/LM4G8XP8/Sweidan_Johansson_Probabilistic Prediction in scikit-learn_annotated.pdf:application/pdf;Sweidan_Johansson_Probabilistic Prediction in scikit-learn.pdf:/Users/neilnatarajan/Zotero/storage/LM4G8XP8/Sweidan_Johansson_Probabilistic Prediction in scikit-learn.pdf:application/pdf},
}

@misc{noauthor_towards_nodate,
	title = {Towards a {Science} of {Human}-{AI} {Decision} {Making}: {An} {Overview} of {Design} {Space} in {Empirical} {Human}-{Subject} {Studies} {\textbar} {Proceedings} of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	url = {https://dl.acm.org/doi/10.1145/3593013.3594087},
	urldate = {2023-07-28},
	keywords = {\_tablet},
	file = {Towards a Science of Human-AI Decision Making\: An Overview of Design Space in Empirical Human-Subject Studies | Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency:/Users/neilnatarajan/Zotero/storage/9KB3NJJA/3593013.html:text/html;Towards a Science of Human-AI Decision Making.pdf:/Users/neilnatarajan/Zotero/storage/5GF78IM9/Towards a Science of Human-AI Decision Making.pdf:application/pdf},
}

@inproceedings{faliagka_application_2012,
	title = {Application of {Machine} {Learning} {Algorithms} to an online {Recruitment} {System}},
	abstract = {In this work, we present a novel approach for evaluating job applicants in online recruitment systems, leveraging machine learning algorithms to solve the candidate ranking problem. An application of our approach is implemented in the form of a prototype system, whose functionality is showcased and evaluated in a real-world recruitment scenario. The proposed system extracts a set of objective criteria from the applicants' LinkedIn profile, and infers their personality characteristics using linguistic analysis on their blog posts. Our system was found to perform consistently compared to human recruiters; thus, it can be trusted for the automation of applicant ranking and personality mining.},
	author = {Faliagka, Evanthia and Ramantas, Kostas and Tsakalidis, Athanasios and Tzimas, Giannis},
	month = jan,
	year = {2012},
	keywords = {\_tablet},
	file = {Faliagka et al_2012_Application of Machine Learning Algorithms to an online Recruitment System.pdf:/Users/neilnatarajan/Zotero/storage/TGN5TFTH/Faliagka et al_2012_Application of Machine Learning Algorithms to an online Recruitment System.pdf:application/pdf},
}

@article{upadhyay_applying_2018,
	title = {Applying artificial intelligence: implications for recruitment},
	volume = {17},
	copyright = {© Emerald Publishing Limited 2018},
	issn = {14754398},
	shorttitle = {Applying artificial intelligence},
	url = {https://www.proquest.com/docview/2133758924/abstract/BB1306B422284823PQ/1},
	doi = {10.1108/SHR-07-2018-0051},
	abstract = {Purpose
This paper aims to review the applications of artificial intelligence (AI) in the hiring process and its practical implications. This paper highlights the strategic shift in recruitment industry caused due to the adoption of AI in the recruitment process.
This paper is prepared by independent academicians who have synthesized their views by a review of the latest reports, articles, research papers and other relevant literature.
This paper describes the impact of developments in the field of AI on the hiring process and the recruitment industry. The application of AI for managing the recruitment process is leading to efficiency as well as qualitative gains for both clients and candidates.
This paper offers strategic insights into automation of the recruitment process and presents practical ideas for implementation of AI in the recruitment industry. It also discusses the strategic implications of the usage of AI in the recruitment industry.
This article describes the role of technological advancements in AI and its application for creating value for the recruitment industry as well as the clients. It saves the valuable reading time of practitioners and researchers by highlighting the AI applications in the recruitment industry in a concise and simple format.},
	language = {English},
	number = {5},
	urldate = {2023-08-01},
	journal = {Strategic HR Review},
	author = {Upadhyay, Ashwani Kumar and Khandelwal, Komal},
	year = {2018},
	note = {Num Pages: 4
Place: Bingley, United Kingdom
Publisher: Emerald Group Publishing Limited},
	keywords = {\_tablet, Artificial intelligence, Automation, Hiring, Recruitment, Strategic},
	pages = {255--258},
	file = {Upadhyay_Khandelwal_2018_Applying artificial intelligence.pdf:/Users/neilnatarajan/Zotero/storage/V8C6L7KB/Upadhyay_Khandelwal_2018_Applying artificial intelligence.pdf:application/pdf},
}

@inproceedings{pandey_applicants_2022,
	title = {Applicants' {Perception} {Towards} the {Application} of {AI} in {Recruitment} {Process}},
	doi = {10.1109/IRTM54583.2022.9791587},
	abstract = {An organization or business needs professional employees to achieve its targets in order to succeed in this dynamic period. They are now at the forefront of the fourth technological revolution. In this new era, everybody needs strong, future and creative workers to stay successful. To order to navigate the new landscape and improve the market climate, companies with a successful management plan will include an acceptable employee. Recruitment strategy is the primary consideration for any company to employ skilled employees who are willing to accomplish their job objectives more efficiently and effectively. Clearly, the recruiting approach as a core feature of the company is focused on data collection for decisionmaking. This paper discusses artificial intelligence (AI) and the effect it has on the recruiting industry. This thesis explored the effect of AI on managers and applicants during the first phases of the recruiting process.},
	booktitle = {2022 {Interdisciplinary} {Research} in {Technology} and {Management} ({IRTM})},
	author = {Pandey, Suruchi and Bahukhandi, Medha},
	month = feb,
	year = {2022},
	keywords = {Artificial Intelligence, Artificial intelligence, Recruitment, AI, Brand management, Companies, Data collection, Employee, Industries, Navigation, Perception, Standards organizations},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/neilnatarajan/Zotero/storage/3KIW4ZZE/9791587.html:text/html;Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process_annotated.pdf:/Users/neilnatarajan/Zotero/storage/VPTUKG3U/Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process_annotated.pdf:application/pdf;Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process.pdf:/Users/neilnatarajan/Zotero/storage/VPTUKG3U/Pandey_Bahukhandi_2022_Applicants' Perception Towards the Application of AI in Recruitment Process.pdf:application/pdf},
}

@article{van_esch_marketing_2019,
	title = {Marketing {AI} recruitment: {The} next phase in job application and selection},
	volume = {90},
	issn = {0747-5632},
	shorttitle = {Marketing {AI} recruitment},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563218304497},
	doi = {10.1016/j.chb.2018.09.009},
	abstract = {Organizations are beginning to adopt and capitalize on the functionality of AI in their recruitment processes. However, little is known about how potential candidates regard the use of AI as part of the recruitment process and whether or not it influences their likelihood to apply for a job. Our research finds that attitudes towards organizations that use AI in the recruitment process, significantly influences the likelihood that potential candidates will complete the application process. The novelty factor of using AI in the recruitment process, mediates and further positively influences job application likelihood. These positive relationships between attitudes towards the use of AI in the recruitment process and the likelihood of applying for a job have several important practical implications. First, it means that whilst anxiety is naturally present when AI is part of the recruitment process, the anxiety doesn't really affect the completion of job applications and therefore, organizations do not need to spend money on either hiding their use of AI or reducing the anxiety levels of potential candidates. To the contrary, the research suggests that organizations do not need to hide their use of AI in fear of alienating potential candidates, rather organizations may want to promote their use of AI in the recruitment process and focus on potential candidates that already have positive views of both the organization and AI.},
	language = {en},
	urldate = {2023-08-01},
	journal = {Computers in Human Behavior},
	author = {van Esch, Patrick and Black, J. Stewart and Ferolie, Joseph},
	month = jan,
	year = {2019},
	keywords = {Artificial intelligence (AI), Technology, Recruitment, Job application likelihood, Marketing, Selection},
	pages = {215--222},
	file = {ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/ZZBI8NGN/S0747563218304497.html:text/html;van Esch et al_2019_Marketing AI recruitment_annotated.pdf:/Users/neilnatarajan/Zotero/storage/BHMBX3PU/van Esch et al_2019_Marketing AI recruitment_annotated.pdf:application/pdf;van Esch et al_2019_Marketing AI recruitment.pdf:/Users/neilnatarajan/Zotero/storage/BHMBX3PU/van Esch et al_2019_Marketing AI recruitment.pdf:application/pdf},
}

@article{pillai_adoption_2020,
	title = {Adoption of artificial intelligence ({AI}) for talent acquisition in {IT}/{ITeS} organizations},
	volume = {27},
	doi = {10.1108/bij-04-2020-0186},
	abstract = {Human resource managers are adopting AI technology for conducting various tasks of human resource management, starting from manpower planning till employee exit. AI technology is prominently used for talent acquisition in organizations. This research investigates the adoption of AI technology for talent acquisition.,This study employs Technology-Organization-Environment (TOE) and Task-Technology-Fit (TTF) framework and proposes a model to explore the adoption of AI technology for talent acquisition. The survey was conducted among the 562 human resource managers and talent acquisition managers with a structured questionnaire. The analysis of data was completed using PLS-SEM.,This research reveals that cost-effectiveness, relative advantage, top management support, HR readiness, competitive pressure and support from AI vendors positively affect AI technology adoption for talent acquisition. Security and privacy issues negatively influence the adoption of AI technology. It is found that task and technology characteristics influence the task technology fit of AI technology for talent acquisition. Adoption and task technology fit of AI technology influence the actual usage of AI technology for talent acquisition. It is revealed that stickiness to traditional talent acquisition methods negatively moderates the association between adoption and actual usage of AI technology for talent acquisition. The proposed model was empirically validated and revealed the predictors of adoption and actual usage of AI technology for talent acquisition.,This paper provides the predictors of the adoption of AI technology for talent acquisition, which is emerging extensively in the human resource domain. It provides vital insights to the human resource managers to benchmark AI technology required for talent acquisition. Marketers can develop their marketing plan considering the factors of adoption. It would help designers to understand the factors of adoption and design the AI technology algorithms and applications for talent acquisition. It contributes to advance the literature of technology adoption by interweaving it with the human resource domain literature on talent acquisition.,This research uniquely validates the model for the adoption of AI technology for talent acquisition using the TOE and TTF framework. It reveals the factors influencing the adoption and actual usage of AI technology for talent acquisition.},
	number = {9},
	journal = {Benchmarking: An International Journal},
	author = {Pillai, Rajasshrie and Sivathanu, Brijesh},
	month = aug,
	year = {2020},
	doi = {10.1108/bij-04-2020-0186},
	note = {MAG ID: 3049223401},
	keywords = {\_tablet},
	pages = {2599--2629},
	file = {Pillai_Sivathanu_2020_Adoption of artificial intelligence (AI) for talent acquisition in IT-ITeS.pdf:/Users/neilnatarajan/Zotero/storage/RZVICFBU/Pillai_Sivathanu_2020_Adoption of artificial intelligence (AI) for talent acquisition in IT-ITeS.pdf:application/pdf},
}

@article{angrave_hr_2016,
	title = {{HR} and analytics: why {HR} is set to fail the big data challenge},
	volume = {26},
	doi = {10.1111/1748-8583.12090},
	abstract = {The HR world is abuzz with talk of big data and the transformative potential of HR analytics. This article takes issue with optimistic accounts which hail HR analytics as a ‘must have’ capability that will ensure HR’s future as a strategic management function while transforming organisational performance for the better. It argues that unless the HR profession wises up to both the potential and drawbacks of this emerging field, and engages operationally and strategically to develop better methods and approaches, it is unlikely that existing practices of HR analytics will deliver transformational change. Indeed, it is possible that current trends will seal the exclusion of HR from strategic, board level influence while doing little to benefit organisations and actively damaging the interests of employees.},
	number = {1},
	journal = {Human Resource Management Journal},
	author = {Angrave, David and Charlwood, Andy and Kirkpatrick, Ian and Lawrence, Mark and Lawrence, Mark T and Stuart, Mark},
	month = jan,
	year = {2016},
	doi = {10.1111/1748-8583.12090},
	note = {MAG ID: 2222612072},
	keywords = {\_tablet},
	pages = {1--11},
	file = {Angrave et al_2016_HR and analytics.pdf:/Users/neilnatarajan/Zotero/storage/JYUEBXTF/Angrave et al_2016_HR and analytics.pdf:application/pdf},
}

@article{dwivedi_artificial_2021,
	title = {Artificial {Intelligence} ({AI}): {Multidisciplinary} perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
	volume = {57},
	issn = {02684012},
	shorttitle = {Artificial {Intelligence} ({AI})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S026840121930917X},
	doi = {10.1016/j.ijinfomgt.2019.08.002},
	abstract = {As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportu­ nities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.},
	language = {en},
	urldate = {2023-08-01},
	journal = {International Journal of Information Management},
	author = {Dwivedi, Yogesh K. and Hughes, Laurie and Ismagilova, Elvira and Aarts, Gert and Coombs, Crispin and Crick, Tom and Duan, Yanqing and Dwivedi, Rohita and Edwards, John and Eirug, Aled and Galanos, Vassilis and Ilavarasan, P. Vigneswara and Janssen, Marijn and Jones, Paul and Kar, Arpan Kumar and Kizgin, Hatice and Kronemann, Bianca and Lal, Banita and Lucini, Biagio and Medaglia, Rony and Le Meunier-FitzHugh, Kenneth and Le Meunier-FitzHugh, Leslie Caroline and Misra, Santosh and Mogaji, Emmanuel and Sharma, Sujeet Kumar and Singh, Jang Bahadur and Raghavan, Vishnupriya and Raman, Ramakrishnan and Rana, Nripendra P. and Samothrakis, Spyridon and Spencer, Jak and Tamilmani, Kuttimani and Tubadji, Annie and Walton, Paul and Williams, Michael D.},
	month = apr,
	year = {2021},
	pages = {101994},
	file = {Dwivedi et al_2021_Artificial Intelligence (AI)_annotated.pdf:/Users/neilnatarajan/Zotero/storage/V9PPGCVI/Dwivedi et al_2021_Artificial Intelligence (AI)_annotated.pdf:application/pdf;Dwivedi et al_2021_Artificial Intelligence (AI).pdf:/Users/neilnatarajan/Zotero/storage/V9PPGCVI/Dwivedi et al_2021_Artificial Intelligence (AI).pdf:application/pdf},
}

@article{black_ai-enabled_2020,
	series = {{ARTIFICIAL} {INTELLIGENCE} {AND} {MACHINE} {LEARNING}},
	title = {{AI}-enabled recruiting: {What} is it and how should a manager use it?},
	volume = {63},
	issn = {0007-6813},
	shorttitle = {{AI}-enabled recruiting},
	url = {https://www.sciencedirect.com/science/article/pii/S0007681319301612},
	doi = {10.1016/j.bushor.2019.12.001},
	abstract = {AI-enabled recruiting systems have evolved from nice to talk about to necessary to utilize. In this article, we outline the reasons underlying this development. First, as competitive advantages have shifted from tangible to intangible assets, human capital has transitioned from supporting cast to a starring role. Second, as digitalization has redesigned both the business and social landscapes, digital recruiting of human capital has moved from the periphery to center stage. Third, recent and near-future advances in AI-enabled recruiting have improved recruiting efficiency to the point that managers ignore them or procrastinate their utilization at their own peril. In addition to explaining the forces that have pushed AI-enabled recruiting systems from nice to necessary, we outline the key strategic steps managers need to take in order to capture its main benefits.},
	language = {en},
	number = {2},
	urldate = {2023-08-01},
	journal = {Business Horizons},
	author = {Black, J. Stewart and van Esch, Patrick},
	month = mar,
	year = {2020},
	keywords = {\_tablet, Artificial intelligence, AI-enabled recruiting, Digital recruiting technology, Human resources},
	pages = {215--226},
	file = {Black_van Esch_2020_AI-enabled recruiting.pdf:/Users/neilnatarajan/Zotero/storage/T3BIQUWA/Black_van Esch_2020_AI-enabled recruiting.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/DBM8TZF3/S0007681319301612.html:text/html},
}

@inproceedings{raghavan_mitigating_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Mitigating bias in algorithmic hiring: evaluating claims and practices},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Mitigating bias in algorithmic hiring},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372828},
	doi = {10.1145/3351095.3372828},
	abstract = {There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.},
	urldate = {2023-08-01},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
	month = jan,
	year = {2020},
	keywords = {algorithmic bias, algorithmic hiring, discrimination law},
	pages = {469--481},
	file = {Raghavan et al_2020_Mitigating bias in algorithmic hiring_annotated.pdf:/Users/neilnatarajan/Zotero/storage/QN32Z9MK/Raghavan et al_2020_Mitigating bias in algorithmic hiring_annotated.pdf:application/pdf;Raghavan et al_2020_Mitigating bias in algorithmic hiring.pdf:/Users/neilnatarajan/Zotero/storage/QN32Z9MK/Raghavan et al_2020_Mitigating bias in algorithmic hiring.pdf:application/pdf},
}

@misc{cappelli_artificial_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Artificial {Intelligence} in {Human} {Resources} {Management}: {Challenges} and a {Path} {Forward}},
	shorttitle = {Artificial {Intelligence} in {Human} {Resources} {Management}},
	url = {https://papers.ssrn.com/abstract=3263878},
	doi = {10.2139/ssrn.3263878},
	abstract = {We consider the gap between the promise and reality of artificial intelligence in human resource management and suggest how progress might be made. We identify four challenges in using data science techniques for HR tasks: 1) complexity of HR phenomena, 2) constraints imposed by small data sets, 3) accountability questions associated with fairness and other ethical and legal constraints, and 4) possible adverse employee reactions to management decisions via data-based algorithms. We propose practical responses to these challenges and converge on three overlapping principles - causal reasoning, randomization and experiments, and employee contribution—that could be both economically efficient and socially appropriate for using data science in the management of employees.},
	language = {en},
	urldate = {2023-08-01},
	author = {Cappelli, Peter and Tambe, Prasanna and Yakubovich, Valery},
	month = apr,
	year = {2019},
	keywords = {big data, artificial intelligence, machine learning, algorithmic management, data science, human resource management},
	file = {Cappelli et al_2019_Artificial Intelligence in Human Resources Management_annotated.pdf:/Users/neilnatarajan/Zotero/storage/MI46DQCH/Cappelli et al_2019_Artificial Intelligence in Human Resources Management_annotated.pdf:application/pdf;Cappelli et al_2019_Artificial Intelligence in Human Resources Management.pdf:/Users/neilnatarajan/Zotero/storage/MI46DQCH/Cappelli et al_2019_Artificial Intelligence in Human Resources Management.pdf:application/pdf},
}

@article{jarrahi_artificial_2018,
	title = {Artificial intelligence and the future of work: {Human}-{AI} symbiosis in organizational decision making},
	volume = {61},
	issn = {0007-6813},
	shorttitle = {Artificial intelligence and the future of work},
	url = {https://www.sciencedirect.com/science/article/pii/S0007681318300387},
	doi = {10.1016/j.bushor.2018.03.007},
	abstract = {Artificial intelligence (AI) has penetrated many organizational processes, resulting in a growing fear that smart machines will soon replace many humans in decision making. To provide a more proactive and pragmatic perspective, this article highlights the complementarity of humans and AI and examines how each can bring their own strength in organizational decision-making processes typically characterized by uncertainty, complexity, and equivocality. With a greater computational information processing capacity and an analytical approach, AI can extend humans’ cognition when addressing complexity, whereas humans can still offer a more holistic, intuitive approach in dealing with uncertainty and equivocality in organizational decision making. This premise mirrors the idea of intelligence augmentation, which states that AI systems should be designed with the intention of augmenting, not replacing, human contributions.},
	language = {en},
	number = {4},
	urldate = {2023-08-01},
	journal = {Business Horizons},
	author = {Jarrahi, Mohammad Hossein},
	month = jul,
	year = {2018},
	keywords = {\_tablet, Artificial intelligence, Analytical and intuitive decision making, Human augmentation, Human-machine symbiosis, Organizational decision making},
	pages = {577--586},
	file = {Jarrahi_2018_Artificial intelligence and the future of work.pdf:/Users/neilnatarajan/Zotero/storage/YVCFE9FC/Jarrahi_2018_Artificial intelligence and the future of work.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/DUANG6K9/S0007681318300387.html:text/html},
}

@article{islam_technology_2022,
	title = {Technology {Adoption} and {Human} {Resource} {Management} {Practices}: {The} {Use} of {Artificial} {Intelligence} for {Recruitment} in {Bangladesh}},
	volume = {9},
	issn = {2322-0937},
	shorttitle = {Technology {Adoption} and {Human} {Resource} {Management} {Practices}},
	url = {https://doi.org/10.1177/23220937221122329},
	doi = {10.1177/23220937221122329},
	abstract = {Artificial intelligence (AI) is now considered indispensable in undertaking operational activities, especially in the area of human resource analytics. However, in practice, the rate of the adoption of such modern algorithms in organisations is still in its early stages. Consequently, the primary objective of this study is to identify the main antecedents of the adoption of AI-based technologies in recruitment, using the lens of the unified theory of acceptance and use of technology (UTAUT) model, alongside perceived credibility and moderating variables, in the context of an emerging nation in South Asia, namely Bangladesh. Data were collected from 283 human resource professionals employed in different manufacturing and service firms in Bangladesh through the administration of a questionnaire, which was analysed by applying PLS-SEM. The outcomes of the study show that all the direct hypothesised relationships were found to be significant, apart from the extended variable of perceived credibility. However, no moderating effect of gender or firm size was found in any of the hypothesised propositions. Finally, policy implications and recommendations for future researchers are proposed.},
	language = {en},
	number = {2},
	urldate = {2023-08-01},
	journal = {South Asian Journal of Human Resources Management},
	author = {Islam, Muhaiminul and Mamun, Abdullah Al and Afrin, Samina and Ali Quaosar, G. M. Azmal and Uddin, Md. Aftab},
	month = dec,
	year = {2022},
	note = {Publisher: SAGE Publications India},
	keywords = {\_tablet},
	pages = {324--349},
	file = {Islam et al_2022_Technology Adoption and Human Resource Management Practices.pdf:/Users/neilnatarajan/Zotero/storage/L9ZMJLHD/Islam et al_2022_Technology Adoption and Human Resource Management Practices.pdf:application/pdf},
}

@article{horodyski_applicants_2023,
	title = {Applicants' perception of artificial intelligence in the recruitment process},
	volume = {11},
	issn = {2451-9588},
	url = {https://www.sciencedirect.com/science/article/pii/S2451958823000362},
	doi = {10.1016/j.chbr.2023.100303},
	abstract = {The proliferation of Artificial intelligence (AI) technologies impacting entire business sectors is also transforming the field of human resources and recruitment. AI-based recruitment tools are changing the way recruitment processes are conducted. However, the perception of AI technology from the candidate's perspective has received limited coverage in the literature. Since little is known about how applicants experience AI-enabled recruitment, this paper explores their experiences and perceptions in hiring processes. The results of this study show that applicants perceive AI technology positively in hiring processes and see it as useful and easy to use. In terms of advantages, reduced response time was recognized as the most significant benefit. The lack of nuance in human judgment, low accuracy and reliability, and immature technology were identified as the biggest drawbacks of AI in recruitment.},
	language = {en},
	urldate = {2023-08-01},
	journal = {Computers in Human Behavior Reports},
	author = {Horodyski, Piotr},
	month = aug,
	year = {2023},
	keywords = {\_tablet, Artificial intelligence, Recruitment, AI, Human resources, Applicants' perception, TAM},
	pages = {100303},
	file = {Horodyski_2023_Applicants' perception of artificial intelligence in the recruitment process.pdf:/Users/neilnatarajan/Zotero/storage/IX4GAUST/Horodyski_2023_Applicants' perception of artificial intelligence in the recruitment process.pdf:application/pdf;ScienceDirect Snapshot:/Users/neilnatarajan/Zotero/storage/XSGHI3VG/S2451958823000362.html:text/html},
}

@book{hartigan_fairness_1989,
	address = {Washington, DC, US},
	series = {Fairness in employment testing:  {Validity} generalization, minority issues, and the {General} {Aptitude} {Test} {Battery}},
	title = {Fairness in employment testing:  {Validity} generalization, minority issues, and the {General} {Aptitude} {Test} {Battery}},
	isbn = {978-0-309-04033-4 978-0-309-04030-3},
	shorttitle = {Fairness in employment testing},
	abstract = {This volume is one of a number of studies conducted under the aegis of the National Research Council/National Academy of Sciences that deal with the use of standardized ability tests to make decisions about people in employment or educational settings.  Because such tests have a sometimes important role in allocating opportunities in American society, their use is quite rightly subject to questioning and not infrequently to legal scrutiny.  At issue in this report is the use of a federally sponsored employment test, the General Aptitude Test Battery (GATB), to match job seekers to requests for job applicants from private- and public-sector employers.  Developed in the late 1940s by the U.S. Employment Service (USES), a division of the Department of Labor, the GATB is used for vocational counseling and job referral by state-administered Employment Service (also known as Job Service) offices located in some 1,800 communities around the country. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	publisher = {National Academy Press},
	editor = {Hartigan, John A. and Wigdor, Alexandra K.},
	year = {1989},
	note = {Pages: xii, 354},
	keywords = {\_tablet, Educational Measurement, Employment Tests, General Aptitude Test Battery, Legal Processes, Private Sector, Public Sector, Test Battery, Test Validity},
	file = {1989_Fairness in employment testing.pdf:/Users/neilnatarajan/Zotero/storage/N4XRPQWN/1989_Fairness in employment testing.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/8BREAF4E/1989-97795-000.html:text/html},
}

@misc{wang_use_2022,
	title = {On the {Use} of {BERT} for {Automated} {Essay} {Scoring}: {Joint} {Learning} of {Multi}-{Scale} {Essay} {Representation}},
	shorttitle = {On the {Use} of {BERT} for {Automated} {Essay} {Scoring}},
	url = {http://arxiv.org/abs/2205.03835},
	doi = {10.48550/arXiv.2205.03835},
	abstract = {In recent years, pre-trained models have become dominant in most natural language processing (NLP) tasks. However, in the area of Automated Essay Scoring (AES), pre-trained models such as BERT have not been properly used to outperform other deep learning models such as LSTM. In this paper, we introduce a novel multi-scale essay representation for BERT that can be jointly learned. We also employ multiple losses and transfer learning from out-of-domain essays to further improve the performance. Experiment results show that our approach derives much benefit from joint learning of multi-scale essay representation and obtains almost the state-of-the-art result among all deep learning models in the ASAP task. Our multi-scale essay representation also generalizes well to CommonLit Readability Prize data set, which suggests that the novel text representation proposed in this paper may be a new and effective choice for long-text tasks.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Wang, Yongjie and Wang, Chuan and Li, Ruobing and Lin, Hui},
	month = may,
	year = {2022},
	note = {arXiv:2205.03835 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, \_tablet},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/8PDW63WZ/2205.html:text/html;Wang et al_2022_On the Use of BERT for Automated Essay Scoring.pdf:/Users/neilnatarajan/Zotero/storage/MHI9UBMN/Wang et al_2022_On the Use of BERT for Automated Essay Scoring.pdf:application/pdf},
}

@misc{cozma_automated_2018,
	title = {Automated essay scoring with string kernels and word embeddings},
	url = {http://arxiv.org/abs/1804.07954},
	doi = {10.48550/arXiv.1804.07954},
	abstract = {In this work, we present an approach based on combining string kernels and word embeddings for automatic essay scoring. String kernels capture the similarity among strings based on counting common character n-grams, which are a low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. To our best knowledge, we are the first to apply string kernels to automatically score essays. We are also the first to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings. We report the best performance on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Cozma, Mădălina and Butnaru, Andrei M. and Ionescu, Radu Tudor},
	month = jul,
	year = {2018},
	note = {arXiv:1804.07954 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6I5AGN6R/1804.html:text/html;Cozma et al_2018_Automated essay scoring with string kernels and word embeddings_annotated.pdf:/Users/neilnatarajan/Zotero/storage/JTCBTTSV/Cozma et al_2018_Automated essay scoring with string kernels and word embeddings_annotated.pdf:application/pdf;Cozma et al_2018_Automated essay scoring with string kernels and word embeddings.pdf:/Users/neilnatarajan/Zotero/storage/JTCBTTSV/Cozma et al_2018_Automated essay scoring with string kernels and word embeddings.pdf:application/pdf},
}

@article{ramesh_automated_2022,
	title = {An automated essay scoring systems: a systematic literature review},
	volume = {55},
	issn = {1573-7462},
	shorttitle = {An automated essay scoring systems},
	url = {https://doi.org/10.1007/s10462-021-10068-2},
	doi = {10.1007/s10462-021-10068-2},
	abstract = {Assessment in the Education system plays a significant role in judging student performance. The present evaluation system is through human assessment. As the number of teachers' student ratio is gradually increasing, the manual evaluation process becomes complicated. The drawback of manual evaluation is that it is time-consuming, lacks reliability, and many more. This connection online examination system evolved as an alternative tool for pen and paper-based methods. Present Computer-based evaluation system works only for multiple-choice questions, but there is no proper evaluation system for grading essays and short answers. Many researchers are working on automated essay grading and short answer scoring for the last few decades, but assessing an essay by considering all parameters like the relevance of the content to the prompt, development of ideas, Cohesion, and Coherence is a big challenge till now. Few researchers focused on Content-based evaluation, while many of them addressed style-based assessment. This paper provides a systematic literature review on automated essay scoring systems. We studied the Artificial Intelligence and Machine Learning techniques used to evaluate automatic essay scoring and analyzed the limitations of the current studies and research trends. We observed that the essay evaluation is not done based on the relevance of the content and coherence.},
	language = {en},
	number = {3},
	urldate = {2023-08-11},
	journal = {Artificial Intelligence Review},
	author = {Ramesh, Dadi and Sanampudi, Suresh Kumar},
	month = mar,
	year = {2022},
	keywords = {\_tablet, Deep learning, Assessment, Essay grading, Natural language processing, Short answer scoring},
	pages = {2495--2527},
	file = {Ramesh_Sanampudi_2022_An automated essay scoring systems.pdf:/Users/neilnatarajan/Zotero/storage/E4AA56VV/Ramesh_Sanampudi_2022_An automated essay scoring systems.pdf:application/pdf},
}

@misc{gozalo-brizuela_survey_2023,
	title = {A survey of {Generative} {AI} {Applications}},
	url = {http://arxiv.org/abs/2306.02781},
	doi = {10.48550/arXiv.2306.02781},
	abstract = {Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Gozalo-Brizuela, Roberto and Garrido-Merchán, Eduardo C.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02781 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/T6AULMZK/Gozalo-Brizuela and Garrido-Merchán - 2023 - A survey of Generative AI Applications.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/JMFDMJWK/2306.html:text/html},
}

@misc{gptzero_gptzero_2023,
	title = {{GPTZero} {\textbar} {Technology}},
	url = {https://gptzero.me/},
	abstract = {The World's \#1 AI Content Detector with over 1 Million Users},
	urldate = {2023-08-31},
	journal = {GPTZero},
	author = {GPTZero},
	month = aug,
	year = {2023},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/DYUXWK7T/technology.html:text/html},
}

@misc{kirchner_new_2023,
	title = {New {AI} classifier for indicating {AI}-written text},
	url = {https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text},
	abstract = {We’re launching a classifier trained to distinguish between AI-written and human-written text.},
	language = {en-US},
	urldate = {2023-08-31},
	author = {Kirchner, Jan Hendrik and Ahmad, Lama and Aaronson, Scott and Leike, Jan},
	month = jan,
	year = {2023},
	file = {Snapshot:/Users/neilnatarajan/Zotero/storage/Q3MPEXFC/new-ai-classifier-for-indicating-ai-written-text.html:text/html},
}

@misc{brown_language_2020-1,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/DYCYMRR8/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/J8VVXXXW/2005.html:text/html},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/DAGVLCV4/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/6PVW37WK/2204.html:text/html},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/P7VNGEEB/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/LCHF5QMU/2303.html:text/html},
}

@article{liang_gpt_2023,
	title = {{GPT} detectors are biased against non-native {English} writers},
	volume = {4},
	issn = {2666-3899},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10382961/},
	doi = {10.1016/j.patter.2023.100779},
	abstract = {GPT detectors frequently misclassify non-native English writing as AI generated, raising concerns about fairness and robustness. Addressing the biases in these detectors is crucial to prevent the marginalization of non-native English speakers in evaluative and educational settings and to create a more equitable digital landscape.},
	number = {7},
	urldate = {2023-09-20},
	journal = {Patterns},
	author = {Liang, Weixin and Yuksekgonul, Mert and Mao, Yining and Wu, Eric and Zou, James},
	month = jul,
	year = {2023},
	pmid = {37521038},
	pmcid = {PMC10382961},
	pages = {100779},
	file = {PubMed Central Full Text PDF:/Users/neilnatarajan/Zotero/storage/FVZUIK6N/Liang et al. - 2023 - GPT detectors are biased against non-native Englis.pdf:application/pdf},
}

@misc{dhaini_detecting_2023,
	title = {Detecting {ChatGPT}: {A} {Survey} of the {State} of {Detecting} {ChatGPT}-{Generated} {Text}},
	shorttitle = {Detecting {ChatGPT}},
	url = {http://arxiv.org/abs/2309.07689},
	doi = {10.48550/arXiv.2309.07689},
	abstract = {While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Dhaini, Mahdi and Poelman, Wessel and Erdogan, Ege},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07689 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/T6BJ76M3/2309.html:text/html;Dhaini et al_2023_Detecting ChatGPT_annotated.pdf:/Users/neilnatarajan/Zotero/storage/UXZJY9TZ/Dhaini et al_2023_Detecting ChatGPT_annotated.pdf:application/pdf;Dhaini et al_2023_Detecting ChatGPT.pdf:/Users/neilnatarajan/Zotero/storage/UXZJY9TZ/Dhaini et al_2023_Detecting ChatGPT.pdf:application/pdf},
}

@misc{xuan_can_2023,
	title = {Can {Users} {Correctly} {Interpret} {Machine} {Learning} {Explanations} and {Simultaneously} {Identify} {Their} {Limitations}?},
	url = {http://arxiv.org/abs/2309.08438},
	doi = {10.48550/arXiv.2309.08438},
	abstract = {Automated decision-making systems are becoming increasingly ubiquitous, motivating an immediate need for their explainability. However, it remains unclear whether users know what insights an explanation offers and, more importantly, what information it lacks. We conducted an online study with 200 participants to assess explainees' ability to realise known and unknown information for four representative explanations: transparent modelling, decision boundary visualisation, counterfactual explainability and feature importance. Our findings demonstrate that feature importance and decision boundary visualisation are the most comprehensible, but their limitations are not necessarily recognised by the users. In addition, correct interpretation of an explanation -- i.e., understanding known information -- is accompanied by high confidence, but a failure to gauge its limits -- thus grasp unknown information -- yields overconfidence; the latter phenomenon is especially prominent for feature importance and transparent modelling. Machine learning explanations should therefore embrace their richness and limitations to maximise understanding and curb misinterpretation.},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Xuan, Yueqing and Small, Edward and Sokol, Kacper and Hettiachchi, Danula and Sanderson, Mark},
	month = sep,
	year = {2023},
	note = {arXiv:2309.08438 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/INK7UMRK/2309.html:text/html;Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously_annotated.pdf:/Users/neilnatarajan/Zotero/storage/9W22XXZ6/Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously_annotated.pdf:application/pdf;Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously.pdf:/Users/neilnatarajan/Zotero/storage/9W22XXZ6/Xuan et al_2023_Can Users Correctly Interpret Machine Learning Explanations and Simultaneously.pdf:application/pdf},
}

@article{kaur_where_nodate,
	title = {Where are the {Humans} in {Human}-{AI} {Interaction}: {The} {Missing} {Human}-{Centered} {Perspective} on {Interpretability} {Tools} for {Machine} {Learning}},
	author = {Kaur, Harmanpreet},
	file = {Kaur_Where are the Humans in Human-AI Interaction_annotated.pdf:/Users/neilnatarajan/Zotero/storage/PR9FX5LC/Kaur_Where are the Humans in Human-AI Interaction_annotated.pdf:application/pdf;Kaur_Where are the Humans in Human-AI Interaction.pdf:/Users/neilnatarajan/Zotero/storage/PR9FX5LC/Kaur_Where are the Humans in Human-AI Interaction.pdf:application/pdf},
}

@misc{natarajan_trust_2023,
	title = {Trust {Explanations} to {Do} {What} {They} {Say}},
	url = {https://arxiv.org/abs/2303.13526v1},
	abstract = {How much are we to trust a decision made by an AI algorithm? Trusting an algorithm without cause may lead to abuse, and mistrusting it may similarly lead to disuse. Trust in an AI is only desirable if it is warranted; thus, calibrating trust is critical to ensuring appropriate use. In the name of calibrating trust appropriately, AI developers should provide contracts specifying use cases in which an algorithm can and cannot be trusted. Automated explanation of AI outputs is often touted as a method by which trust can be built in the algorithm. However, automated explanations arise from algorithms themselves, so trust in these explanations is similarly only desirable if it is warranted. Developers of algorithms explaining AI outputs (xAI algorithms) should provide similar contracts, which should specify use cases in which an explanation can and cannot be trusted.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Natarajan, Neil and Binns, Reuben and Zhao, Jun and Shadbolt, Nigel},
	month = feb,
	year = {2023},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/MQ9KKH72/Natarajan et al. - 2023 - Trust Explanations to Do What They Say.pdf:application/pdf},
}

@misc{aivodji_fairwashing_2019,
	title = {Fairwashing: the risk of rationalization},
	shorttitle = {Fairwashing},
	url = {http://arxiv.org/abs/1901.09749},
	doi = {10.48550/arXiv.1901.09749},
	abstract = {Black-box explanation is the problem of explaining how a machine learning model -- whose internal logic is hidden to the auditor and generally complex -- produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Aïvodji, Ulrich and Arai, Hiromi and Fortineau, Olivier and Gambs, Sébastien and Hara, Satoshi and Tapp, Alain},
	month = may,
	year = {2019},
	note = {arXiv:1901.09749 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Aïvodji et al_2019_Fairwashing_annotated.pdf:/Users/neilnatarajan/Zotero/storage/Y8EFH6JN/Aïvodji et al_2019_Fairwashing_annotated.pdf:application/pdf;Aïvodji et al_2019_Fairwashing.pdf:/Users/neilnatarajan/Zotero/storage/Y8EFH6JN/Aïvodji et al_2019_Fairwashing.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/LENDNTHG/1901.html:text/html},
}

@article{braun_using_2006,
	title = {Using thematic analysis in psychology},
	volume = {3},
	doi = {10.1191/1478088706qp063oa},
	abstract = {Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.},
	journal = {Qualitative Research in Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	month = jan,
	year = {2006},
	pages = {77--101},
	file = {Braun_Clarke_2006_Using thematic analysis in psychology_annotated.pdf:/Users/neilnatarajan/Zotero/storage/ZKGPPLK2/Braun_Clarke_2006_Using thematic analysis in psychology_annotated.pdf:application/pdf;Braun_Clarke_2006_Using thematic analysis in psychology.pdf:/Users/neilnatarajan/Zotero/storage/ZKGPPLK2/Braun_Clarke_2006_Using thematic analysis in psychology.pdf:application/pdf},
}

@article{braun_conceptual_2022,
	title = {Conceptual and design thinking for thematic analysis},
	volume = {9},
	issn = {2326-3598},
	doi = {10.1037/qup0000196},
	abstract = {Thematic analysis (TA) is widely used in qualitative psychology. In using TA, researchers must choose between a diverse range of approaches that can differ considerably in their underlying (but often implicit) conceptualizations of qualitative research, meaningful knowledge production, and key constructs such as themes, as well as analytic procedures. This diversity within the method of TA is typically poorly understood and rarely acknowledged, resulting in the frequent publication of research lacking in design coherence. Furthermore, because TA offers researchers something closer to a method (a transtheoretical tool or technique) rather than a methodology (a theoretically informed framework for research), one with considerable theoretical and design flexibility, researchers need to engage in careful conceptual and design thinking to produce TA research with methodological integrity. In this article, we support researchers in their conceptual and design thinking for TA, and particularly for the reflexive approach we have developed, by guiding them through the conceptual underpinnings of different approaches to TA, and key design considerations. We outline our typology of three main “schools” of TA—coding reliability, codebook, and reflexive—and consider how these differ in their conceptual underpinnings, with a particular focus on the distinct characteristics of our reflexive approach. We discuss key areas of design—research questions, data collection, participant/data item selection strategy and criteria, ethics, and quality standards and practices—and end with guidance on reporting standards for reflexive TA. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {1},
	journal = {Qualitative Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	year = {2022},
	note = {Place: US
Publisher: Educational Publishing Foundation},
	keywords = {Data Collection, Ethics, Integrity, Methodology, Procedural Knowledge, Qualitative Methods, Reporting Standards, Thematic Analysis},
	pages = {3--26},
	file = {Braun_Clarke_2022_Conceptual and design thinking for thematic analysis_annotated.pdf:/Users/neilnatarajan/Zotero/storage/EXUYUKSY/Braun_Clarke_2022_Conceptual and design thinking for thematic analysis_annotated.pdf:application/pdf;Braun_Clarke_2022_Conceptual and design thinking for thematic analysis.pdf:/Users/neilnatarajan/Zotero/storage/EXUYUKSY/Braun_Clarke_2022_Conceptual and design thinking for thematic analysis.pdf:application/pdf;Snapshot:/Users/neilnatarajan/Zotero/storage/R7JRQYN2/2021-45248-001.html:text/html},
}

@article{foody_thematic_2004,
	title = {Thematic {Map} {Comparison}},
	volume = {70},
	doi = {10.14358/PERS.70.5.627},
	abstract = {The accuracy of thematic maps derived by image classification analyses is often compared in remote sensing studies. This comparison is typically achieved by a basic subjective assessment of the observed difference in accuracy but should be undertaken in a statistically rigorous fashion.
One approach for the evaluation of the statistical significance of a difference in map accuracy that has been widely used in remote sensing research is based on the comparison of the kappa coefficient of agreement derived for each map. The conventional approach to the comparison of kappa coefficients
assumes that the samples used in their calculation are independent, an assumption that is commonly unsatisfied because the same sample of ground data sites is often used for each map. Alternative methods to evaluate the statistical significance of differences in accuracy are available for
both related and independent samples. Approaches for map comparison based on the kappa coefficient and proportion of correctly allocated cases, the two most widely used metrics of thematic map accuracy in remote sensing, are discussed. An example illustrates how classifications based on the
same sample of ground data sites may be compared rigorously and highlights the importance of distinguishing between one- and two-sided statistical tests in the comparison of classification accuracy statements.},
	number = {5},
	journal = {Photogrammetric Engineering \& Remote Sensing},
	author = {Foody, Giles M.},
	month = may,
	year = {2004},
	pages = {627--633},
	file = {Foody_2004_Thematic Map Comparison_annotated.pdf:/Users/neilnatarajan/Zotero/storage/H3PIX7WZ/Foody_2004_Thematic Map Comparison_annotated.pdf:application/pdf;Foody_2004_Thematic Map Comparison.pdf:/Users/neilnatarajan/Zotero/storage/H3PIX7WZ/Foody_2004_Thematic Map Comparison.pdf:application/pdf},
}

@article{braun_toward_2023,
	title = {Toward good practice in thematic analysis: {Avoiding} common problems and be(com)ing a knowing researcher},
	volume = {24},
	issn = {2689-5269},
	shorttitle = {Toward good practice in thematic analysis},
	url = {https://doi.org/10.1080/26895269.2022.2129597},
	doi = {10.1080/26895269.2022.2129597},
	number = {1},
	urldate = {2023-11-03},
	journal = {International Journal of Transgender Health},
	author = {Braun, Virginia and Clarke, Victoria},
	month = jan,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/26895269.2022.2129597},
	pages = {1--6},
	file = {Braun_Clarke_2023_Toward good practice in thematic analysis_annotated.pdf:/Users/neilnatarajan/Zotero/storage/YMDUWCTY/Braun_Clarke_2023_Toward good practice in thematic analysis_annotated.pdf:application/pdf;Braun_Clarke_2023_Toward good practice in thematic analysis.pdf:/Users/neilnatarajan/Zotero/storage/YMDUWCTY/Braun_Clarke_2023_Toward good practice in thematic analysis.pdf:application/pdf},
}

@incollection{zimmerman_research_2014,
	address = {New York, NY},
	title = {Research {Through} {Design} in {HCI}},
	isbn = {978-1-4939-0378-8},
	url = {https://doi.org/10.1007/978-1-4939-0378-8_8},
	abstract = {In Research through Design (RtD), researchers generate new knowledge by understanding the current state and then suggesting an improved future state in the form of a design. It involves deep reflection in iteratively understanding the people, problem, and context around a situation that researchers feel they can improve.},
	language = {en},
	urldate = {2023-11-03},
	booktitle = {Ways of {Knowing} in {HCI}},
	publisher = {Springer},
	author = {Zimmerman, John and Forlizzi, Jodi},
	editor = {Olson, Judith S. and Kellogg, Wendy A.},
	year = {2014},
	doi = {10.1007/978-1-4939-0378-8_8},
	keywords = {Interaction Design, Design Team, Ethical Stance, Participatory Design, Problem Framing},
	pages = {167--189},
}

@misc{shumailov_curse_2023,
	title = {The {Curse} of {Recursion}: {Training} on {Generated} {Data} {Makes} {Models} {Forget}},
	shorttitle = {The {Curse} of {Recursion}},
	url = {http://arxiv.org/abs/2305.17493},
	doi = {10.48550/arXiv.2305.17493},
	abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
	urldate = {2023-11-14},
	publisher = {arXiv},
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
	month = may,
	year = {2023},
	note = {arXiv:2305.17493 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/DSDBV8L6/Shumailov et al. - 2023 - The Curse of Recursion Training on Generated Data.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/ILIGJLUD/2305.html:text/html},
}

@misc{bai_constitutional_2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle = {Constitutional {AI}},
	url = {http://arxiv.org/abs/2212.08073},
	doi = {10.48550/arXiv.2212.08073},
	abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/neilnatarajan/Zotero/storage/6CLABGJV/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf:application/pdf;arXiv.org Snapshot:/Users/neilnatarajan/Zotero/storage/W8UE75XV/2212.html:text/html},
}

@incollection{zangwill_aesthetic_2023,
	edition = {Summer 2023},
	title = {Aesthetic {Judgment}},
	url = {https://plato.stanford.edu/archives/sum2023/entries/aesthetic-judgment/},
	abstract = {Beauty is an important part of our lives. Ugliness too. It is nosurprise then that philosophers since antiquity have been interestedin our experiences of and judgments about beauty and ugliness. Theyhave tried to understand the nature of these experiences andjudgments, and they have also wanted to know whether these experiencesand judgments were legitimate. Both these projects took a sharpenedform in the twentieth century, when this part of our lives came undera sustained attack in both European and North American intellectualcircles. Much of the discourse about beauty since the eighteenthcentury had deployed a notion of the “aesthetic”, and sothat notion in particular came in for criticism. This disdain for theaesthetic may have roots in a broader cultural Puritanism, which fearsthe connection between the aesthetic and pleasure. At one time, fromthe 1960s to the 1990s, even to suggest that an artwork might be goodbecause it is pleasurable, as opposed to cognitively, morally orpolitically beneficial, was to court derision. (This is less truenow.) The twentieth century was not kind to the notions of beauty orthe aesthetic. Nevertheless, there were always somethinkers—philosophers, as well as others in the study ofparticular arts—who persisted in thinking seriously about beautyand the aesthetic. In the first part of this essay, we will look atthe particularly rich account of judgments of beauty given to us byImmanuel Kant. The notion of a “judgment of taste” iscentral to Kant’s account and also to virtually everyone workingin traditional aesthetics; so we begin by examining Kant’scharacterization of the judgment of taste. In the second part, we lookat the issues that twentieth century thinkers raised. In the thirdpart, we consider disinterestedness, which is taken by Kant to be partof the judgment of taste. We end, in the fourth part, by drawing onKant’s account of the judgment of taste to consider whether thenotion of the aesthetic is viable.},
	urldate = {2023-12-05},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Zangwill, Nick},
	editor = {Zalta, Edward N. and Nodelman, Uri},
	year = {2023},
	keywords = {aesthetic, concept of the, Aristotle, General Topics: aesthetics, Hume, David: aesthetics, Kant, Immanuel: aesthetics and teleology, pleasure, relativism},
	file = {SEP - Snapshot:/Users/neilnatarajan/Zotero/storage/43SUM3E6/aesthetic-judgment.html:text/html},
}

@article{papenmeier_its_2022,
	title = {It’s {Complicated}: {The} {Relationship} between {User} {Trust}, {Model} {Accuracy} and {Explanations} in {AI}},
	volume = {29},
	issn = {1073-0516, 1557-7325},
	shorttitle = {It’s {Complicated}},
	url = {https://dl.acm.org/doi/10.1145/3495013},
	doi = {10.1145/3495013},
	abstract = {Automated decision-making systems become increasingly powerful due to higher model complexity. While powerful in prediction accuracy, Deep Learning models are black boxes by nature, preventing users from making informed judgments about the correctness and fairness of such an automated system. Explanations have been proposed as a general remedy to the black box problem. However, it remains unclear if effects of explanations on user trust generalise over varying accuracy levels. In an online user study with 959 participants, we examined the practical consequences of adding explanations for user trust: We evaluated trust for three explanation types on three classifiers of varying accuracy. We find that the influence of our explanations on trust differs depending on the classifier’s accuracy. Thus, the interplay between trust and explanations is more complex than previously reported. Our findings also reveal discrepancies between self-reported and behavioural trust, showing that the choice of trust measure impacts the results.},
	language = {en},
	number = {4},
	urldate = {2023-12-05},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Papenmeier, Andrea and Kern, Dagmar and Englebienne, Gwenn and Seifert, Christin},
	month = aug,
	year = {2022},
	pages = {1--33},
	file = {Papenmeier et al. - 2022 - It’s Complicated The Relationship between User Tr.pdf:/Users/neilnatarajan/Zotero/storage/4LMCZAYI/Papenmeier et al. - 2022 - It’s Complicated The Relationship between User Tr.pdf:application/pdf},
}

@inproceedings{poursabzi-sangdeh_manipulating_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Manipulating and {Measuring} {Model} {Interpretability}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445315},
	doi = {10.1145/3411764.3445315},
	abstract = {With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N = 3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.},
	urldate = {2023-12-05},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Poursabzi-Sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Wortman Vaughan, Jennifer Wortman and Wallach, Hanna},
	month = may,
	year = {2021},
	keywords = {interpretability, human-centered machine learning, machine-assisted decision making},
	pages = {1--52},
	file = {Submitted Version:/Users/neilnatarajan/Zotero/storage/SJTLWT5B/Poursabzi-Sangdeh et al. - 2021 - Manipulating and Measuring Model Interpretability.pdf:application/pdf},
}

@article{nourani_effects_2019,
	title = {The {Effects} of {Meaningful} and {Meaningless} {Explanations} on {Trust} and {Perceived} {System} {Accuracy} in {Intelligent} {Systems}},
	volume = {7},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2769-1349},
	url = {https://ojs.aaai.org/index.php/HCOMP/article/view/5284},
	doi = {10.1609/hcomp.v7i1.5284},
	abstract = {Machine learning and artificial intelligence algorithms can assist human decision making and analysis tasks. While such technology shows promise, willingness to use and rely on intelligent systems may depend on whether people can trust and understand them. To address this issue, researchers have explored the use of explainable interfaces that attempt to help explain why or how a system produced the output for a given input. However, the effects of meaningful and meaningless explanations (determined by their alignment with human logic) are not properly understood, especially with users who are non-experts in data science. Additionally, we wanted to explore how explanation inclusion and level of meaningfulness would affect the user’s perception of accuracy. We designed a controlled experiment using an image classification scenario with local explanations to evaluate and better understand these issues. Our results show that whether explanations are human-meaningful can significantly affect perception of a system’s accuracy independent of the actual accuracy observed from system usage. Participants significantly underestimated the system’s accuracy when it provided weak, less human-meaningful explanations. Therefore, for intelligent systems with explainable interfaces, this research demonstrates that users are less likely to accurately judge the accuracy of algorithms that do not operate based on human-understandable rationale.},
	language = {en},
	urldate = {2023-12-05},
	journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	author = {Nourani, Mahsan and Kabir, Samia and Mohseni, Sina and Ragan, Eric D.},
	month = oct,
	year = {2019},
	pages = {97--105},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/QZXQRVFQ/Nourani et al. - 2019 - The Effects of Meaningful and Meaningless Explanat.pdf:application/pdf},
}

@inproceedings{schrills_color_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Color for {Characters} - {Effects} of {Visual} {Explanations} of {AI} on {Trust} and {Observability}},
	isbn = {978-3-030-50334-5},
	doi = {10.1007/978-3-030-50334-5_8},
	abstract = {The present study investigates the effects of prototypical visualization approaches aimed at increasing the explainability of machine learning systems in regard to perceived trustworthiness and observability. As the amount of processes automated by artificial intelligence (AI) increases, so does the need to investigate users’ perception. Previous research on explainable AI (XAI) tends to focus on technological optimization. The limited amount of empirical user research leaves key questions unanswered, such as which XAI designs actually improve perceived trustworthiness and observability. We assessed three different visual explanation approaches, consisting of either only a table with classification scores used for classification, or, additionally, one of two different backtraced visual explanations. In a within-subjects design with N = 83 we examined the effects on trust and observability in an online experiment. While observability benefitted from visual explanations, information-rich explanations also led to decreased trust. Explanations can support human-AI interaction, but differentiated effects on trust and observability have to be expected. The suitability of different explanatory approaches for individual AI applications should be further examined to ensure a high level of trust and observability in e.g. automated image processing.},
	language = {en},
	booktitle = {Artificial {Intelligence} in {HCI}},
	publisher = {Springer International Publishing},
	author = {Schrills, Tim and Franke, Thomas},
	editor = {Degen, Helmut and Reinerman-Jones, Lauren},
	year = {2020},
	keywords = {Explainable AI, Human-AI interaction, Human-automation interaction, Machine learning, Trust in Automation},
	pages = {121--135},
}

@book{hildebrandt_law_nodate,
	title = {Law for computer scientists and other folk},
	author = {Hildebrandt},
	file = {_.pdf:/Users/neilnatarajan/Zotero/storage/P6LUM9QL/_.pdf:application/pdf},
}

@article{gilpin_chaos_nodate,
	title = {Chaos as an interpretable benchmark for...},
	author = {Gilpin},
	file = {Gilpin - 2021 - Chaos as an interpretable benchmark for forecastin.pdf:/Users/neilnatarajan/Zotero/storage/N5S487Q2/Gilpin - 2021 - Chaos as an interpretable benchmark for forecastin.pdf:application/pdf},
}

@misc{javed_svarah_2023,
	title = {Svarah: {Evaluating} {English} {ASR} {Systems} on {Indian} {Accents}},
	shorttitle = {Svarah},
	url = {https://arxiv.org/abs/2305.15760v1},
	abstract = {India is the second largest English-speaking country in the world with a speaker base of roughly 130 million. Thus, it is imperative that automatic speech recognition (ASR) systems for English should be evaluated on Indian accents. Unfortunately, Indian speakers find a very poor representation in existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent Archive, etc. In this work, we address this gap by creating Svarah, a benchmark that contains 9.6 hours of transcribed English audio from 117 speakers across 65 geographic locations throughout India, resulting in a diverse range of accents. Svarah comprises both read speech and spontaneous conversational data, covering various domains, such as history, culture, tourism, etc., ensuring a diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR systems on Svarah and show that there is clear scope for improvement on Indian accents. Svarah as well as all our code will be publicly available.},
	language = {en},
	urldate = {2024-01-12},
	journal = {arXiv.org},
	author = {Javed, Tahir and Joshi, Sakshi and Nagarajan, Vignesh and Sundaresan, Sai and Nawale, Janki and Raman, Abhigyan and Bhogale, Kaushal and Kumar, Pratyush and Khapra, Mitesh M.},
	month = may,
	year = {2023},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/74V763ZP/Javed et al. - 2023 - Svarah Evaluating English ASR Systems on Indian A.pdf:application/pdf},
}

@article{bradley_robustness_1978,
	title = {Robustness?},
	volume = {31},
	issn = {0007-1102, 2044-8317},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/10.1111/j.2044-8317.1978.tb00581.x},
	doi = {10.1111/j.2044-8317.1978.tb00581.x},
	abstract = {The actual behaviour of the probability of a Type I error under assumption violation is quite complex, depending upon a wide variety of interacting factors. Yet allegations of robustness tend to ignore its highly particularistic nature and neglect to mention important qualifying conditions. The result is often a vast overgeneralization which nevertheless is difficult to refute since a standard quantitative definition of what constitutes robustness does not exist. Yet under any halfway reasonable quantitative definition, many of the most prevalent claims of robustness would be demonstrably false. Therefore robustness is a highly questionable concept.},
	language = {en},
	number = {2},
	urldate = {2024-02-19},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Bradley, James V.},
	month = nov,
	year = {1978},
	pages = {144--152},
}

@inproceedings{niculescu-mizil_predicting_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {Predicting good probabilities with supervised learning},
	isbn = {978-1-59593-180-1},
	url = {https://doi.org/10.1145/1102351.1102430},
	doi = {10.1145/1102351.1102430},
	abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	month = aug,
	year = {2005},
	pages = {625--632},
}

@article{mandrekar_receiver_2010,
	title = {Receiver {Operating} {Characteristic} {Curve} in {Diagnostic} {Test} {Assessment}},
	volume = {5},
	issn = {1556-0864},
	url = {https://www.sciencedirect.com/science/article/pii/S1556086415306043},
	doi = {10.1097/JTO.0b013e3181ec173d},
	abstract = {The performance of a diagnostic test in the case of a binary predictor can be evaluated using the measures of sensitivity and specificity. However, in many instances, we encounter predictors that are measured on a continuous or ordinal scale. In such cases, it is desirable to assess performance of a diagnostic test over the range of possible cutpoints for the predictor variable. This is achieved by a receiver operating characteristic (ROC) curve that includes all the possible decision thresholds from a diagnostic test result. In this brief report, we discuss the salient features of the ROC curve, as well as discuss and interpret the area under the ROC curve, and its utility in comparing two different tests or predictor variables of interest.},
	number = {9},
	urldate = {2024-02-19},
	journal = {Journal of Thoracic Oncology},
	author = {Mandrekar, Jayawant N.},
	month = sep,
	year = {2010},
	keywords = {AUC, ROC, Sensitivity, Specificity},
	pages = {1315--1316},
	file = {Full Text:/Users/neilnatarajan/Zotero/storage/F6QXFX5R/Mandrekar - 2010 - Receiver Operating Characteristic Curve in Diagnos.pdf:application/pdf},
}

@article{dehouche_plagiarism_2021,
	title = {Plagiarism in the age of massive {Generative} {Pre}-trained {Transformers} ({GPT}-3)},
	volume = {21},
	issn = {1863-5415, 1611-8014},
	url = {https://www.int-res.com/abstracts/esep/v21/p17-23/},
	doi = {10.3354/esep00195},
	abstract = {As if 2020 was not a peculiar enough year, its fifth month saw the relatively quiet publication of a preprint describing the most powerful natural language processing (NLP) system to date — GPT-3 (Generative Pre-trained Transformer-3) — created by the Silicon Valley research firm OpenAI. Though the software implementation of GPT-3 is still in its initial beta release phase, and its full capabilities are still unknown as of the time of this writing, it has been shown that this artificial intelligence can comprehend prompts in natural language, on virtually any topic, and generate relevant original text content that is indistinguishable from human writing. Moreover, access to these capabilities, in a limited yet worrisome enough extent, is available to the general public. This paper presents examples of original content generated by the author using GPT-3. These examples illustrate some of the capabilities of GPT-3 in comprehending prompts in natural language and generating convincing content in response. I use these examples to raise specific fundamental questions pertaining to the intellectual property of this content and the potential use of GPT-3 to facilitate plagiarism. The goal is to instigate a sense of urgency, as well as a sense of present tardiness on the part of the academic community in addressing these questions.},
	language = {en},
	urldate = {2024-02-20},
	journal = {Ethics in Science and Environmental Politics},
	author = {Dehouche, N},
	month = mar,
	year = {2021},
	pages = {17--23},
	file = {Dehouche - 2021 - Plagiarism in the age of massive Generative Pre-tr.pdf:/Users/neilnatarajan/Zotero/storage/CYAD6KDV/Dehouche - 2021 - Plagiarism in the age of massive Generative Pre-tr.pdf:application/pdf},
}

@article{platt_probabilistic_2000,
	title = {Probabilistic {Outputs} for {Support} {Vector} {Machines} and {Comparisons} to {Regularized} {Likelihood} {Methods}},
	volume = {10},
	abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
	journal = {Adv. Large Margin Classif.},
	author = {Platt, John},
	month = jun,
	year = {2000},
	file = {Full Text PDF:/Users/neilnatarajan/Zotero/storage/U3HNMIGP/Platt - 2000 - Probabilistic Outputs for Support Vector Machines .pdf:application/pdf},
}

@inproceedings{zadrozny_transforming_2002,
	address = {New York, NY, USA},
	series = {{KDD} '02},
	title = {Transforming classifier scores into accurate multiclass probability estimates},
	isbn = {978-1-58113-567-1},
	url = {https://doi.org/10.1145/775047.775151},
	doi = {10.1145/775047.775151},
	abstract = {Class membership probability estimates are important for many applications of data mining in which classification outputs are combined with other sources of information for decision-making, such as example-dependent misclassification costs, the outputs of other classifiers, or domain knowledge. Previous calibration methods apply only to two-class problems. Here, we show how to obtain accurate probability estimates for multiclass problems by combining calibrated binary probability estimates. We also propose a new method for obtaining calibrated two-class probability estimates that can be applied to any classifier that produces a ranking of examples. Using naive Bayes and support vector machine classifiers, we give experimental results from a variety of two-class and multiclass domains, including direct marketing, text categorization and digit recognition.},
	urldate = {2024-02-22},
	booktitle = {Proceedings of the eighth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Zadrozny, Bianca and Elkan, Charles},
	month = jul,
	year = {2002},
	pages = {694--699},
}
